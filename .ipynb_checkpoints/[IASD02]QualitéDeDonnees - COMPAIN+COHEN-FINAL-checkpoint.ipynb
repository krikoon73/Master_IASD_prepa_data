{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini projet Qualité de Données : Détections des doublons\n",
    "## ***Christophe COMPAIN / Sander COHEN***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectif et Méthodologie suivie\n",
    "L'objectif du projet est d'identifier les logiciels vendus sur les deux plateformes.\n",
    "Pour ce faire, nous disposons des données pour chacune des plateformes isolément, respectivement dans les fichiers ***Company1.csv*** et ***Company2.csv***.\n",
    "\n",
    "Au cours de l'étude, nous avons essayé nombre de prétraitements et algorithmes de recherche de doublons différents. Nous ne présenterons ici que ceux finalement sélectionnés, et ne ferons qu'évoquer une partie de ceux mis de côté (ceux ayant nous semblant le plus interessant dans l'idée même s'il ne permettent pas d'améliorer significativement les résultats).\n",
    "\n",
    "Après imports des packages et données, nous explorerons brièvement dans une première partie les données et en tirerons des points d'attention pour le prétraitement et la détection des doublons.\n",
    "Dans une deuxième partie, nous présenterons et effectuerons notre pré-traitement. La troisième partie comportera nos fonctions et algorithmes de detections de doublons, ainsi que nos résultats.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 - Import des packages utilisés et import des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\scohe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\scohe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:\\\\OneDrive - Université Paris-Dauphine\\\\Bureau\\\\Cours Master\\\\12-Qualité de Données\\\\\\Projet\\\\mini-projet\\\\\"\n",
    "file1= \"Data\\\\Company1.csv\" #\"SampleData\\\\Sample_Company1.csv\"\n",
    "file2= \"Data\\\\Company2.csv\" #\"SampleData\\\\Sample_Company2.csv\"\n",
    "real= \"Data\\\\Ground_truth_mappings.csv\" #\"SampleData\\\\Sample_Groud_truth_mappings.csv\"\n",
    "#path = \"/Users/ccompain/Documents/code/Dauphine/MasterIASD_prepa_data/mini-projet/github/\"\n",
    "#file1 = \"Data/Company1.csv\"\n",
    "#file2 = \"Data/Company2.csv\"\n",
    "#real= \"Data/Ground_truth_mappings.csv\" #\"SampleData\\\\Sample_Groud_truth_mappings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "company1 = pd.read_csv(path+file1, encoding = \"ISO-8859-1\")\n",
    "company2 = pd.read_csv(path+file2, encoding = \"ISO-8859-1\")\n",
    "ground_truth_matches = pd.read_csv(path+real, encoding = \"ISO-8859-1\").drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Exploration des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Aspects généraux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>b000jz4hqo</td>\n",
       "      <td>clickart 950 000 - premier image pack (dvd-rom)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>broderbund</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>b0006zf55o</td>\n",
       "      <td>ca international - arcserve lap/desktop oem 30pk</td>\n",
       "      <td>oem arcserve backup v11.1 win 30u for laptops ...</td>\n",
       "      <td>computer associates</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>b00004tkvy</td>\n",
       "      <td>noah's ark activity center (jewel case ages 3-8)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>victory multimedia</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>b000g80lqo</td>\n",
       "      <td>peachtree by sage premium accounting for nonpr...</td>\n",
       "      <td>peachtree premium accounting for nonprofits 20...</td>\n",
       "      <td>sage software</td>\n",
       "      <td>599.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>b0006se5bq</td>\n",
       "      <td>singing coach unlimited</td>\n",
       "      <td>singing coach unlimited - electronic learning ...</td>\n",
       "      <td>carry-a-tune technologies</td>\n",
       "      <td>99.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              title  \\\n",
       "0  b000jz4hqo    clickart 950 000 - premier image pack (dvd-rom)   \n",
       "1  b0006zf55o   ca international - arcserve lap/desktop oem 30pk   \n",
       "2  b00004tkvy   noah's ark activity center (jewel case ages 3-8)   \n",
       "3  b000g80lqo  peachtree by sage premium accounting for nonpr...   \n",
       "4  b0006se5bq                            singing coach unlimited   \n",
       "\n",
       "                                         description  \\\n",
       "0                                                NaN   \n",
       "1  oem arcserve backup v11.1 win 30u for laptops ...   \n",
       "2                                                NaN   \n",
       "3  peachtree premium accounting for nonprofits 20...   \n",
       "4  singing coach unlimited - electronic learning ...   \n",
       "\n",
       "                manufacturer   price  \n",
       "0                 broderbund    0.00  \n",
       "1        computer associates    0.00  \n",
       "2         victory multimedia    0.00  \n",
       "3              sage software  599.99  \n",
       "4  carry-a-tune technologies   99.99  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>11125907881740407428</td>\n",
       "      <td>learning quickbooks 2007</td>\n",
       "      <td>learning quickbooks 2007</td>\n",
       "      <td>intuit</td>\n",
       "      <td>38.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>11538923464407758599</td>\n",
       "      <td>superstart! fun with reading &amp; writing!</td>\n",
       "      <td>fun with reading &amp; writing! is designed to hel...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>11343515411965421256</td>\n",
       "      <td>qb pos 6.0 basic software</td>\n",
       "      <td>qb pos 6.0 basic retail mngmt software. for re...</td>\n",
       "      <td>intuit</td>\n",
       "      <td>637.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>12049235575237146821</td>\n",
       "      <td>math missions: the amazing arcade adventure (g...</td>\n",
       "      <td>save spectacle city by disrupting randall unde...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>12244614697089679523</td>\n",
       "      <td>production prem cs3 mac upgrad</td>\n",
       "      <td>adobe cs3 production premium mac upgrade from ...</td>\n",
       "      <td>adobe software</td>\n",
       "      <td>805.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               name  \\\n",
       "0  11125907881740407428                           learning quickbooks 2007   \n",
       "1  11538923464407758599            superstart! fun with reading & writing!   \n",
       "2  11343515411965421256                          qb pos 6.0 basic software   \n",
       "3  12049235575237146821  math missions: the amazing arcade adventure (g...   \n",
       "4  12244614697089679523                     production prem cs3 mac upgrad   \n",
       "\n",
       "                                         description    manufacturer   price  \n",
       "0                           learning quickbooks 2007          intuit   38.99  \n",
       "1  fun with reading & writing! is designed to hel...             NaN    8.49  \n",
       "2  qb pos 6.0 basic retail mngmt software. for re...          intuit  637.99  \n",
       "3  save spectacle city by disrupting randall unde...             NaN   12.95  \n",
       "4  adobe cs3 production premium mac upgrade from ...  adobe software  805.99  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons noter que :\n",
    "* les produits semblent être des logiciels informatiques, et les 'manufacturer' des éditeurs;\n",
    "* les noms des différentes colonnes correspondent entre les deux dataframes, à l'exception de la colonne 'title' de ***company1*** qui correspond à 'name' de ***company2***;\n",
    "* dans ***company 1***, plusieurs 'description' sont en 'NaN' et plusieurs 'price' sont à 0;\n",
    "* dans ***company 2***, plusieurs 'manufacturer' sont en 'NaN'.\n",
    "\n",
    "Une analyse plus poussée nous a permis également de detecter d'autres caractéristiques à retraiter :\n",
    "* certains prix sont affichés en livres sterling dans ***company 2***. Ne connaissant pas la devise initiale, nous retraiterons uniquement la chaine de caractères en supprimant les caractères non numériques;\n",
    "* certaines 'description' sont en 'NaN' également dans ***company 2***;\n",
    "* les deux dataframes comportent des typo.\n",
    "\n",
    "Afin de détecter l'ampleur des 'NaN', nous effectuons un *count* sur les deux dataframes :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              1363\n",
       "title           1363\n",
       "description     1248\n",
       "manufacturer    1363\n",
       "price           1363\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              3226\n",
       "name            3226\n",
       "description     3035\n",
       "manufacturer     232\n",
       "price           3226\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous observons que :\n",
    "* 'manufacturer' est très peu rempli dans ***company2***;\n",
    "* 'description' est rempli pour plus de 90% des deux dataframes;\n",
    "* ***company2*** contient plus de 2.3 fois plus d'entrées que ***company1***. \n",
    "\n",
    "Concernant ce dernier point, cela peut laisser présager des doublons multiples (un logiciel présent dans ***company1*** pourrait avoir plusieurs matches dans ***company2***). Dans la pratique, c'est effectivement ce que nous observons dans le dataframe ***ground_truth_matches***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top manufacturers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "manufacturer\n",
       "adobe                   87\n",
       "encore software         76\n",
       "topics entertainment    73\n",
       "encore                  62\n",
       "microsoft               58\n",
       "aspyr media             27\n",
       "apple                   26\n",
       "fogware publishing      19\n",
       "intuit                  18\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company1.groupby('manufacturer')['id'].count().sort_values(ascending=False).head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "manufacturer\n",
       "punch software         30\n",
       "abacus software        27\n",
       "freeverse software     24\n",
       "individual software    24\n",
       "apple software         15\n",
       "intuit                  8\n",
       "webroot software        7\n",
       "onone software          7\n",
       "ce software             6\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company2.groupby('manufacturer')['id'].count().sort_values(ascending=False).head(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant retraitement des 'NaN' de 'manufacturer' dans ***company2***, nous observons que:\n",
    "* les 'manufacturer' principaux sont différents dans les deux dataframes;\n",
    "* certains 'manufacturer' sont comptés deux fois comme 'encore' et 'encore software' dans ***company1***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2  - Prétraitement des données\n",
    "\n",
    "Dans cette partie, nous allons effectuer un prétraitement sur les dataframes ***company1*** et ***company2***, et notamment :\n",
    "* créer une fonction *prep* qui contiendra notre prétraitement des colonnes string utilisées qui :\n",
    "    1. supprimera les caractères non alphanumériques, \n",
    "    2. convertira les caractères en minuscules,\n",
    "    3. remplacera certains mots choisis (exemples : 'professional' -> 'pro', 'windows' -> 'win'),\n",
    "    4. après tokenization, supprimera des stopwords qui nous semblent polluer la détection de doublons (stopwords de nltk agrementé de quelques mots parmi les plus utilisés de la base comme 'software', 'entertainment', 'inc'...),\n",
    "    5. lemmatisera les mots de façon à limiter les typo, pluriels...\n",
    "    6. renverra la chaine de caractères comprenant l'ensemble des mots séparés par des espaces\n",
    "* créer une fonction *retreatprice* qui permettra de conserver uniquement les caractères numériques (et notamment supprimer les 'gbp') et renvoit le résultat sous forme de 'float';\n",
    "* normaliser les deux dataframes en :\n",
    "    1. renommant la colonne 'title' de ***company 1*** en 'name';\n",
    "    2. remplaçant les 'NaN' par des ' ' dans les différentes colonnes;\n",
    "    3. appliquant la fonction *retreatprice* à 'price' pour obtenir une nouvelle colonne 'price_retreat'\n",
    "    4. appliquant la fonction *prep* à la concatenation des colonnes 'manufacturer' et 'name' pour obtenir une nouvelle colonne 'full data'. Cette colonne sera à la base de notre calcul de similarité\n",
    "* ajouter une colonne 'Version', basée sur la recherche des termes 'win', 'mac' et 'linux' au sein des colonnes 'name' et 'description' (en cas de présence multiple ou d'absence, 'None' est renvoyé).\n",
    "\n",
    "Nous n'incluons pas les codes d'autres tentatives non conservées comme, par exemple, la suppression au sein des colonnes 'full data' dans chacun des deux dataframes des mots uniques (ie qu'on ne retrouve qu'une fois dans tout le dataframe). Ceux-ci, différenciant, aurait pu polluer nos calculs de similarités (comme le font les codes existant dans ***company 2*** du type '22070152'), mais semblent au final permettre de limiter les faux positifs. Nous avons fait le choix de ne pas sélectionner à la main les mots uniques améliorant nos résultats pour ne pas artificiellement gonfler notre F-measure (ie en connaissant à priori ***ground_truth_matches***).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Récupération d'une base de stop_words utilisée dans la fonction prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Récupération d'une base de stop_words utilisée dans la fonction prep\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "stop_words.update([\"r\",\"v\",\"software\",\"entertainment\",\"inc\",\"usa\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création de la fonction de nettoyage textuelle *prep*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prep(texte):\n",
    "    #suppression des caracteres non alphanumériques + tout en minuscule\n",
    "    texte = re.sub(\"[^a-zA-Z0-9_]\", \" \",str(texte)).lower()\n",
    "    #remplacement de mots\n",
    "    texte = texte.replace(\"professional\", \"pro\").replace(\"windows\",\"win\").replace(\"upgrade\",\"upg\").replace(\"deluxe\",\"dlx\")\n",
    "    #tokenization par mot\n",
    "    tokens = nltk.word_tokenize(texte)\n",
    "    #suppression des stopwords\n",
    "    filtered_tokens = [w for w in tokens if not w in stop_words]\n",
    "#    # Stemming\n",
    "#    texte = [nltk.stem.SnowballStemmer('english').stem(w) for w in filtered_tokens]\n",
    "    # Lemmatization\n",
    "    texte = [nltk.stem.WordNetLemmatizer().lemmatize(w) for w in filtered_tokens]\n",
    "    #remise sous forme d'une string\n",
    "    return \" \".join(texte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création de la fonction de normalisation des prix *retreatprice*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def retreatprice(texte):\n",
    "    #suppression des caracteres non alphanumériques + tout en minuscule\n",
    "    return float(re.sub(\"[^0-9.]\", \" \",str(texte)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalisation des dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "company1['Company']=\"company1\"\n",
    "company1=company1.rename(columns={\"title\": \"name\"})\n",
    "company1['name'] = company1['name'].fillna(' ')\n",
    "company1['manufacturer'] = company1['manufacturer'].fillna(' ')\n",
    "company1['description'] = company1['description'].fillna(' ')\n",
    "company1['price'] = company1['price'].fillna(' ')\n",
    "company1['price_retreat'] = company1['price'].apply(retreatprice)\n",
    "company1['full data']=company1['manufacturer'].apply(prep) + ' ' + company1['name'].apply(prep) # + ' ' + company1['description'].apply(prep)\n",
    "\n",
    "company2['Company']=\"company2\"\n",
    "company2['name'] = company2['name'].fillna(' ')\n",
    "company2['manufacturer'] = company2['manufacturer'].fillna(' ')\n",
    "company2['description'] = company2['description'].fillna(' ')\n",
    "company2['price'] = company2['price'].fillna(' ')\n",
    "company2['price_retreat'] = company2['price'].apply(retreatprice)\n",
    "company2['full data']=company2['manufacturer'].apply(prep) + ' ' + company2['name'].apply(prep) # + ' ' + company2['description'].apply(prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ajout d'une colonne 'Version'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ajout d'une colonne de version - pour ce faire, nous créons des colonnes intermédiaires 'Win', 'Mac' et 'Linux'\n",
    "company1['Win']=np.where(company1['name'].str.contains('win') | company1['description'].str.contains('win'),1,0)\n",
    "company1['Mac']=np.where(company1['name'].str.contains('mac') | company1['description'].str.contains('mac'),1,0)\n",
    "company1['Linux']=np.where(company1['name'].str.contains('linux') | company1['description'].str.contains('linux'),1,0)\n",
    "company1['version']=np.select(\n",
    "    [(company1['Win']+company1['Mac']+company1['Linux']==1) & (company1['Win']==1),\n",
    "     (company1['Win']+company1['Mac']+company1['Linux']==1) & (company1['Mac']==1),\n",
    "     (company1['Win']+company1['Mac']+company1['Linux']==1) & (company1['Linux']==1),\n",
    "     (company1['Win']+company1['Mac']+company1['Linux']!=1)\n",
    "    ],\n",
    "    ['Win','Mac','Linux','None'])\n",
    "\n",
    "company2['Win']=np.where(company2['name'].str.contains('win') | company2['description'].str.contains('win'),1,0)\n",
    "company2['Mac']=np.where(company2['name'].str.contains('mac') | company2['description'].str.contains('mac'),1,0)\n",
    "company2['Linux']=np.where(company2['name'].str.contains('linux') | company2['description'].str.contains('linux'),1,0)\n",
    "company2['version']=np.select(\n",
    "    [(company2['Win']+company2['Mac']+company2['Linux']==1) & (company2['Win']==1),\n",
    "     (company2['Win']+company2['Mac']+company2['Linux']==1) & (company2['Mac']==1),\n",
    "     (company2['Win']+company2['Mac']+company2['Linux']==1) & (company2['Linux']==1),\n",
    "     (company2['Win']+company2['Mac']+company2['Linux']!=1)\n",
    "    ],\n",
    "    ['Win','Mac','Linux','None'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3  - Détection des doublons\n",
    "\n",
    "Après de nombreuses tentatives en utilisant des mesures de distance ou similarité présentes dans le package *nltk* (jaccard distance, edit distance...), nous avons testé d'autres methodes et notamment l'utilisation de l'algorithme tf-idf présent dans le package *sklearn* qui, dans sa version la plus classique, permet, pour chaque document, de calculer une mesure, par n-gram présent, proportionnelle à la fréquence du n-gram dans le document (Terme Frequency) et inversement proportionnelle à la fréquence dans la base (Inverse Document Frequency).\n",
    "\n",
    "Une fois la fonction *TfidfVectorizer* appliquée à la concatenation d'eventuels sous-dataframes de ***company 1*** et ***company 2*** (nous y reviendrons), nous serons en mesure, pour chaque couple d'articles de calculer une mesure de similarité en effectuant un produit scalaire des deux lignes divisé par leurs normes.\n",
    "\n",
    "Outre cette mesure de similarité, nous filtrons également nos résultats selon deux autres critères:\n",
    "* si les prix retraités 'price_retreat' ne sont pas nuls, le rapport entre le plus élevé et le plus faible doit au maximum être de 2 (hypothèse que le prix d'un même produit ne peut pas aller du simple au double entre les deux bases);\n",
    "* la 'Version' doit être identique ou l'une des deux doit être 'None' (ie non déterminée), un produit spécifié comme Windows ne pouvant tourner sur Mac.\n",
    "\n",
    "Dans la pratique, la fonction *filtre_tfidf* créée ci-dessous permet, à partir des paramètres *corpus*, *ngram*, *max_df*, *sim_lim* et *stop* :\n",
    "* appliquera la fonction *TfidfVectorizer* au dataframe *corpus* avec comme paramètres la taille des n-grams *ngram*, une proportion des n-grams conservés *max_df* (les plus representatifs), d'éventuels stopwords spécifiques *stop* et en activant le paramètre *sublinear_tf* (en effet, nous souhaitons privilegier la mesure idf à la mesure tf car plus appropriée à notre étude : un terme peu fréquent dans la base corpus doit être privilégié),\n",
    "* calculera la similarité pour les couples où les filtres sur le prix et la version sont vérifiés;\n",
    "* ajoutera les couples pour lesquels la similarité est supérieure à *sim_lim* au dataframe ***matches_df*** ;\n",
    "* calculera precision, recall et f-measure actuels.\n",
    "\n",
    "D'autres part, nous appliquerons une méthode de type *Blocking* selon l'editeur du logiciel. Comme la colonne 'manufacturer' n'est que faiblement remplie dans ***company 2***, nous selectionnerons dans la pratique les 'manufacturer' les plus importants de ***company 1*** et, un par un, ne conservons que les lignes comportant son nom (exemple: toutes les lignes comportant 'adobe', puis toutes les lignes comportant 'encore', etc...). Une fois ceci fait, nous appliquerons la fonction à toutes les lignes n'ayant pas été selectionnée comme faisant partie d'un 'match' (bien qu'il y ait des doublons dans ***company 2***, nous n'avons pas trouvé de critère discriminant permettant de les detecter).\n",
    "\n",
    "Il nous semble important de préciser encore nos choix de paramètres :\n",
    "* *corpus* comme expliqué précédemment sera la concaténation des lignes selectionnées de ***company 1*** et de ***company 2***;\n",
    "* *ngram* :\n",
    "    * pour les gros éditeurs, nous décidons de considérer des n-grams de 1 et 2 mots puis, sur les lignes non matchées sur des n-grams de 1 mot, via une boucle;\n",
    "    * pour le reste de la base, nous décidons d'utiliser egalement une boucle, d'abord sur les  n-grams de 1 à 3 mots puis, sur les lignes non matchées sur des n-grams de 1 et 2 mots, enfin sur des n-grams de 1 mot;\n",
    "* pour *max_df*, , il nous apparaît important de le limiter, particulièrement sur le reste de la base, afin de réduire les temps de calculs matriciels. Nous prenons ainsi 1% sur cette base, et 99% pour les gros éditeurs;\n",
    "* le choix de la similarité minimum *sim_lim* nous apparait critique. Nous avons sélectionné 0.5 pour le reste de la base. Concernant les gros éditeurs, sachant que les corpus comprennent déjà une similarité importante (du fait de concerner un éditeur unique) nous proposons de l'augmenter et de multiplier par 1.2 la valeur seuil soit 0.6;\n",
    "* enfin *stop* n'est utilisé que pour les gros éditeurs où nous supprimons le nom de l'éditeur concerné "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création de la fonction *filtre_tfidf*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtre_tfidf(corpus, ngram, max_df,sim_lim, stop):\n",
    "    global number_of_matches, matches, matches_df\n",
    "    start = time.process_time()\n",
    "    new_number_of_matches = 0\n",
    "    new_matches=[]\n",
    "\n",
    "    \n",
    "    #Application de TfidfVectorizer au corpus\n",
    "    vectorizer = TfidfVectorizer(ngram_range=ngram, max_df=max_df,sublinear_tf=True,stop_words=[stop]) \n",
    "    vectors = vectorizer.fit_transform(corpus['full data'])\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    dense = vectors.todense()\n",
    "\n",
    "    \n",
    "    for i in range(len(company1_light)):\n",
    "        price1 = float(company1_light.iloc[i,6]) \n",
    "        for j in range(len(company2_light)):\n",
    "            price2 = float(company2_light.iloc[j,6]) \n",
    "            #double filtre sur le ratio de prix et sur la version du logiciel\n",
    "            if (price1* price2 == 0 or max(price1, price2)/min(price1, price2)<2) and (company1_light.iloc[i,11]==company2_light.iloc[j,11] or company1_light.iloc[i,11]=='None' or company2_light.iloc[j,11]=='None'):\n",
    "                #calcul de la similarité\n",
    "                try :\n",
    "                    similarity = np.dot(dense[i],np.transpose(dense[len(company1_light)+j])).item(0)/math.sqrt(np.dot(dense[i],np.transpose(dense[i])).item(0) * np.dot(dense[len(company1_light)+j],np.transpose(dense[len(company1_light)+j])).item(0))\n",
    "                except : \n",
    "                    similarity = 0\n",
    "                #test de la similarité\n",
    "                if  similarity > sim_lim:\n",
    "                    new_number_of_matches = new_number_of_matches +1\n",
    "                    new_matches.append((company1_light.iloc[i,0],company2_light.iloc[j,0]))\n",
    "\n",
    "    #remplissage de matches_df\n",
    "    if new_number_of_matches>0:\n",
    "        print(\"New matches: {}\".format(new_number_of_matches))\n",
    "        number_of_matches= number_of_matches + new_number_of_matches\n",
    "        print(\"Total matches: {}\".format(number_of_matches))\n",
    "        if matches== []:\n",
    "            matches = new_matches \n",
    "            matches_df = pd.DataFrame(matches)\n",
    "            matches_df.columns= ['idCompany1','idCompany2']\n",
    "        else:\n",
    "            new_matches_df = pd.DataFrame(new_matches)\n",
    "            new_matches_df.columns= ['idCompany1','idCompany2']\n",
    "            matches_df = pd.concat([matches_df, new_matches_df],sort=False,ignore_index=True).drop_duplicates()\n",
    "        \n",
    "        #calcul des mesures\n",
    "        diff_df = pd.merge(ground_truth_matches, matches_df, how='outer', indicator='Exist')\n",
    "        true_positives = diff_df[diff_df.Exist=='both']\n",
    "        false_positives = diff_df[diff_df.Exist=='right_only']\n",
    "        false_negatives = diff_df[diff_df.Exist=='left_only']\n",
    "        print(\"Number of true positives: {}\".format(len(true_positives)))\n",
    "        print(\"Number of false positives: {}\".format(len(false_positives)))\n",
    "        print(\"Number of false negatives: {}\".format(len(false_negatives)))\n",
    "        precision = len(true_positives)/(len(true_positives)+ len(false_positives))\n",
    "        print(\"Precision: {}\".format(precision))\n",
    "        recall = len(true_positives)/(len(true_positives)+ len(false_negatives))\n",
    "        print(\"Recall: {}\".format(recall))\n",
    "        try :\n",
    "            f_measure = 2*(precision*recall)/(precision+recall)\n",
    "            print(\"F measure: {}\".format(f_measure))\n",
    "        except:\n",
    "            print(\"F measure not calculable\")\n",
    "    else:\n",
    "        print(\"No new match\")\n",
    "    end = time.process_time()\n",
    "    print(\"Processing time: {}\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adobe with ngram=(1,2)\n",
      "New matches: 87\n",
      "Total matches: 87\n",
      "Number of true positives: 50\n",
      "Number of false positives: 37\n",
      "Number of false negatives: 1250\n",
      "Precision: 0.5747126436781609\n",
      "Recall: 0.038461538461538464\n",
      "F measure: 0.07209805335255949\n",
      "Processing time: 0.515625\n",
      " \n",
      "adobe with ngram=(1,1)\n",
      "New matches: 13\n",
      "Total matches: 100\n",
      "Number of true positives: 56\n",
      "Number of false positives: 44\n",
      "Number of false negatives: 1244\n",
      "Precision: 0.56\n",
      "Recall: 0.043076923076923075\n",
      "F measure: 0.07999999999999999\n",
      "Processing time: 0.15625\n",
      " \n",
      "encore with ngram=(1,2)\n",
      "New matches: 131\n",
      "Total matches: 231\n",
      "Number of true positives: 122\n",
      "Number of false positives: 109\n",
      "Number of false negatives: 1178\n",
      "Precision: 0.5281385281385281\n",
      "Recall: 0.09384615384615384\n",
      "F measure: 0.15937295885042455\n",
      "Processing time: 1.140625\n",
      " \n",
      "encore with ngram=(1,1)\n",
      "New matches: 18\n",
      "Total matches: 249\n",
      "Number of true positives: 133\n",
      "Number of false positives: 116\n",
      "Number of false negatives: 1167\n",
      "Precision: 0.5341365461847389\n",
      "Recall: 0.10230769230769231\n",
      "F measure: 0.17172369270497093\n",
      "Processing time: 0.296875\n",
      " \n",
      "topic with ngram=(1,2)\n",
      "New matches: 35\n",
      "Total matches: 284\n",
      "Number of true positives: 160\n",
      "Number of false positives: 124\n",
      "Number of false negatives: 1140\n",
      "Precision: 0.5633802816901409\n",
      "Recall: 0.12307692307692308\n",
      "F measure: 0.20202020202020204\n",
      "Processing time: 0.140625\n",
      " \n",
      "topic with ngram=(1,1)\n",
      "New matches: 1\n",
      "Total matches: 285\n",
      "Number of true positives: 161\n",
      "Number of false positives: 124\n",
      "Number of false negatives: 1139\n",
      "Precision: 0.5649122807017544\n",
      "Recall: 0.12384615384615384\n",
      "F measure: 0.20315457413249213\n",
      "Processing time: 0.046875\n",
      " \n",
      "microsoft with ngram=(1,2)\n",
      "New matches: 36\n",
      "Total matches: 321\n",
      "Number of true positives: 181\n",
      "Number of false positives: 140\n",
      "Number of false negatives: 1119\n",
      "Precision: 0.5638629283489096\n",
      "Recall: 0.13923076923076924\n",
      "F measure: 0.22331893892658852\n",
      "Processing time: 0.484375\n",
      " \n",
      "microsoft with ngram=(1,1)\n",
      "New matches: 15\n",
      "Total matches: 336\n",
      "Number of true positives: 190\n",
      "Number of false positives: 146\n",
      "Number of false negatives: 1110\n",
      "Precision: 0.5654761904761905\n",
      "Recall: 0.14615384615384616\n",
      "F measure: 0.23227383863080683\n",
      "Processing time: 0.3125\n",
      " \n",
      "aspyr with ngram=(1,2)\n",
      "New matches: 18\n",
      "Total matches: 354\n",
      "Number of true positives: 207\n",
      "Number of false positives: 147\n",
      "Number of false negatives: 1093\n",
      "Precision: 0.5847457627118644\n",
      "Recall: 0.15923076923076923\n",
      "F measure: 0.2503022974607013\n",
      "Processing time: 0.0625\n",
      " \n",
      "aspyr with ngram=(1,1)\n",
      "New matches: 5\n",
      "Total matches: 359\n",
      "Number of true positives: 211\n",
      "Number of false positives: 148\n",
      "Number of false negatives: 1089\n",
      "Precision: 0.5877437325905293\n",
      "Recall: 0.16230769230769232\n",
      "F measure: 0.2543701024713683\n",
      "Processing time: 0.015625\n",
      " \n",
      "apple with ngram=(1,2)\n",
      "New matches: 32\n",
      "Total matches: 391\n",
      "Number of true positives: 237\n",
      "Number of false positives: 154\n",
      "Number of false negatives: 1063\n",
      "Precision: 0.6061381074168798\n",
      "Recall: 0.1823076923076923\n",
      "F measure: 0.280307510348906\n",
      "Processing time: 0.09375\n",
      " \n",
      "apple with ngram=(1,1)\n",
      "New matches: 9\n",
      "Total matches: 400\n",
      "Number of true positives: 245\n",
      "Number of false positives: 155\n",
      "Number of false negatives: 1055\n",
      "Precision: 0.6125\n",
      "Recall: 0.18846153846153846\n",
      "F measure: 0.2882352941176471\n",
      "Processing time: 0.046875\n",
      " \n",
      "fogware with ngram=(1,2)\n",
      "New matches: 5\n",
      "Total matches: 405\n",
      "Number of true positives: 250\n",
      "Number of false positives: 155\n",
      "Number of false negatives: 1050\n",
      "Precision: 0.6172839506172839\n",
      "Recall: 0.19230769230769232\n",
      "F measure: 0.2932551319648094\n",
      "Processing time: 0.03125\n",
      " \n",
      "fogware with ngram=(1,1)\n",
      "New matches: 4\n",
      "Total matches: 409\n",
      "Number of true positives: 254\n",
      "Number of false positives: 155\n",
      "Number of false negatives: 1046\n",
      "Precision: 0.6210268948655256\n",
      "Recall: 0.19538461538461538\n",
      "F measure: 0.2972498537156232\n",
      "Processing time: 0.015625\n",
      " \n",
      "intuit with ngram=(1,2)\n",
      "New matches: 10\n",
      "Total matches: 419\n",
      "Number of true positives: 264\n",
      "Number of false positives: 155\n",
      "Number of false negatives: 1036\n",
      "Precision: 0.630071599045346\n",
      "Recall: 0.20307692307692307\n",
      "F measure: 0.30715532286212915\n",
      "Processing time: 0.03125\n",
      " \n",
      "intuit with ngram=(1,1)\n",
      "New matches: 5\n",
      "Total matches: 424\n",
      "Number of true positives: 269\n",
      "Number of false positives: 155\n",
      "Number of false negatives: 1031\n",
      "Precision: 0.6344339622641509\n",
      "Recall: 0.20692307692307693\n",
      "F measure: 0.31206496519721577\n",
      "Processing time: 0.03125\n",
      " \n",
      "punch with ngram=(1,2)\n",
      "New matches: 22\n",
      "Total matches: 446\n",
      "Number of true positives: 278\n",
      "Number of false positives: 168\n",
      "Number of false negatives: 1022\n",
      "Precision: 0.6233183856502242\n",
      "Recall: 0.21384615384615385\n",
      "F measure: 0.3184421534936999\n",
      "Processing time: 0.046875\n",
      " \n",
      "punch with ngram=(1,1)\n",
      "New matches: 1\n",
      "Total matches: 447\n",
      "Number of true positives: 279\n",
      "Number of false positives: 168\n",
      "Number of false negatives: 1021\n",
      "Precision: 0.6241610738255033\n",
      "Recall: 0.21461538461538462\n",
      "F measure: 0.31940469376073266\n",
      "Processing time: 0.03125\n",
      " \n",
      "sony with ngram=(1,2)\n",
      "New matches: 14\n",
      "Total matches: 461\n",
      "Number of true positives: 289\n",
      "Number of false positives: 172\n",
      "Number of false negatives: 1011\n",
      "Precision: 0.6268980477223427\n",
      "Recall: 0.22230769230769232\n",
      "F measure: 0.3282226007950028\n",
      "Processing time: 0.0625\n",
      " \n",
      "sony with ngram=(1,1)\n",
      "New matches: 6\n",
      "Total matches: 467\n",
      "Number of true positives: 291\n",
      "Number of false positives: 176\n",
      "Number of false negatives: 1009\n",
      "Precision: 0.6231263383297645\n",
      "Recall: 0.22384615384615383\n",
      "F measure: 0.3293718166383701\n",
      "Processing time: 0.046875\n",
      " \n",
      "nova with ngram=(1,2)\n",
      "New matches: 11\n",
      "Total matches: 478\n",
      "Number of true positives: 300\n",
      "Number of false positives: 178\n",
      "Number of false negatives: 1000\n",
      "Precision: 0.6276150627615062\n",
      "Recall: 0.23076923076923078\n",
      "F measure: 0.3374578177727784\n",
      "Processing time: 0.03125\n",
      " \n",
      "nova with ngram=(1,1)\n",
      "New matches: 3\n",
      "Total matches: 481\n",
      "Number of true positives: 303\n",
      "Number of false positives: 178\n",
      "Number of false negatives: 997\n",
      "Precision: 0.6299376299376299\n",
      "Recall: 0.23307692307692307\n",
      "F measure: 0.34025828186412127\n",
      "Processing time: 0.03125\n",
      " \n",
      "corel with ngram=(1,2)\n",
      "New matches: 6\n",
      "Total matches: 487\n",
      "Number of true positives: 309\n",
      "Number of false positives: 178\n",
      "Number of false negatives: 991\n",
      "Precision: 0.6344969199178645\n",
      "Recall: 0.2376923076923077\n",
      "F measure: 0.3458310016787913\n",
      "Processing time: 0.03125\n",
      " \n",
      "corel with ngram=(1,1)\n",
      "New matches: 1\n",
      "Total matches: 488\n",
      "Number of true positives: 310\n",
      "Number of false positives: 178\n",
      "Number of false negatives: 990\n",
      "Precision: 0.6352459016393442\n",
      "Recall: 0.23846153846153847\n",
      "F measure: 0.3467561521252796\n",
      "Processing time: 0.046875\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#boucle sur les big manufacturer\n",
    "big_manufacturer_list=[\"adobe\",\"encore\",\"topic\",\"microsoft\",\"aspyr\",\"apple\",\"fogware\",\"intuit\",\"punch\",\"sony\",\"nova\",\"corel\"]\n",
    "number_of_matches = 0\n",
    "matches=[]\n",
    "matches_df=[]\n",
    "\n",
    "for filtre in big_manufacturer_list :\n",
    "    #boucle sur n-grams = (1,2) puis (1,1)\n",
    "    for max_ngram in [2,1]:\n",
    "        #filtre sur les éventuels matchs déjà obtenus\n",
    "        try :\n",
    "            company1_light=company1[~company1.id.isin(matches_df.idCompany1)]\n",
    "            company2_light=company2[~company2.id.isin(matches_df.idCompany2)]    \n",
    "        except : \n",
    "            company1_light=company1\n",
    "            company2_light=company2        \n",
    "        #filtre sur l'éditeur\n",
    "        company1_light = company1_light[company1_light['full data'].str.contains(filtre)].reset_index(drop=True)\n",
    "        company2_light = company2_light[company2_light['full data'].str.contains(filtre)].reset_index(drop=True)\n",
    "        corpus = pd.concat([company1_light, company2_light],sort=False,ignore_index=True)\n",
    "        print(\"{} with ngram=(1,{})\".format(filtre,max_ngram))\n",
    "        filtre_tfidf(corpus, (1,max_ngram), 0.99 ,0.6, filtre)\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other entries with ngram=(1,3)\n",
      "New matches: 485\n",
      "Total matches: 973\n",
      "Number of true positives: 636\n",
      "Number of false positives: 337\n",
      "Number of false negatives: 664\n",
      "Precision: 0.6536485097636177\n",
      "Recall: 0.48923076923076925\n",
      "F measure: 0.559612846458425\n",
      "Processing time: 616.5625\n",
      " \n",
      "Other entries with ngram=(1,2)\n",
      "New matches: 168\n",
      "Total matches: 1141\n",
      "Number of true positives: 751\n",
      "Number of false positives: 390\n",
      "Number of false negatives: 549\n",
      "Precision: 0.6581945661700263\n",
      "Recall: 0.5776923076923077\n",
      "F measure: 0.6153215895124949\n",
      "Processing time: 290.125\n",
      " \n",
      "Other entries with ngram=(1,1)\n",
      "New matches: 300\n",
      "Total matches: 1441\n",
      "Number of true positives: 899\n",
      "Number of false positives: 542\n",
      "Number of false negatives: 401\n",
      "Precision: 0.6238723108952117\n",
      "Recall: 0.6915384615384615\n",
      "F measure: 0.655964976286027\n",
      "Processing time: 35.203125\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#Boucle sur le reste de la base avec n-grams = (1,3) puis (1,2) puis (1,1)\n",
    "for max_ngram in [3,2,1]:\n",
    "    #filtre sur les matchs déjà obtenus\n",
    "    company1_light=company1[~company1.id.isin(matches_df.idCompany1)]\n",
    "    company2_light=company2[~company2.id.isin(matches_df.idCompany2)]    \n",
    "    corpus = pd.concat([company1_light, company2_light],sort=False,ignore_index=True)\n",
    "    print(\"Other entries with ngram=(1,{})\".format(max_ngram))\n",
    "    filtre_tfidf(corpus, (1,max_ngram), 0.01 ,0.5, \"\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Au final, notre méthode nous permet, sur la base complète, d'obtenir une F-measure de 65.60% provenant d'une précision de 62.39% et d'un recall de 69.15%.***\n",
    "\n",
    "En effet, nous avons décidé d'équilibrer relativement precision et recall, ne connaissant pas l'utilisation qui aurait été faite de notre résultat.  \n",
    "Dans le cas où nous aurions dû pénaliser les faux positifs (par exemple, pour des détections d'opportunité d'arbitrage entre les deux compagnies), nous aurions pu augmenter la valeur de *sim_lim*. Au contraire, si l'objectif est de détecter un maximum des doublons (par exemple dans un objectif de ne conserver que des articles 'rares', ie les négatifs restants), nous aurions pu diminuer *sim_lim* et augmenter le ratio maximum des prix à 3."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
