{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We aim in this notebook to identify duplicates in a CSV containing information about restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants = pd.read_csv(\"./restaurants.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe contains duplicates records that represent to the same 'real-world' restanrants. The column 'unique_id' was added for this purpose. Two records that are associated with the same attribute value for unique_id represents the same restaurant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants[df_restaurants.unique_id == '23']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, the two records share the same value for attributes 'name' and 'address'. However, they have slightly different values for the columns 'city' and 'cuisine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants[df_restaurants.unique_id == '22']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, on the other hand, the two records are associated with different names, cities and cuisiones.\n",
    "\n",
    "This file represents a simple example of datasets, on which we can experiment with th etechniques presented in the course to try identify duplicates, without using (that is relying on the values of) the column \"unique_id\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by adding a new column to identify the records (lines) in our dataframe\n",
    "df_restaurants.insert(0,'record_ID', range(0, len(df_restaurants)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exhaustive comparisons: every record is compared with every other record\n",
    "\n",
    "We start by applying an exhaustive strategy whereby every record in the CSV file, is compared with every other record. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below does this for us. In doing so, it uses the following rule:\n",
    "\n",
    "For two records to match, i.e. refer to the same restaurant in the real world:\n",
    "* The edit distance between the attribute name values of the two records needs to be smaller or equal to 3, and \n",
    "* they need to have the same value for the cuisine attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants[df_restaurants.record_ID.isin([43, 622])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = len(df_restaurants)\n",
    "matches = []\n",
    "matchescomplet = []\n",
    "\n",
    "number_of_matches = 0\n",
    "tokens1=[]\n",
    "tokens2=[]\n",
    "start = time.process_time()\n",
    "for i in range(0,num_records):\n",
    "    \n",
    "    # Après tokenization , calcul du ngrams (n=1) pour le name qui servira pour la Jaccard distance, pour la ligne i\n",
    "    tokens1name = nltk.word_tokenize(df_restaurants.iloc[i,1]) \n",
    "    ng1_tokensname = set(nltk.ngrams(tokens1name, n=1))\n",
    "    \n",
    "    # Après tokenization , calcul du ngrams (n=1) pour l'adresse qui servira pour la Jaccard distance,, pour la ligne i\n",
    "    tokens1adr = nltk.word_tokenize(df_restaurants.iloc[i,2]) \n",
    "    ng1_tokensadr = set(nltk.ngrams(tokens1adr, n=1))\n",
    "    \n",
    "    \n",
    "    for j in range(i+1,num_records):\n",
    "        \n",
    "        # Après tokenization , calcul du ngrams (n=1) pour le name qui servira pour la Jaccard distance, , pour la ligne j\n",
    "        tokens2name = nltk.word_tokenize( df_restaurants.iloc[j,1]) \n",
    "        ng2_tokensname = set(nltk.ngrams(tokens2name, n=1))\n",
    "        \n",
    "        # Après tokenization , calcul du ngrams (n=1) pour le name qui servira pour la Jaccard distance, , pour la ligne j\n",
    "        tokens2adr = nltk.word_tokenize( df_restaurants.iloc[j,2]) \n",
    "        ng2_tokensadr = set(nltk.ngrams(tokens2adr, n=1))\n",
    "       \n",
    "        # calcul de la Jaccard distance pour le name entre la ligne i et la ligne j (\"item based\" avec ngrams (n=1)) \n",
    "        jd_ng1_ng2_name = nltk.jaccard_distance(ng1_tokensname, ng2_tokensname)  # jaccard distance entre les ngram=1 des names\n",
    "        \n",
    "        # calcul de la Jaccard distance pour l'adresse entre la ligne i et la ligne j (\"item based\" avec ngrams (n=1)) \n",
    "        jd_ng1_ng2_adr = nltk.jaccard_distance(ng1_tokensadr, ng2_tokensadr)  # jaccard distance entre les ngram=1 des adresses\n",
    "    \n",
    "        # Rule for matching: \n",
    "        # disjonction entre une similarité entre les names (name_score<=1) \n",
    "        # et une similarité conjugée entre les adresses et les noms (jd_ng1_ng2_adr <= 0.6 and jd_ng1_ng2_name <= 0.6)\n",
    "        name_score = nltk.edit_distance(df_restaurants.iloc[i,1], df_restaurants.iloc[j,1])\n",
    "        \n",
    "        # Rule for matching: Distance between names is smaller or equal to 3 and the cuisine is the same \n",
    "        if (jd_ng1_ng2_adr <= 0.6 and jd_ng1_ng2_name <= 0.6) or name_score<=1 :\n",
    "            number_of_matches = number_of_matches +1 \n",
    "            # matchescomplet.append((df_restaurants.iloc[i,0],df_restaurants.iloc[i,1], df_restaurants.iloc[i,2],df_restaurants.iloc[i,5], df_restaurants.iloc[j,0],df_restaurants.iloc[j,1], df_restaurants.iloc[j,2],df_restaurants.iloc[j,5]))\n",
    "            matches.append((df_restaurants.iloc[i,0],df_restaurants.iloc[j,0]))\n",
    "\n",
    "end = time.process_time()\n",
    "\n",
    "print(\"Number of matches: {}\".format(number_of_matches))\n",
    "print(\"Processing time: {}\".format(end - start))\n",
    "for _ in matchescomplet:\n",
    "     print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quelques tests pour ajuster les critères de notre algorithme\n",
    "name_score = nltk.edit_distance(df_restaurants.iloc[73,1], df_restaurants.iloc[763,1])\n",
    "name_score   # 11\n",
    "# comme on le voit ci-dessous, la différence est le mot \"restaurant\", l'edit distance est très importante (11), \n",
    "# on ne peut pas se baser dessus pour dire que c le même resto, il faut qu'on ajoute un critère \"item based\" \n",
    "# en plus du critère edit_distance name_score<=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qq tests pour ajuster les critères de notre alogorithme\n",
    "df_restaurants[df_restaurants.record_ID.isin([73, 763])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_score = nltk.edit_distance(df_restaurants.iloc[32,1], df_restaurants.iloc[759,1])\n",
    "#print(name_score)\n",
    "tokens1 = nltk.word_tokenize(df_restaurants.iloc[73,1]) \n",
    "tokens2 = nltk.word_tokenize( df_restaurants.iloc[763,1]) \n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "ng1_tokens = set(nltk.ngrams(tokens1, n=1))\n",
    "ng2_tokens = set(nltk.ngrams(tokens2, n=1))\n",
    "print(ng1_tokens)\n",
    "print(ng2_tokens)\n",
    "\n",
    "jd_sent_1_2 = nltk.jaccard_distance(ng1_tokens, ng2_tokens)\n",
    "print(jd_sent_1_2)\n",
    "# jd_ng1_ng2_adr <= 0.6,ce seuil de 0.6 suffira dire que les lignes 32 et 759 sont le même restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adresse\n",
    "#print(name_score)\n",
    "tokens1 = nltk.word_tokenize(df_restaurants.iloc[73,2]) \n",
    "tokens2 = nltk.word_tokenize( df_restaurants.iloc[763,2]) \n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "ng1_tokens = set(nltk.ngrams(tokens1, n=1))\n",
    "ng2_tokens = set(nltk.ngrams(tokens2, n=1))\n",
    "print(ng1_tokens)\n",
    "print(ng2_tokens)\n",
    "\n",
    "jd_sent_1_2 = nltk.jaccard_distance(ng1_tokens, ng2_tokens)\n",
    "print(jd_sent_1_2)\n",
    "# ça ne passe pas , mais c pas grave car mettre le seuil à 0.67 va nous rajouter beaucoup de faux positifs \n",
    "# on a testé ce seuil plus élevé de 0.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_score = nltk.edit_distance(df_restaurants.iloc[6,1], df_restaurants.iloc[754,1])\n",
    "name_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_score = nltk.edit_distance(df_restaurants.iloc[32,1], df_restaurants.iloc[759,1])\n",
    "#print(name_score)\n",
    "tokens1 = nltk.word_tokenize(df_restaurants.iloc[6,1]) \n",
    "tokens2 = nltk.word_tokenize( df_restaurants.iloc[754,1]) \n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "ng1_tokens = set(nltk.ngrams(tokens1, n=1))\n",
    "ng2_tokens = set(nltk.ngrams(tokens2, n=1))\n",
    "print(ng1_tokens)\n",
    "print(ng2_tokens)\n",
    "\n",
    "jd_sent_1_2 = nltk.jaccard_distance(ng1_tokens, ng2_tokens)\n",
    "print(jd_sent_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_score = nltk.edit_distance(df_restaurants.iloc[6,1], df_restaurants.iloc[754,1])\n",
    "name_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_score = nltk.edit_distance(df_restaurants.iloc[32,1], df_restaurants.iloc[759,1])\n",
    "#print(name_score)\n",
    "tokens1 = nltk.word_tokenize(df_restaurants.iloc[32,1]) \n",
    "tokens2 = nltk.word_tokenize( df_restaurants.iloc[759,1]) \n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "ng1_tokens = set(nltk.ngrams(tokens1, n=1))\n",
    "ng2_tokens = set(nltk.ngrams(tokens2, n=1))\n",
    "print(ng1_tokens)\n",
    "print(ng2_tokens)\n",
    "\n",
    "jd_sent_1_2 = nltk.jaccard_distance(ng1_tokens, ng2_tokens)\n",
    "print(jd_sent_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "for match in matches:\n",
    "    print(\"The following records {} and {} match\".format(match[0],match[1]))\n",
    "    print(\"The restaurants with the following names {} and {} match.\".format(df_restaurants.iloc[match[0],1],df_restaurants.iloc[match[1],1]))\n",
    "    print(\"The restaurants with the following addresses {} and {} match.\".format(df_restaurants.iloc[match[0],2],df_restaurants.iloc[match[1],2]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the rule applied in the above code is not great. You may want to try other kind of distances, other thresholds, and other rules to identify matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing the quality of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we first need to compute the ground truth (that is the list of correct matches) considering the attribute unique_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches = pd.read_csv(\"./restaurants.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches.insert(0, 'record_ID', range(0, len(ground_truth_matches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches = pd.merge(ground_truth_matches,\n",
    "                                ground_truth_matches,\n",
    "                                on = 'unique_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ground_truth_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches = ground_truth_matches.query('record_ID_x < record_ID_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches = ground_truth_matches[['record_ID_x','record_ID_y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ground_truth_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ground_truth_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(matches))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df.columns= ['record_ID_x','record_ID_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on s'assure que les couples record_ID_x et record_ID_y sont dans le bons sens (record_ID_x < record_ID_y)\n",
    "# comme dans ground_truth\n",
    "matches_df[matches_df['record_ID_x'] >= matches_df['record_ID_y'] ]\n",
    "# 0 lignes trouvées , donc c OK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df = pd.merge(ground_truth_matches, matches_df, how='outer', indicator='Exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives = diff_df[diff_df.Exist=='both']\n",
    "false_positives = diff_df[diff_df.Exist=='right_only']\n",
    "false_negatives = diff_df[diff_df.Exist=='left_only']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# les vrais duplicats que notre algo a pu détecter\n",
    "true_positives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of a true positive\n",
    "df_restaurants[df_restaurants.record_ID.isin(['6','754'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notre algo les a sortis comme restos en double mais c pas vrai\n",
    "false_positives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notre critère de duplicate :\n",
    "# (jd_ng1_ng2_adr <= 0.6 and jd_ng1_ng2_name <= 0.6) or (name_score<=1 and jd_ng1_ng2_adr <= 0.6) \n",
    "# eliminer grace jd_ng1_ng2_adr = 0.6666\n",
    "df_restaurants[df_restaurants.record_ID.isin(['55','56'])]\n",
    "# le name est le même donc l'algo dit que ce le même restaurant alors que ce n'est pas vrai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pareil c pas le même resto alors que l'algo les a retenu comme duplicate\n",
    "# car les names diffèrent d'un seul caractère.\n",
    "df_restaurants[df_restaurants.record_ID.isin(['87','88'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_score<=1\n",
    "name_score = nltk.edit_distance(df_restaurants.iloc[87,1], df_restaurants.iloc[88,1])\n",
    "name_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# les vrais duplicates que l'algo n'a pas détecté\n",
    "false_negatives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants[df_restaurants.record_ID.isin(['32','759'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faux négatif\n",
    "# pour l'algo le 32 et le 759 c'est pas le même restaurant, pourtant c le même\n",
    "# en effet les names diffèrents en lettres et en mots : \n",
    "# name_score > 1 et jd_ng1_ng2_name > 0.6 (ça suffit pour l'algo pour l'éliminer ) et en plus jd_ng1_ng2_adr > 0.6\n",
    "name_score = nltk.edit_distance(df_restaurants.iloc[32,1], df_restaurants.iloc[759,1])\n",
    "name_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # (jd_ng1_ng2_adr <= 0.6) and jd_ng1_ng2_name <= 0.6) or (name_score<=1)\n",
    "    \n",
    "# name_score = nltk.edit_distance(df_restaurants.iloc[32,1], df_restaurants.iloc[759,1])\n",
    "\n",
    "tokens1 = nltk.word_tokenize(df_restaurants.iloc[32,1])   # name\n",
    "tokens2 = nltk.word_tokenize( df_restaurants.iloc[759,1]) \n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "ng1_tokens = set(nltk.ngrams(tokens1, n=1))\n",
    "ng2_tokens = set(nltk.ngrams(tokens2, n=1))\n",
    "print(ng1_tokens)\n",
    "print(ng2_tokens)\n",
    "\n",
    "jd_sent_1_2 = nltk.jaccard_distance(ng1_tokens, ng2_tokens)\n",
    "print(jd_sent_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # (jd_ng1_ng2_adr <= 0.6) and (name_score<=2 or jd_ng1_ng2_name <= 0.67)\n",
    "    \n",
    "# name_score = nltk.edit_distance(df_restaurants.iloc[32,1], df_restaurants.iloc[759,1])\n",
    "#print(name_score)\n",
    "tokens1 = nltk.word_tokenize(df_restaurants.iloc[73,2])   # adresse \n",
    "tokens2 = nltk.word_tokenize( df_restaurants.iloc[763,2]) \n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "ng1_tokens = set(nltk.ngrams(tokens1, n=1))\n",
    "ng2_tokens = set(nltk.ngrams(tokens2, n=1))\n",
    "print(ng1_tokens)\n",
    "print(ng2_tokens)\n",
    "\n",
    "jd_sent_1_2 = nltk.jaccard_distance(ng1_tokens, ng2_tokens)\n",
    "print(jd_sent_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ground_truth_matches))\n",
    "print(len(matches_df))\n",
    "print(len(true_positives) , 'true_positives')\n",
    "print(len(false_positives) ,'false_positives')\n",
    "print(len(false_negatives)  , 'false_negatives')\n",
    "\n",
    "# len(true_positives)  +  len(false_negatives) = len(ground_truth_matches)\n",
    "\n",
    "# len(matches_df)) - len(false_positif) + len(false_negatives)     = ground_truth_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = len(true_positives)/(len(true_positives)+ len(false_positives))\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you are using pyton 2.7 (instead of Python 3), you would need to convert integers to float prior to performing the division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = len(true_positives)/(len(true_positives)+ len(false_negatives))\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_measure = 2*(precision*recall)/(precision+recall)\n",
    "print(f_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windowing (SNM) method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 841 842\n",
    "# qq tests pour choisir sur quel champ on va faire le sort \n",
    "# le sorted name parait intéressant\n",
    "df_restaurants.sort_values(by=['name']).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le tri est fait dans ce qui suit selon le champ \"name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 50   # \n",
    "\n",
    "# tri par name car c ce qui permet d'avoir des resto en double les plus proches possibles \n",
    "\n",
    "df_restaurants= df_restaurants.sort_values(by=['name'])  \n",
    "\n",
    "number_of_matchesw = 0\n",
    "num_records = len(df_restaurants)\n",
    "matchesw = []\n",
    "matchescompletw = []\n",
    "\n",
    "start = time.process_time()\n",
    "for i in range(0,min(window,len(df_restaurants))):\n",
    "    \n",
    "    tokens1name = nltk.word_tokenize(df_restaurants.iloc[i,1]) \n",
    "    ng1_tokensname = set(nltk.ngrams(tokens1name, n=1))\n",
    "    \n",
    "    tokens1adr = nltk.word_tokenize(df_restaurants.iloc[i,2]) \n",
    "    ng1_tokensadr = set(nltk.ngrams(tokens1adr, n=1))\n",
    "    \n",
    "    \n",
    "    for j in range(i+1,min(window,len(df_restaurants))):\n",
    "        tokens2name = nltk.word_tokenize( df_restaurants.iloc[j,1]) \n",
    "        ng2_tokensname = set(nltk.ngrams(tokens2name, n=1))\n",
    "        \n",
    "        \n",
    "        tokens2adr = nltk.word_tokenize( df_restaurants.iloc[j,2]) \n",
    "        ng2_tokensadr = set(nltk.ngrams(tokens2adr, n=1))\n",
    "#         print(tokens1)\n",
    "#         print(tokens2)       \n",
    "        jd_ng1_ng2_name = nltk.jaccard_distance(ng1_tokensname, ng2_tokensname)  # jaccard distance entre les ngram=1 des names\n",
    "        jd_ng1_ng2_adr = nltk.jaccard_distance(ng1_tokensadr, ng2_tokensadr)  # jaccard distance entre les ngram=1 des adresses\n",
    "    \n",
    "        name_score = nltk.edit_distance(df_restaurants.iloc[i,1], df_restaurants.iloc[j,1])\n",
    "        \n",
    "        # Rule for matching: Distance between names is smaller or equal to 3 and the cuisine is the same \n",
    "        if (jd_ng1_ng2_adr <= 0.6 and jd_ng1_ng2_name <= 0.6) or name_score<=1 :\n",
    "            number_of_matchesw = number_of_matchesw +1 \n",
    "            # matchescomplet.append((df_restaurants.iloc[i,0],df_restaurants.iloc[i,1], df_restaurants.iloc[i,2],df_restaurants.iloc[i,5], df_restaurants.iloc[j,0],df_restaurants.iloc[j,1], df_restaurants.iloc[j,2],df_restaurants.iloc[j,5]))\n",
    "            matchesw.append((df_restaurants.iloc[i,0],df_restaurants.iloc[j,0]))\n",
    "            matchescompletw.append((df_restaurants.iloc[i,0],df_restaurants.iloc[i,1], df_restaurants.iloc[i,2],df_restaurants.iloc[i,5], df_restaurants.iloc[j,0],df_restaurants.iloc[j,1], df_restaurants.iloc[j,2],df_restaurants.iloc[j,5]))\n",
    "                     \n",
    "            \n",
    "            \n",
    "for i in range(window,len(df_restaurants)):\n",
    "    \n",
    "    tokens1name = nltk.word_tokenize(df_restaurants.iloc[i,1]) \n",
    "    ng1_tokensname = set(nltk.ngrams(tokens1name, n=1))\n",
    "    \n",
    "    tokens1adr = nltk.word_tokenize(df_restaurants.iloc[i,2]) \n",
    "    ng1_tokensadr = set(nltk.ngrams(tokens1adr, n=1))\n",
    "    \n",
    "    \n",
    "    for j in range(i-window+1,i):\n",
    "        tokens2name = nltk.word_tokenize( df_restaurants.iloc[j,1]) \n",
    "        ng2_tokensname = set(nltk.ngrams(tokens2name, n=1))\n",
    "        \n",
    "        \n",
    "        tokens2adr = nltk.word_tokenize( df_restaurants.iloc[j,2]) \n",
    "        ng2_tokensadr = set(nltk.ngrams(tokens2adr, n=1))\n",
    "     \n",
    "        jd_ng1_ng2_name = nltk.jaccard_distance(ng1_tokensname, ng2_tokensname)  # jaccard distance entre les ngram=1 des names\n",
    "        jd_ng1_ng2_adr = nltk.jaccard_distance(ng1_tokensadr, ng2_tokensadr)  # jaccard distance entre les ngram=1 des adresses\n",
    "    \n",
    "        name_score = nltk.edit_distance(df_restaurants.iloc[i,1], df_restaurants.iloc[j,1])\n",
    "        \n",
    "        # Rule for matching: Distance between names is smaller or equal to 3 and the cuisine is the same \n",
    "        if (jd_ng1_ng2_adr <= 0.6 and jd_ng1_ng2_name <= 0.6) or name_score<=1 :\n",
    "            number_of_matchesw = number_of_matchesw +1 \n",
    "            # matchescomplet.append((df_restaurants.iloc[i,0],df_restaurants.iloc[i,1], df_restaurants.iloc[i,2],df_restaurants.iloc[i,5], df_restaurants.iloc[j,0],df_restaurants.iloc[j,1], df_restaurants.iloc[j,2],df_restaurants.iloc[j,5]))\n",
    "            matchesw.append((df_restaurants.iloc[i,0],df_restaurants.iloc[j,0]))\n",
    "            matchescompletw.append((df_restaurants.iloc[i,0],df_restaurants.iloc[i,1], df_restaurants.iloc[i,2],df_restaurants.iloc[i,5], df_restaurants.iloc[j,0],df_restaurants.iloc[j,1], df_restaurants.iloc[j,2],df_restaurants.iloc[j,5]))\n",
    "            \n",
    "end = time.process_time()\n",
    "\n",
    "print(\"Number of matches: {}\".format(number_of_matchesw))\n",
    "print(\"Processing time: {}\".format(end - start))            \n",
    "for _ in matchescompletw:\n",
    "     print(_)  \n",
    "# for _ in matches:\n",
    "#      print(_)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "for match in matchesw:\n",
    "    print(\"The following records {} and {} match\".format(match[0],match[1]))\n",
    "    print(\"The restaurants with the following names {} and {} match.\".format(df_restaurants.iloc[match[0],1],df_restaurants.iloc[match[1],1]))\n",
    "    print(\"The restaurants with the following addresses {} and {} match.\".format(df_restaurants.iloc[match[0],2],df_restaurants.iloc[match[1],2]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matchesw_df = pd.DataFrame(matchesw)\n",
    "matchesw_df.columns= ['record_ID_x','record_ID_y']\n",
    "\n",
    "matchesw_df['MIN'] = matchesw_df[['record_ID_x','record_ID_y']].min(axis=1)\n",
    "matchesw_df['MAX'] = matchesw_df[['record_ID_x','record_ID_y']].max(axis=1)\n",
    "matchesw_df=matchesw_df[['MIN','MAX']]\n",
    "matchesw_df.columns=['record_ID_x','record_ID_y']\n",
    "matchesw_df\n",
    "\n",
    "\n",
    "diffw_df = pd.merge(ground_truth_matches, matchesw_df, how='outer', indicator='Exist')\n",
    "true_positivesw = diffw_df[diffw_df.Exist=='both']\n",
    "false_positivesw = diffw_df[diffw_df.Exist=='right_only']\n",
    "false_negativesw = diffw_df[diffw_df.Exist=='left_only']\n",
    "precisionw = len(true_positivesw)/(len(true_positivesw)+ len(false_positivesw))\n",
    "print(precisionw)\n",
    "recallw = len(true_positivesw)/(len(true_positivesw)+ len(false_negativesw))\n",
    "print(recallw)\n",
    "f_measurew = 2*(precisionw*recallw)/(precisionw+recallw)\n",
    "print(f_measurew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ground_truth_matches))\n",
    "print(len(matchesw_df))\n",
    "print(len(true_positivesw))\n",
    "print(len(false_positivesw))\n",
    "print(len(false_negativesw))  \n",
    "# len(true_positives)  +  len(false_negatives) = len(ground_truth_matches)\n",
    "# len(matches_df)) - len(false_positif) + len(false_negatives)     = ground_truth_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that in the above code, we do not implement the SNM algorithm in its entirety. In particular, we do not implement the last phase of inferring matches using transitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blocking method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants = pd.read_csv(\"./restaurants.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_restaurants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by adding a new column to identify the records (lines) in our dataframe\n",
    "df_restaurants.insert(0,'record_ID', range(0, len(df_restaurants)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The blocks correspond to resturants that are located in the same citydf_restaurants.loc[df_restaurants['city']==' atlanta']\n",
    "df_restaurants.loc[df_restaurants['city']==' atlanta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants.loc[df_restaurants['city'].str.strip()=='atlanta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on va créer un dict \"df_restov\" des restaurants de chaque ville\n",
    "# pour une clé= ville, la valeur du dict serait égale à un dataframe représentant les restos de cette ville\n",
    "df_restov= {}\n",
    "for ville in df_restaurants['city'].unique():\n",
    "    \n",
    "    df_restov[ville]   = df_restaurants.loc[df_restaurants['city']==ville]\n",
    "    num_records = len(df_restov[ville])\n",
    "    print(ville)   # on affiche la ville\n",
    "    print(num_records) # on affiche le nombre de restos par ville\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on vérifie  pour atlanta que ça marche bien, on a bien le dataframe qu'on voudrait.\n",
    "print(type(df_restov[\" atlanta\"]))\n",
    "print(df_restov[\" atlanta\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restov[\" atlanta\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on testel'algo précédent sur juste un dataframe celui des restos de \" atlanta\"  (avec un espace devant)\n",
    "num_records = len(df_restov[\" atlanta\"])\n",
    "amatches = []\n",
    "amatchescomplet = []\n",
    "\n",
    "anumber_of_matches = 0\n",
    "tokens1=[]\n",
    "tokens2=[]\n",
    "start = time.process_time()\n",
    "for i in range(0,num_records):\n",
    "    \n",
    "    # Après tokenization , calcul du ngrams (n=1) pour le name qui servira pour la Jaccard distance, pour la ligne i\n",
    "    tokens1name = nltk.word_tokenize(df_restov[\" atlanta\"].iloc[i,1]) \n",
    "    ng1_tokensname = set(nltk.ngrams(tokens1name, n=1))\n",
    "    \n",
    "    # Après tokenization , calcul du ngrams (n=1) pour l'adresse qui servira pour la Jaccard distance,, pour la ligne i\n",
    "    tokens1adr = nltk.word_tokenize(df_restov[\" atlanta\"].iloc[i,2]) \n",
    "    ng1_tokensadr = set(nltk.ngrams(tokens1adr, n=1))\n",
    "    \n",
    "    \n",
    "    for j in range(i+1,num_records):\n",
    "        \n",
    "        # Après tokenization , calcul du ngrams (n=1) pour le name qui servira pour la Jaccard distance, , pour la ligne j\n",
    "        tokens2name = nltk.word_tokenize( df_restov[\" atlanta\"].iloc[j,1]) \n",
    "        ng2_tokensname = set(nltk.ngrams(tokens2name, n=1))\n",
    "        \n",
    "        # Après tokenization , calcul du ngrams (n=1) pour le name qui servira pour la Jaccard distance, , pour la ligne j\n",
    "        tokens2adr = nltk.word_tokenize( df_restov[\" atlanta\"].iloc[j,2]) \n",
    "        ng2_tokensadr = set(nltk.ngrams(tokens2adr, n=1))\n",
    "     \n",
    "        # calcul de la Jaccard distance pour le name entre la ligne i et la ligne j (\"item based\" avec ngrams (n=1)) \n",
    "        jd_ng1_ng2_name = nltk.jaccard_distance(ng1_tokensname, ng2_tokensname)\n",
    "        \n",
    "        # calcul de la Jaccard distance pour l'adresse entre la ligne i et la ligne j (\"item based\" avec ngrams (n=1)) \n",
    "        jd_ng1_ng2_adr = nltk.jaccard_distance(ng1_tokensadr, ng2_tokensadr)  \n",
    "    \n",
    "        name_score = nltk.edit_distance(df_restov[\" atlanta\"].iloc[i,1], df_restov[\" atlanta\"].iloc[j,1])\n",
    "        \n",
    "        # Rule for matching: \n",
    "        # disjonction entre une similarité entre les names (name_score<=1) \n",
    "        # et une similarité conjugée entre les adresses et les noms (jd_ng1_ng2_adr <= 0.6 and jd_ng1_ng2_name <= 0.6)\n",
    "        if (jd_ng1_ng2_adr <= 0.6 and jd_ng1_ng2_name <= 0.6) or name_score<=1 :\n",
    "            anumber_of_matches = anumber_of_matches +1 \n",
    "            matchescomplet.append((df_restov[\" atlanta\"].iloc[i,0],df_restov[\" atlanta\"].iloc[i,1], \\\n",
    "            df_restov[\" atlanta\"].iloc[i,2],df_restov[\" atlanta\"].iloc[i,3], df_restov[\" atlanta\"].iloc[i,5], \\\n",
    "            df_restov[\" atlanta\"].iloc[j,0],df_restov[\" atlanta\"].iloc[j,1], df_restov[\" atlanta\"].iloc[j,2], \\\n",
    "            df_restov[\" atlanta\"].iloc[j,3],df_restov[\" atlanta\"].iloc[j,5]))\n",
    "            amatches.append((df_restov[\" atlanta\"].iloc[i,0],df_restov[\" atlanta\"].iloc[j,0]))\n",
    "\n",
    "end = time.process_time()\n",
    "\n",
    "print(\"Number of matches: {}\".format(anumber_of_matches))\n",
    "print(\"Processing time: {}\".format(end - start))\n",
    "for _ in amatchescomplet:\n",
    "     print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nous allons refaire le dict mais en éliminant les espaces saisis avant et après chaque ville\n",
    "# par précaution pour éviter des villes en double\n",
    "# et nous allons imprimer le nombre de restos par ville.\n",
    "\n",
    "df_restov={}\n",
    "cumul= 0\n",
    "# il faut enlever les espaces au début et à la fin de chaque ville dans le dataframe, \n",
    "# sinon on va rater des restos en double car ils ne seront pas dans le même block.\n",
    "\n",
    "for ville in df_restaurants['city'].str.strip().unique():   \n",
    "     print(ville)\n",
    "     df_restov[ville]   = df_restaurants.loc[df_restaurants['city'].str.strip()==ville]\n",
    "     print(len(df_restov[ville]))\n",
    "     cumul += len(df_restov[ville])\n",
    "\n",
    "print(cumul)\n",
    "# on vérifie qu'on retrouve bien un total de 865 restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Généralisation de la BLOCKING METHOD à toutes les villes \n",
    "bmatches = []\n",
    "bmatchescomplet = []\n",
    "bnumber_of_matches = 0\n",
    "start = time.process_time()\n",
    "    \n",
    "for ville in df_restaurants['city'].str.strip().unique():\n",
    "        # affichage de la ville et du nombre de restos par ville\n",
    "        # pour les matcher entre eux\n",
    "        print(ville)  \n",
    "        num_records = len(df_restov[ville])\n",
    "        print(num_records)\n",
    "        \n",
    "        tokens1=[]\n",
    "        tokens2=[]\n",
    "       \n",
    "        for i in range(0,num_records):\n",
    "\n",
    "            tokens1name = nltk.word_tokenize(df_restov[ville].iloc[i,1]) \n",
    "            ng1_tokensname = set(nltk.ngrams(tokens1name, n=1))\n",
    "\n",
    "            tokens1adr = nltk.word_tokenize(df_restov[ville].iloc[i,2]) \n",
    "            ng1_tokensadr = set(nltk.ngrams(tokens1adr, n=1))\n",
    "\n",
    "\n",
    "            for j in range(i+1,num_records):\n",
    "\n",
    "                tokens2name = nltk.word_tokenize( df_restov[ville].iloc[j,1]) \n",
    "                ng2_tokensname = set(nltk.ngrams(tokens2name, n=1))\n",
    "\n",
    "\n",
    "                tokens2adr = nltk.word_tokenize( df_restov[ville].iloc[j,2]) \n",
    "                ng2_tokensadr = set(nltk.ngrams(tokens2adr, n=1))\n",
    "\n",
    "                jd_ng1_ng2_name = nltk.jaccard_distance(ng1_tokensname, ng2_tokensname)  # jaccard distance entre les ngram=1 des names\n",
    "                jd_ng1_ng2_adr = nltk.jaccard_distance(ng1_tokensadr, ng2_tokensadr)  # jaccard distance entre les ngram=1 des adresses\n",
    "\n",
    "                name_score = nltk.edit_distance(df_restov[ville].iloc[i,1], df_restov[ville].iloc[j,1])\n",
    "\n",
    "                # Rule for matching: Item based Jaccard Distance with ngram=1 between adresses and between names or edit distance between names \n",
    "                if (jd_ng1_ng2_adr <= 0.6 and jd_ng1_ng2_name <= 0.6) or name_score<=1 :\n",
    "                    bnumber_of_matches = bnumber_of_matches +1 \n",
    "                    bmatchescomplet.append((df_restov[ville].iloc[i,0],df_restov[ville].iloc[i,1], \\\n",
    "                    df_restov[ville].iloc[i,2],df_restov[ville].iloc[i,3], df_restov[ville].iloc[i,5], \\\n",
    "                    df_restov[ville].iloc[j,0],df_restov[ville].iloc[j,1], df_restov[ville].iloc[j,2], \\\n",
    "                    df_restov[ville].iloc[j,3],df_restov[ville].iloc[j,5]))\n",
    "                    bmatches.append((df_restov[ville].iloc[i,0],df_restov[ville].iloc[j,0]))\n",
    "\n",
    "end = time.process_time()\n",
    "\n",
    "print(\"Number of matches: {}\".format(bnumber_of_matches))\n",
    "print(\"Processing time: {}\".format(end - start))\n",
    "# for _ in matchescomplet:\n",
    "#        print(_)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rappel des résultats de l'algo original sans blocking:\n",
    "####  Number of matches: 127\n",
    "#### Processing time: 167.984375\n",
    "\n",
    "#### les infos de l'algo avec  blocking ci-dessus\n",
    "#### Number of matches: 67\n",
    "#### Processing time: 25.6875\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ground_truth_matches = pd.read_csv(\"./restaurants.csv\")\n",
    "len(ground_truth_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches.insert(0, 'record_ID', range(0, len(ground_truth_matches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches = pd.merge(ground_truth_matches,\n",
    "                                ground_truth_matches,\n",
    "                                on = 'unique_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches = ground_truth_matches.query('record_ID_x < record_ID_y')\n",
    "len(ground_truth_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches = ground_truth_matches[['record_ID_x','record_ID_y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmatches_df = pd.DataFrame(bmatches)\n",
    "bmatches_df.columns= ['record_ID_x','record_ID_y']\n",
    "bmatches_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on s'assure que les couples record_ID_x et record_ID_y sont dans le bons sens (record_ID_x < record_ID_y)\n",
    "bmatches_df[bmatches_df['record_ID_x'] >= bmatches_df['record_ID_y'] ]\n",
    "# 0 lignes trouvées , donc c OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df = pd.merge(ground_truth_matches, bmatches_df, how='outer', indicator='Exist')\n",
    "diff_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btrue_positives = diff_df[diff_df.Exist=='both']\n",
    "bfalse_positives = diff_df[diff_df.Exist=='right_only']\n",
    "bfalse_negatives = diff_df[diff_df.Exist=='left_only']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# un vrai positif: c un vrai couple de restos en double qui a été détecté par notre algo sous forme de blocking method.\n",
    "# en effet il vérifie le critère de name (edit_distance=0) et en plus les 2 restos se trouve dans la même ville d'atlanta.\n",
    "df_restaurants[df_restaurants.record_ID.isin(['6','754'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# les couples détectés par notre algo comme des doubles mais à tort, ce ne sont pas des doubles.\n",
    "false_positives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants[df_restaurants.record_ID.isin(['96','196'])]\n",
    "# ce couple n'est pas dans le ground_truth car unique_id différent\n",
    "# mais il est dans le bmatches_df , (jd_ng1_ng2_adr <= 0.6 and jd_ng1_ng2_name <= 0.6) \n",
    "# cad les names sont proches pour la jaccard distance item based\n",
    "# et les adresses sont proches pour la jaccard distance item based.\n",
    "# et en plus ils se trouvent dans la même ville atlanta (blocking method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# les couples de restos en double mais qui ne sont pas détectés par notre algo comme des doubles.\n",
    "false_negatives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants[df_restaurants.record_ID.isin(['2','753'])]\n",
    "# ce couple est dans le ground_truth car même unique_id \n",
    "# mais il n'est pas dans le matches_df, malgré qu' ils ont le même name et la  même adresse (dans l'algo les détecte bien)\n",
    "# mais le Blocking method ne permet pas à l'algo de les matcher car ils sont considérés ayant des villes différentes :\n",
    "# 'new york' et 'new york city'  , à cause d'une mauvaise saisie de la ville."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bfalse_negatives) # y a beaucoup de false_ngatives par rapport à l'algo dans Blocking method (on avait 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false negative\n",
    "df_restaurants[df_restaurants.record_ID.isin(['26','756'])]\n",
    "# du au blocking method : new yor et new york city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false negative\n",
    "df_restaurants[df_restaurants.record_ID.isin(['32','759'])]\n",
    "# dû aux matching imprécis de l'algo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false negative\n",
    "df_restaurants[df_restaurants.record_ID.isin(['36','760'])]\n",
    "# du au blocking method : new yor et new york city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ground_truth_matches))\n",
    "print(len(bmatches_df))\n",
    "print(len(btrue_positives) , 'true_positives')\n",
    "print(len(bfalse_positives) ,'false_positives')\n",
    "print(len(bfalse_negatives)  , 'false_negatives')\n",
    "\n",
    "# len(true_positives)  +  len(false_negatives) = len(ground_truth_matches)\n",
    "\n",
    "# len(matches_df)) - len(false_positif) + len(false_negatives)     = ground_truth_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bprecision = len(btrue_positives)/(len(btrue_positives)+ len(bfalse_positives))\n",
    "print(bprecision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brecall = len(btrue_positives)/(len(btrue_positives)+ len(bfalse_negatives))\n",
    "print(brecall)\n",
    "# recall faible car y a beaucoup de false negatives\n",
    "# y a des duplicates que l'algo avec Blocking method n'a pas détecté car saisie à tort dans des villes différentes\n",
    "# surtout new york et new york city "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_measure = 2*(bprecision*brecall)/(bprecision+brecall)\n",
    "print(bf_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chiffres de l'algo original sans blocking method\n",
    "### precision: 0.7795\n",
    "\n",
    "### recall : 0.8839\n",
    "\n",
    "### f_measure :0.82845"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
