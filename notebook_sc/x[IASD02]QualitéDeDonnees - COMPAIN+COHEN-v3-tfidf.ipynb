{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini projet Qualité de Données : Détections des doublons\n",
    "## ***Christophe COMPAIN / Sander COHEN***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectif et Données Disponibles\n",
    "L'objectif du projet est d'identifier les logiciels vendus sur les deux plateformes.\n",
    "\n",
    "Pour ce faire, nous disposons des données pour chacune des plateformes isolément, respectivement dans les fichiers ***Company1.csv*** et ***Company2.csv***. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages, Variables Globales et import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\scohe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\scohe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import time\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:\\\\OneDrive - Université Paris-Dauphine\\\\Bureau\\\\Cours Master\\\\12-Qualité de Données\\\\\\Projet\\\\mini-projet\\\\\"\n",
    "file1= \"Data\\\\Company1.csv\" ##\"Data\\\\Company1.csv\" #\"SampleData\\\\Sample_Company1.csv\"\n",
    "file2= \"Data\\\\Company2.csv\" ##\"Data\\\\Company2.csv\" #\"SampleData\\\\Sample_Company2.csv\"\n",
    "real= \"Data\\\\Ground_truth_mappings.csv\" ##\"Data\\\\Ground_truth_mappings.csv\" #\"SampleData\\\\Sample_Groud_truth_mappings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "company1 = pd.read_csv(path+file1, encoding = \"ISO-8859-1\")\n",
    "company2 = pd.read_csv(path+file2, encoding = \"ISO-8859-1\")\n",
    "ground_truth_matches = pd.read_csv(path+real, encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>b000jz4hqo</td>\n",
       "      <td>clickart 950 000 - premier image pack (dvd-rom)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>broderbund</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>b0006zf55o</td>\n",
       "      <td>ca international - arcserve lap/desktop oem 30pk</td>\n",
       "      <td>oem arcserve backup v11.1 win 30u for laptops ...</td>\n",
       "      <td>computer associates</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>b00004tkvy</td>\n",
       "      <td>noah's ark activity center (jewel case ages 3-8)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>victory multimedia</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>b000g80lqo</td>\n",
       "      <td>peachtree by sage premium accounting for nonpr...</td>\n",
       "      <td>peachtree premium accounting for nonprofits 20...</td>\n",
       "      <td>sage software</td>\n",
       "      <td>599.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>b0006se5bq</td>\n",
       "      <td>singing coach unlimited</td>\n",
       "      <td>singing coach unlimited - electronic learning ...</td>\n",
       "      <td>carry-a-tune technologies</td>\n",
       "      <td>99.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              title  \\\n",
       "0  b000jz4hqo    clickart 950 000 - premier image pack (dvd-rom)   \n",
       "1  b0006zf55o   ca international - arcserve lap/desktop oem 30pk   \n",
       "2  b00004tkvy   noah's ark activity center (jewel case ages 3-8)   \n",
       "3  b000g80lqo  peachtree by sage premium accounting for nonpr...   \n",
       "4  b0006se5bq                            singing coach unlimited   \n",
       "\n",
       "                                         description  \\\n",
       "0                                                NaN   \n",
       "1  oem arcserve backup v11.1 win 30u for laptops ...   \n",
       "2                                                NaN   \n",
       "3  peachtree premium accounting for nonprofits 20...   \n",
       "4  singing coach unlimited - electronic learning ...   \n",
       "\n",
       "                manufacturer   price  \n",
       "0                 broderbund    0.00  \n",
       "1        computer associates    0.00  \n",
       "2         victory multimedia    0.00  \n",
       "3              sage software  599.99  \n",
       "4  carry-a-tune technologies   99.99  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>11125907881740407428</td>\n",
       "      <td>learning quickbooks 2007</td>\n",
       "      <td>learning quickbooks 2007</td>\n",
       "      <td>intuit</td>\n",
       "      <td>38.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>11538923464407758599</td>\n",
       "      <td>superstart! fun with reading &amp; writing!</td>\n",
       "      <td>fun with reading &amp; writing! is designed to hel...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>11343515411965421256</td>\n",
       "      <td>qb pos 6.0 basic software</td>\n",
       "      <td>qb pos 6.0 basic retail mngmt software. for re...</td>\n",
       "      <td>intuit</td>\n",
       "      <td>637.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>12049235575237146821</td>\n",
       "      <td>math missions: the amazing arcade adventure (g...</td>\n",
       "      <td>save spectacle city by disrupting randall unde...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>12244614697089679523</td>\n",
       "      <td>production prem cs3 mac upgrad</td>\n",
       "      <td>adobe cs3 production premium mac upgrade from ...</td>\n",
       "      <td>adobe software</td>\n",
       "      <td>805.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               name  \\\n",
       "0  11125907881740407428                           learning quickbooks 2007   \n",
       "1  11538923464407758599            superstart! fun with reading & writing!   \n",
       "2  11343515411965421256                          qb pos 6.0 basic software   \n",
       "3  12049235575237146821  math missions: the amazing arcade adventure (g...   \n",
       "4  12244614697089679523                     production prem cs3 mac upgrad   \n",
       "\n",
       "                                         description    manufacturer   price  \n",
       "0                           learning quickbooks 2007          intuit   38.99  \n",
       "1  fun with reading & writing! is designed to hel...             NaN    8.49  \n",
       "2  qb pos 6.0 basic retail mngmt software. for re...          intuit  637.99  \n",
       "3  save spectacle city by disrupting randall unde...             NaN   12.95  \n",
       "4  adobe cs3 production premium mac upgrade from ...  adobe software  805.99  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idCompany1</th>\n",
       "      <th>idCompany2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>b000jz4hqo</td>\n",
       "      <td>18441480711193821750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>b00004tkvy</td>\n",
       "      <td>18441110047404795849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>b000g80lqo</td>\n",
       "      <td>18441188461196475272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>b0006se5bq</td>\n",
       "      <td>18428750969726461849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>b00021xhzw</td>\n",
       "      <td>18430621475529168165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idCompany1            idCompany2\n",
       "0  b000jz4hqo  18441480711193821750\n",
       "1  b00004tkvy  18441110047404795849\n",
       "2  b000g80lqo  18441188461196475272\n",
       "3  b0006se5bq  18428750969726461849\n",
       "4  b00021xhzw  18430621475529168165"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_matches.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation d'un premier duplicat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>b00004tkvy</td>\n",
       "      <td>noah's ark activity center (jewel case ages 3-8)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>victory multimedia</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                             title description  \\\n",
       "2  b00004tkvy  noah's ark activity center (jewel case ages 3-8)         NaN   \n",
       "\n",
       "         manufacturer  price  \n",
       "2  victory multimedia    0.0  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company1[company1.id == ground_truth_matches.idCompany1[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1881</td>\n",
       "      <td>18441110047404795849</td>\n",
       "      <td>the beginners bible: noah's ark activity cente...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                               name  \\\n",
       "1881  18441110047404795849  the beginners bible: noah's ark activity cente...   \n",
       "\n",
       "     description manufacturer price  \n",
       "1881         NaN          NaN  9.95  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company2[company2.id == ground_truth_matches.idCompany2[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words('english'))  \n",
    "stop_words.update(\"r\")\n",
    "\n",
    "def prep(texte):\n",
    "    #suppression des caracteres non alphanumériques + tout en minuscule\n",
    "    texte = re.sub(\"[^a-zA-Z0-9_]\", \" \",str(texte)).lower()\n",
    "    #tokenization par mot\n",
    "    tokens = nltk.word_tokenize(texte)\n",
    "    #supreesion des stopwords\n",
    "    filtered_tokens = [w for w in tokens if not w in stop_words]\n",
    "#    # Stemming\n",
    "#    texte = [nltk.stem.SnowballStemmer('english').stem(w) for w in filtered_tokens]\n",
    "    # Lemmatization\n",
    "    texte = [nltk.stem.WordNetLemmatizer().lemmatize(w) for w in filtered_tokens]\n",
    "    #remise sous forme d'une string\n",
    "    return \" \".join(texte)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>price</th>\n",
       "      <th>Company</th>\n",
       "      <th>full data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4584</td>\n",
       "      <td>14872602878188858026</td>\n",
       "      <td>jumpstart(r) advanced 1st grade</td>\n",
       "      <td>prepare your child for the 1st grade and beyon...</td>\n",
       "      <td></td>\n",
       "      <td>19.99</td>\n",
       "      <td>company2</td>\n",
       "      <td>jumpstart advanced 1st grade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4585</td>\n",
       "      <td>14916162814320983138</td>\n",
       "      <td>ibm(r) viavoice(r) advanced edition 10</td>\n",
       "      <td>ibm viavoice advanced edition release 10 is a ...</td>\n",
       "      <td></td>\n",
       "      <td>78.95</td>\n",
       "      <td>company2</td>\n",
       "      <td>ibm viavoice advanced edition 10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4586</td>\n",
       "      <td>14974113209571399013</td>\n",
       "      <td>xbox 360: gears of war</td>\n",
       "      <td>as marcus fenix you fight a war against the im...</td>\n",
       "      <td></td>\n",
       "      <td>59.99</td>\n",
       "      <td>company2</td>\n",
       "      <td>xbox 360 gear war</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4587</td>\n",
       "      <td>14986935400648190776</td>\n",
       "      <td>documents to go premium 7.0</td>\n",
       "      <td>this pda software enables you to use your docu...</td>\n",
       "      <td></td>\n",
       "      <td>49.99</td>\n",
       "      <td>company2</td>\n",
       "      <td>document go premium 7 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4588</td>\n",
       "      <td>14996991014087320062</td>\n",
       "      <td>microsoft(r) picture it! digital image pro 9.0</td>\n",
       "      <td>picture it! digital image pro puts you in cont...</td>\n",
       "      <td></td>\n",
       "      <td>99.87</td>\n",
       "      <td>company2</td>\n",
       "      <td>microsoft picture digital image pro 9 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                            name  \\\n",
       "4584  14872602878188858026                 jumpstart(r) advanced 1st grade   \n",
       "4585  14916162814320983138          ibm(r) viavoice(r) advanced edition 10   \n",
       "4586  14974113209571399013                          xbox 360: gears of war   \n",
       "4587  14986935400648190776                     documents to go premium 7.0   \n",
       "4588  14996991014087320062  microsoft(r) picture it! digital image pro 9.0   \n",
       "\n",
       "                                            description manufacturer  price  \\\n",
       "4584  prepare your child for the 1st grade and beyon...               19.99   \n",
       "4585  ibm viavoice advanced edition release 10 is a ...               78.95   \n",
       "4586  as marcus fenix you fight a war against the im...               59.99   \n",
       "4587  this pda software enables you to use your docu...               49.99   \n",
       "4588  picture it! digital image pro puts you in cont...               99.87   \n",
       "\n",
       "       Company                                 full data  \n",
       "4584  company2              jumpstart advanced 1st grade  \n",
       "4585  company2          ibm viavoice advanced edition 10  \n",
       "4586  company2                         xbox 360 gear war  \n",
       "4587  company2                   document go premium 7 0  \n",
       "4588  company2   microsoft picture digital image pro 9 0  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company1['Company']=\"company1\"\n",
    "company1=company1.rename(columns={\"title\": \"name\"})\n",
    "company2['Company']=\"company2\"\n",
    "corpus = pd.concat([company1, company2],sort=False,ignore_index=True)\n",
    "corpus['name'] = corpus['name'].fillna(' ')\n",
    "corpus['manufacturer'] = corpus['manufacturer'].fillna(' ')\n",
    "corpus['description'] = corpus['description'].fillna(' ')\n",
    "corpus['full data']=corpus['manufacturer'].apply(prep) + ' ' + corpus['name'].apply(prep) # + ' ' + corpus['description'].apply(prep)#corpus['manufacturer'] + ' ' + corpus['name'] + ' ' + corpus['description']\n",
    "#corpus.reset_index(drop=True)\n",
    "len(corpus)\n",
    "corpus.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       broderbund clickart 950 000 premier image pack...\n",
       "1       computer associate ca international arcserve l...\n",
       "2       victory multimedia noah ark activity center je...\n",
       "3       sage software peachtree sage premium accountin...\n",
       "4           carry tune technology singing coach unlimited\n",
       "                              ...                        \n",
       "4584                         jumpstart advanced 1st grade\n",
       "4585                     ibm viavoice advanced edition 10\n",
       "4586                                    xbox 360 gear war\n",
       "4587                              document go premium 7 0\n",
       "4588              microsoft picture digital image pro 9 0\n",
       "Name: full data, Length: 4589, dtype: object"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus['full data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_df=0.10) #ngram_range=(1),\n",
    "vectors = vectorizer.fit_transform(corpus['full data'])\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "#denselist = dense.tolist()\n",
    "#df = pd.DataFrame(denselist, columns=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4589, 18717)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feature_names\n",
    "dense.shape #.head() 4589 rows × 12391 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 1590\n",
      "Number of true positives: 749\n",
      "Number of false positives: 841\n",
      "Number of false negatives: 551\n",
      "Precision: 0.4710691823899371\n",
      "Recall: 0.5761538461538461\n",
      "F measure: 0.5183391003460208\n"
     ]
    }
   ],
   "source": [
    "##tfidf\n",
    "number_of_matches = 0\n",
    "matches=[]\n",
    "for i in range(len(company1)):\n",
    "    try :  \n",
    "        price1 = float(company1.iloc[i,4]) \n",
    "    except : \n",
    "        price1 = 0\n",
    "    for j in range(len(company2)):\n",
    "        try :  \n",
    "            price2 = float(company2.iloc[j,4]) \n",
    "        except : \n",
    "            price2 = 0\n",
    "        if price1* price2 == 0:\n",
    "            price_ratio=1\n",
    "        else:\n",
    "            price_ratio =max(price1, price2)/min(price1, price2)\n",
    "        similarity = np.dot(dense[i],np.transpose(dense[len(company1)+j])).item(0)\n",
    "        if ((similarity > 0.5) and (price_ratio<2)):# or name_score<=1) :\n",
    "            number_of_matches = number_of_matches +1\n",
    "            matches.append((company1.iloc[i,0],company2.iloc[j,0]))\n",
    "print(\"Number of matches: {}\".format(number_of_matches))\n",
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df.columns= ['idCompany1','idCompany2']\n",
    "diff_df = pd.merge(ground_truth_matches, matches_df, how='outer', indicator='Exist')\n",
    "true_positives = diff_df[diff_df.Exist=='both']\n",
    "false_positives = diff_df[diff_df.Exist=='right_only']\n",
    "false_negatives = diff_df[diff_df.Exist=='left_only']\n",
    "print(\"Number of true positives: {}\".format(len(true_positives)))\n",
    "print(\"Number of false positives: {}\".format(len(false_positives)))\n",
    "print(\"Number of false negatives: {}\".format(len(false_negatives)))\n",
    "precision = len(true_positives)/(len(true_positives)+ len(false_positives))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "recall = len(true_positives)/(len(true_positives)+ len(false_negatives))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "f_measure = 2*(precision*recall)/(precision+recall)\n",
    "print(\"F measure: {}\".format(f_measure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'company1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-580cf73f04ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcompany1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcompany1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mfalse_negatives\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'company1' is not defined"
     ]
    }
   ],
   "source": [
    "f=10\n",
    "company1[company1['id']==false_negatives.iloc[f,0]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>price</th>\n",
       "      <th>Company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3207</td>\n",
       "      <td>14691100460127614059</td>\n",
       "      <td>family tree maker 16 collectors edition</td>\n",
       "      <td>build and complete your family tree with the f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.99</td>\n",
       "      <td>company2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                     name  \\\n",
       "3207  14691100460127614059  family tree maker 16 collectors edition   \n",
       "\n",
       "                                            description manufacturer  price  \\\n",
       "3207  build and complete your family tree with the f...          NaN  99.99   \n",
       "\n",
       "       Company  \n",
       "3207  company2  "
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company2[company2['id']==false_negatives.iloc[f,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,5), max_df=0.10) #ngram_range=(1),\n",
    "vectors = vectorizer.fit_transform(corpus['full data'])\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "#denselist = dense.tolist()\n",
    "#df = pd.DataFrame(denselist, columns=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4589, 62777)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feature_names\n",
    "dense.shape #.head() 4589 rows × 12391 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 189\n",
      "Number of true positives: 147\n",
      "Number of false positives: 42\n",
      "Number of false negatives: 1153\n",
      "Precision: 0.7777777777777778\n",
      "Recall: 0.11307692307692307\n",
      "F measure: 0.19744795164539958\n"
     ]
    }
   ],
   "source": [
    "##tfidf\n",
    "number_of_matches = 0\n",
    "matches=[]\n",
    "for i in range(len(company1)):\n",
    "    try :  \n",
    "        price1 = float(company1.iloc[i,4]) \n",
    "    except : \n",
    "        price1 = 0\n",
    "    for j in range(len(company2)):\n",
    "        try :  \n",
    "            price2 = float(company2.iloc[j,4]) \n",
    "        except : \n",
    "            price2 = 0\n",
    "        if price1* price2 == 0:\n",
    "            price_ratio=1\n",
    "        else:\n",
    "            price_ratio =max(price1, price2)/min(price1, price2)\n",
    "        similarity = np.dot(dense[i],np.transpose(dense[len(company1)+j])).item(0)\n",
    "        if ((similarity > 0.7) and (price_ratio<2)):# or name_score<=1) :\n",
    "            number_of_matches = number_of_matches +1\n",
    "            matches.append((company1.iloc[i,0],company2.iloc[j,0]))\n",
    "print(\"Number of matches: {}\".format(number_of_matches))\n",
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df.columns= ['idCompany1','idCompany2']\n",
    "diff_df = pd.merge(ground_truth_matches, matches_df, how='outer', indicator='Exist')\n",
    "true_positives = diff_df[diff_df.Exist=='both']\n",
    "false_positives = diff_df[diff_df.Exist=='right_only']\n",
    "false_negatives = diff_df[diff_df.Exist=='left_only']\n",
    "print(\"Number of true positives: {}\".format(len(true_positives)))\n",
    "print(\"Number of false positives: {}\".format(len(false_positives)))\n",
    "print(\"Number of false negatives: {}\".format(len(false_negatives)))\n",
    "precision = len(true_positives)/(len(true_positives)+ len(false_positives))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "recall = len(true_positives)/(len(true_positives)+ len(false_negatives))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "f_measure = 2*(precision*recall)/(precision+recall)\n",
    "print(\"F measure: {}\".format(f_measure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_df=0.10) #ngram_range=(1),\n",
    "vectors = vectorizer.fit_transform(corpus['full data'])\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "#denselist = dense.tolist()\n",
    "#df = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = TfidfVectorizer(ngram_range=(3,5), max_df=0.10) #ngram_range=(1),\n",
    "vectors2 = vectorizer.fit_transform(corpus['full data'])\n",
    "feature_names2 = vectorizer.get_feature_names()\n",
    "dense2 = vectors.todense()\n",
    "#denselist = dense.tolist()\n",
    "#df = pd.DataFrame(denselist, columns=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 931\n",
      "Number of true positives: 545\n",
      "Number of false positives: 386\n",
      "Number of false negatives: 755\n",
      "Precision: 0.5853920515574651\n",
      "Recall: 0.41923076923076924\n",
      "F measure: 0.48857014791573283\n"
     ]
    }
   ],
   "source": [
    "##tfidf\n",
    "number_of_matches = 0\n",
    "matches=[]\n",
    "for i in range(len(company1)):\n",
    "    try :  \n",
    "        price1 = float(company1.iloc[i,4]) \n",
    "    except : \n",
    "        price1 = 0\n",
    "    for j in range(len(company2)):\n",
    "        try :  \n",
    "            price2 = float(company2.iloc[j,4]) \n",
    "        except : \n",
    "            price2 = 0\n",
    "        if price1* price2 == 0:\n",
    "            price_ratio=1\n",
    "        else:\n",
    "            price_ratio =max(price1, price2)/min(price1, price2)\n",
    "        similarity = np.dot(dense[i],np.transpose(dense[len(company1)+j])).item(0)\n",
    "        similarity2 = np.dot(dense2[i],np.transpose(dense2[len(company1)+j])).item(0)\n",
    "        if ((max(similarity,similarity2) > 0.6) and (price_ratio<2)):# or name_score<=1) :\n",
    "            number_of_matches = number_of_matches +1\n",
    "            matches.append((company1.iloc[i,0],company2.iloc[j,0]))\n",
    "print(\"Number of matches: {}\".format(number_of_matches))\n",
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df.columns= ['idCompany1','idCompany2']\n",
    "diff_df = pd.merge(ground_truth_matches, matches_df, how='outer', indicator='Exist')\n",
    "true_positives = diff_df[diff_df.Exist=='both']\n",
    "false_positives = diff_df[diff_df.Exist=='right_only']\n",
    "false_negatives = diff_df[diff_df.Exist=='left_only']\n",
    "print(\"Number of true positives: {}\".format(len(true_positives)))\n",
    "print(\"Number of false positives: {}\".format(len(false_positives)))\n",
    "print(\"Number of false negatives: {}\".format(len(false_negatives)))\n",
    "precision = len(true_positives)/(len(true_positives)+ len(false_positives))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "recall = len(true_positives)/(len(true_positives)+ len(false_negatives))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "f_measure = 2*(precision*recall)/(precision+recall)\n",
    "print(\"F measure: {}\".format(f_measure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "company1['clean_title']=company1['title'].apply(prep)\n",
    "company1['clean_descr']=company1['description'].apply(prep)\n",
    "company2['clean_name']=company2['name'].apply(prep)\n",
    "company2['clean_descr']=company2['description'].apply(prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>price</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_descr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>b00004tkvy</td>\n",
       "      <td>noah's ark activity center (jewel case ages 3-8)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>victory multimedia</td>\n",
       "      <td>0.0</td>\n",
       "      <td>noah ark activ center jewel case age 3 8</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                             title description  \\\n",
       "2  b00004tkvy  noah's ark activity center (jewel case ages 3-8)         NaN   \n",
       "\n",
       "         manufacturer  price                               clean_title  \\\n",
       "2  victory multimedia    0.0  noah ark activ center jewel case age 3 8   \n",
       "\n",
       "  clean_descr  \n",
       "2         nan  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company1[company1.id == ground_truth_matches.idCompany1[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 5980\n",
      "Number of true positives: 1045\n",
      "Number of false positives: 4935\n",
      "Number of false negatives: 255\n",
      "Precision: 0.17474916387959866\n",
      "Recall: 0.8038461538461539\n",
      "F measure: 0.2870879120879121\n"
     ]
    }
   ],
   "source": [
    "##Methode Christophe\n",
    "number_of_matches = 0\n",
    "matches=[]\n",
    "for i in range(len(company1)):\n",
    "    tokens1name = nltk.word_tokenize(company1.iloc[i,5])\n",
    "    ng1_tokensname = set(nltk.ngrams(tokens1name, n=1))\n",
    "    #print(ng1_tokensname)\n",
    "    for j in range(len(company2)):\n",
    "        tokens2name = nltk.word_tokenize(company2.iloc[j,5])\n",
    "        ng2_tokensname = set(nltk.ngrams(tokens2name, n=1))\n",
    "        jd_ng1_ng2_name = nltk.jaccard_distance(ng1_tokensname, ng2_tokensname)\n",
    "        name_score = nltk.edit_distance(company1.iloc[i,5], company2.iloc[j,5])\n",
    "        )if ((jd_ng1_ng2_name <= 0.7)):# or name_score<=1) :\n",
    "            number_of_matches = number_of_matches +1\n",
    "            matches.append((company1.iloc[i,0],company2.iloc[j,0]))\n",
    "print(\"Number of matches: {}\".format(number_of_matches))\n",
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df.columns= ['idCompany1','idCompany2']\n",
    "diff_df = pd.merge(ground_truth_matches, matches_df, how='outer', indicator='Exist')\n",
    "true_positives = diff_df[diff_df.Exist=='both']\n",
    "false_positives = diff_df[diff_df.Exist=='right_only']\n",
    "false_negatives = diff_df[diff_df.Exist=='left_only']\n",
    "print(\"Number of true positives: {}\".format(len(true_positives)))\n",
    "print(\"Number of false positives: {}\".format(len(false_positives)))\n",
    "print(\"Number of false negatives: {}\".format(len(false_negatives)))\n",
    "precision = len(true_positives)/(len(true_positives)+ len(false_positives))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "recall = len(true_positives)/(len(true_positives)+ len(false_negatives))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "f_measure = 2*(precision*recall)/(precision+recall)\n",
    "print(\"F measure: {}\".format(f_measure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 97\n",
      "Number of true positives: 79\n",
      "Number of false positives: 18\n",
      "Number of false negatives: 27\n",
      "Precision: 0.8144329896907216\n",
      "Recall: 0.7452830188679245\n",
      "F measure: 0.7783251231527093\n"
     ]
    }
   ],
   "source": [
    "##Methode 2 : jaccard distance en mixant nom et manufacturer\n",
    "number_of_matches = 0\n",
    "matches=[]\n",
    "for i in range(len(company1)):\n",
    "    tokens1name = nltk.word_tokenize(company1.iloc[i,5] + ' ' + str(company1.iloc[i,3]))\n",
    "    ng1_tokensname = set(nltk.ngrams(tokens1name, n=1))\n",
    "    #print(ng1_tokensname)\n",
    "    for j in range(len(company2)):\n",
    "        tokens2name = nltk.word_tokenize(company2.iloc[j,5] + ' ' + str(company2.iloc[j,3]))\n",
    "        ng2_tokensname = set(nltk.ngrams(tokens2name, n=1))\n",
    "        jd_ng1_ng2_name = nltk.jaccard_distance(ng1_tokensname, ng2_tokensname)\n",
    "        name_score = nltk.edit_distance(company1.iloc[i,5] , company2.iloc[j,5] )\n",
    "        if (jd_ng1_ng2_name <= 0.75) : #or name_score<=1 :\n",
    "            number_of_matches = number_of_matches +1\n",
    "            matches.append((company1.iloc[i,0],company2.iloc[j,0]))\n",
    "print(\"Number of matches: {}\".format(number_of_matches))\n",
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df.columns= ['idCompany1','idCompany2']\n",
    "diff_df = pd.merge(ground_truth_matches, matches_df, how='outer', indicator='Exist')\n",
    "true_positives = diff_df[diff_df.Exist=='both']\n",
    "false_positives = diff_df[diff_df.Exist=='right_only']\n",
    "false_negatives = diff_df[diff_df.Exist=='left_only']\n",
    "print(\"Number of true positives: {}\".format(len(true_positives)))\n",
    "print(\"Number of false positives: {}\".format(len(false_positives)))\n",
    "print(\"Number of false negatives: {}\".format(len(false_negatives)))\n",
    "precision = len(true_positives)/(len(true_positives)+ len(false_positives))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "recall = len(true_positives)/(len(true_positives)+ len(false_negatives))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "f_measure = 2*(precision*recall)/(precision+recall)\n",
    "print(\"F measure: {}\".format(f_measure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1,3))\n",
    "transformed_descrip1 = tfidf.fit_transform(company1['clean_descr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_descrip1_as_array = transformed_descrip1.toarray()\n",
    "len(transformed_descrip1_as_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '000 clipart', '000 clipart digit', '000 hand', '000 hand select', '000 headlin', '000 headlin images2', '000 imag', '000 imag mean', '000 industri', '000 industri profil', '000 opentyp', '000 opentyp font', '000 per', '000 per sag', '000 photo', '000 photo easi', '000 royalti', '000 royalti free', '000 select']\n"
     ]
    }
   ],
   "source": [
    "feature_names1=tfidf.get_feature_names()\n",
    "print(feature_names1[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 38\n",
      "Number of true positives: 38\n",
      "Number of false positives: 0\n",
      "Number of false negatives: 64\n",
      "Precision: 1.0\n",
      "Recall: 0.37254901960784315\n",
      "F measure: 0.5428571428571428\n"
     ]
    }
   ],
   "source": [
    "##Methode 3 : idem avec des n-grams de 2\n",
    "number_of_matches = 0\n",
    "matches=[]\n",
    "for i in range(len(company1)):\n",
    "    tokens1name = nltk.word_tokenize(company1.iloc[i,1] + ' ' + str(company1.iloc[i,3]))\n",
    "    ng1_tokensname = set(nltk.ngrams(tokens1name, n=2))\n",
    "    #print(ng1_tokensname)\n",
    "    for j in range(len(company2)):\n",
    "        tokens2name = nltk.word_tokenize(company2.iloc[j,1] + ' ' + str(company2.iloc[j,3]))\n",
    "        ng2_tokensname = set(nltk.ngrams(tokens2name, n=2))\n",
    "        jd_ng1_ng2_name = nltk.jaccard_distance(ng1_tokensname, ng2_tokensname)\n",
    "        name_score = nltk.edit_distance(company1.iloc[i,1] , company2.iloc[j,1] )\n",
    "        if (jd_ng1_ng2_name <= 0.7) : #or name_score<=1 :\n",
    "            number_of_matches = number_of_matches +1\n",
    "            matches.append((company1.iloc[i,0],company2.iloc[j,0]))\n",
    "print(\"Number of matches: {}\".format(number_of_matches))\n",
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df.columns= ['idCompany1','idCompany2']\n",
    "diff_df = pd.merge(ground_truth_matches, matches_df, how='outer', indicator='Exist')\n",
    "true_positives = diff_df[diff_df.Exist=='both']\n",
    "false_positives = diff_df[diff_df.Exist=='right_only']\n",
    "false_negatives = diff_df[diff_df.Exist=='left_only']\n",
    "print(\"Number of true positives: {}\".format(len(true_positives)))\n",
    "print(\"Number of false positives: {}\".format(len(false_positives)))\n",
    "print(\"Number of false negatives: {}\".format(len(false_negatives)))\n",
    "precision = len(true_positives)/(len(true_positives)+ len(false_positives))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "recall = len(true_positives)/(len(true_positives)+ len(false_negatives))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "f_measure = 2*(precision*recall)/(precision+recall)\n",
    "print(\"F measure: {}\".format(f_measure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 25\n",
      "Number of true positives: 25\n",
      "Number of false positives: 0\n",
      "Number of false negatives: 75\n",
      "Precision: 1.0\n",
      "Recall: 0.25\n",
      "F measure: 0.4\n"
     ]
    }
   ],
   "source": [
    "##Methode 4 : idem avec des n-grams de 3\n",
    "number_of_matches = 0\n",
    "matches=[]\n",
    "for i in range(len(company1)):\n",
    "    tokens1name = nltk.word_tokenize(company1.iloc[i,1] + ' ' + str(company1.iloc[i,3]))\n",
    "    ng1_tokensname = set(nltk.ngrams(tokens1name, n=3))\n",
    "    #print(ng1_tokensname)\n",
    "    for j in range(len(company2)):\n",
    "        tokens2name = nltk.word_tokenize(company2.iloc[j,1] + ' ' + str(company2.iloc[j,3]))\n",
    "        ng2_tokensname = set(nltk.ngrams(tokens2name, n=3))\n",
    "        jd_ng1_ng2_name = nltk.jaccard_distance(ng1_tokensname, ng2_tokensname)\n",
    "        name_score = nltk.edit_distance(company1.iloc[i,1] , company2.iloc[j,1] )\n",
    "        if (jd_ng1_ng2_name <= 0.75) : #or name_score<=1 :\n",
    "            number_of_matches = number_of_matches +1\n",
    "            matches.append((company1.iloc[i,0],company2.iloc[j,0]))\n",
    "print(\"Number of matches: {}\".format(number_of_matches))\n",
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df.columns= ['idCompany1','idCompany2']\n",
    "diff_df = pd.merge(ground_truth_matches, matches_df, how='outer', indicator='Exist')\n",
    "true_positives = diff_df[diff_df.Exist=='both']\n",
    "false_positives = diff_df[diff_df.Exist=='right_only']\n",
    "false_negatives = diff_df[diff_df.Exist=='left_only']\n",
    "print(\"Number of true positives: {}\".format(len(true_positives)))\n",
    "print(\"Number of false positives: {}\".format(len(false_positives)))\n",
    "print(\"Number of false negatives: {}\".format(len(false_negatives)))\n",
    "precision = len(true_positives)/(len(true_positives)+ len(false_positives))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "recall = len(true_positives)/(len(true_positives)+ len(false_negatives))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "f_measure = 2*(precision*recall)/(precision+recall)\n",
    "print(\"F measure: {}\".format(f_measure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 38\n",
      "Number of true positives: 32\n",
      "Number of false positives: 6\n",
      "Number of false negatives: 70\n",
      "Precision: 0.8421052631578947\n",
      "Recall: 0.3137254901960784\n",
      "F measure: 0.45714285714285713\n"
     ]
    }
   ],
   "source": [
    "##Methode 4 : jaccard distance en mixant nom, description et manufacturer\n",
    "number_of_matches = 0\n",
    "matches=[]\n",
    "for i in range(len(company1)):\n",
    "    tokens1name = nltk.word_tokenize(company1.iloc[i,1] + ' ' + str(company1.iloc[i,2]) + ' ' + str(company1.iloc[i,3]))\n",
    "    ng1_tokensname = set(nltk.ngrams(tokens1name, n=1))\n",
    "    #print(ng1_tokensname)\n",
    "    for j in range(len(company2)):\n",
    "        tokens2name = nltk.word_tokenize(company2.iloc[j,1] + ' ' + str(company2.iloc[j,2]) + ' ' + str(company2.iloc[j,3]))\n",
    "        ng2_tokensname = set(nltk.ngrams(tokens2name, n=1))\n",
    "        jd_ng1_ng2_name = nltk.jaccard_distance(ng1_tokensname, ng2_tokensname)\n",
    "        name_score = nltk.edit_distance(company1.iloc[i,1] , company2.iloc[j,1] )\n",
    "        if (jd_ng1_ng2_name <= 0.8) : #or name_score<=1 :\n",
    "            number_of_matches = number_of_matches +1\n",
    "            matches.append((company1.iloc[i,0],company2.iloc[j,0]))\n",
    "print(\"Number of matches: {}\".format(number_of_matches))\n",
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df.columns= ['idCompany1','idCompany2']\n",
    "diff_df = pd.merge(ground_truth_matches, matches_df, how='outer', indicator='Exist')\n",
    "true_positives = diff_df[diff_df.Exist=='both']\n",
    "false_positives = diff_df[diff_df.Exist=='right_only']\n",
    "false_negatives = diff_df[diff_df.Exist=='left_only']\n",
    "print(\"Number of true positives: {}\".format(len(true_positives)))\n",
    "print(\"Number of false positives: {}\".format(len(false_positives)))\n",
    "print(\"Number of false negatives: {}\".format(len(false_negatives)))\n",
    "precision = len(true_positives)/(len(true_positives)+ len(false_positives))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "recall = len(true_positives)/(len(true_positives)+ len(false_negatives))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "f_measure = 2*(precision*recall)/(precision+recall)\n",
    "print(\"F measure: {}\".format(f_measure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 91\n",
      "Number of true positives: 81\n",
      "Number of false positives: 10\n",
      "Number of false negatives: 24\n",
      "Precision: 0.8901098901098901\n",
      "Recall: 0.7714285714285715\n",
      "F measure: 0.8265306122448981\n"
     ]
    }
   ],
   "source": [
    "##Methode 5 : methode 2 + price ratio<3\n",
    "number_of_matches = 0\n",
    "matches=[]\n",
    "for i in range(len(company1)):\n",
    "    try :  \n",
    "        price1 = float(company1.iloc[i,4]) \n",
    "    except : \n",
    "        price1 = 0\n",
    "    tokens1name = nltk.word_tokenize(str(company1.iloc[i,3]) + ' ' + company1.iloc[i,1])\n",
    "    ng1_tokensname = set(nltk.ngrams(tokens1name, n=1))\n",
    "    #print(ng1_tokensname)\n",
    "    for j in range(len(company2)):\n",
    "        try :  \n",
    "            price2 = float(company2.iloc[j,4]) \n",
    "        except : \n",
    "            price2 = 0\n",
    "        if price1* price2 == 0:\n",
    "            price_ratio=1\n",
    "        else:\n",
    "            price_ratio =max(price1, price2)/min(price1, price2)\n",
    "        tokens2name = nltk.word_tokenize(str(company2.iloc[j,3]) + ' ' + company2.iloc[j,1]  )\n",
    "        ng2_tokensname = set(nltk.ngrams(tokens2name, n=1))\n",
    "        jd_ng1_ng2_name = nltk.jaccard_distance(ng1_tokensname, ng2_tokensname)\n",
    "        name_score = nltk.edit_distance(company1.iloc[i,1] , company2.iloc[j,1] )\n",
    "        if ((jd_ng1_ng2_name <= 0.65) and price_ratio<3) or ((jd_ng1_ng2_name <= 0.75) and price_ratio<1.25): #or name_score<=1 :\n",
    "            number_of_matches = number_of_matches +1\n",
    "            matches.append((company1.iloc[i,0],company2.iloc[j,0]))\n",
    "print(\"Number of matches: {}\".format(number_of_matches))\n",
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df.columns= ['idCompany1','idCompany2']\n",
    "diff_df = pd.merge(ground_truth_matches, matches_df, how='outer', indicator='Exist')\n",
    "true_positives = diff_df[diff_df.Exist=='both']\n",
    "false_positives = diff_df[diff_df.Exist=='right_only']\n",
    "false_negatives = diff_df[diff_df.Exist=='left_only']\n",
    "print(\"Number of true positives: {}\".format(len(true_positives)))\n",
    "print(\"Number of false positives: {}\".format(len(false_positives)))\n",
    "print(\"Number of false negatives: {}\".format(len(false_negatives)))\n",
    "precision = len(true_positives)/(len(true_positives)+ len(false_positives))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "recall = len(true_positives)/(len(true_positives)+ len(false_negatives))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "f_measure = 2*(precision*recall)/(precision+recall)\n",
    "print(\"F measure: {}\".format(f_measure))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, the two records share the same value for attributes 'name' and 'address'. However, they have slightly different values for the columns 'city' and 'cuisine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>18374143831267894200</td>\n",
       "      <td>sound studio 3 for mac</td>\n",
       "      <td>easytouse mac os x application for recording a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                    name  \\\n",
       "78  18374143831267894200  sound studio 3 for mac   \n",
       "\n",
       "                                          description manufacturer  price  \n",
       "78  easytouse mac os x application for recording a...          NaN  79.99  "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nan',\n",
       " 'weekly',\n",
       " 'reader',\n",
       " 'preparing',\n",
       " 'for',\n",
       " 'kindergarten',\n",
       " '2008',\n",
       " '(',\n",
       " 'pc/mac',\n",
       " ')',\n",
       " 'fogware']"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens2name "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, on the other hand, the two records are associated with different names, cities and cuisiones.\n",
    "\n",
    "This file represents a simple example of datasets, on which we can experiment with th etechniques presented in the course to try identify duplicates, without using (that is relying on the values of) the column \"unique_id\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by adding a new column to identify the records (lines) in our dataframe\n",
    "df_restaurants.insert(0,'record_ID', range(0, len(df_restaurants)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exhaustive comparisons: every record is compared with every other record\n",
    "\n",
    "We start by applying an exhaustive strategy whereby every record in the CSV file, is compared with every other record. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below does this for us. In doing so, it uses the following rule:\n",
    "\n",
    "For two records to match, i.e. refer to the same restaurant in the real world:\n",
    "* The edit distance between the attribute name values of the two records needs to be smaller or equal to 3, and \n",
    "* they need to have the same value for the cuisine attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants[df_restaurants.record_ID.isin([43, 622])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = len(df_restaurants)\n",
    "matches = []\n",
    "matchescomplet = []\n",
    "\n",
    "number_of_matches = 0\n",
    "tokens1=[]\n",
    "tokens2=[]\n",
    "start = time.process_time()\n",
    "for i in range(0,num_records):\n",
    "    \n",
    "    # Après tokenization , calcul du ngrams (n=1) pour le name qui servira pour la Jaccard distance, pour la ligne i\n",
    "    tokens1name = nltk.word_tokenize(df_restaurants.iloc[i,1]) \n",
    "    ng1_tokensname = set(nltk.ngrams(tokens1name, n=1))\n",
    "    \n",
    "    # Après tokenization , calcul du ngrams (n=1) pour l'adresse qui servira pour la Jaccard distance,, pour la ligne i\n",
    "    tokens1adr = nltk.word_tokenize(df_restaurants.iloc[i,2]) \n",
    "    ng1_tokensadr = set(nltk.ngrams(tokens1adr, n=1))\n",
    "    \n",
    "    \n",
    "    for j in range(i+1,num_records):\n",
    "        \n",
    "        # Après tokenization , calcul du ngrams (n=1) pour le name qui servira pour la Jaccard distance, , pour la ligne j\n",
    "        tokens2name = nltk.word_tokenize( df_restaurants.iloc[j,1]) \n",
    "        ng2_tokensname = set(nltk.ngrams(tokens2name, n=1))\n",
    "        \n",
    "        # Après tokenization , calcul du ngrams (n=1) pour le name qui servira pour la Jaccard distance, , pour la ligne j\n",
    "        tokens2adr = nltk.word_tokenize( df_restaurants.iloc[j,2]) \n",
    "        ng2_tokensadr = set(nltk.ngrams(tokens2adr, n=1))\n",
    "       \n",
    "        # calcul de la Jaccard distance pour le name entre la ligne i et la ligne j (\"item based\" avec ngrams (n=1)) \n",
    "        jd_ng1_ng2_name = nltk.jaccard_distance(ng1_tokensname, ng2_tokensname)  # jaccard distance entre les ngram=1 des names\n",
    "        \n",
    "        # calcul de la Jaccard distance pour l'adresse entre la ligne i et la ligne j (\"item based\" avec ngrams (n=1)) \n",
    "        jd_ng1_ng2_adr = nltk.jaccard_distance(ng1_tokensadr, ng2_tokensadr)  # jaccard distance entre les ngram=1 des adresses\n",
    "    \n",
    "        # Rule for matching: \n",
    "        # disjonction entre une similarité entre les names (name_score<=1) \n",
    "        # et une similarité conjugée entre les adresses et les noms (jd_ng1_ng2_adr <= 0.6 and jd_ng1_ng2_name <= 0.6)\n",
    "        name_score = nltk.edit_distance(df_restaurants.iloc[i,1], df_restaurants.iloc[j,1])\n",
    "        \n",
    "        # Rule for matching: Distance between names is smaller or equal to 3 and the cuisine is the same \n",
    "        if (jd_ng1_ng2_adr <= 0.6 and jd_ng1_ng2_name <= 0.6) or name_score<=1 :\n",
    "            number_of_matches = number_of_matches +1 \n",
    "            # matchescomplet.append((df_restaurants.iloc[i,0],df_restaurants.iloc[i,1], df_restaurants.iloc[i,2],df_restaurants.iloc[i,5], df_restaurants.iloc[j,0],df_restaurants.iloc[j,1], df_restaurants.iloc[j,2],df_restaurants.iloc[j,5]))\n",
    "            matches.append((df_restaurants.iloc[i,0],df_restaurants.iloc[j,0]))\n",
    "\n",
    "end = time.process_time()\n",
    "\n",
    "print(\"Number of matches: {}\".format(number_of_matches))\n",
    "print(\"Processing time: {}\".format(end - start))\n",
    "for _ in matchescomplet:\n",
    "     print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quelques tests pour ajuster les critères de notre algorithme\n",
    "name_score = nltk.edit_distance(df_restaurants.iloc[73,1], df_restaurants.iloc[763,1])\n",
    "name_score   # 11\n",
    "# comme on le voit ci-dessous, la différence est le mot \"restaurant\", l'edit distance est très importante (11), \n",
    "# on ne peut pas se baser dessus pour dire que c le même resto, il faut qu'on ajoute un critère \"item based\" \n",
    "# en plus du critère edit_distance name_score<=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qq tests pour ajuster les critères de notre alogorithme\n",
    "df_restaurants[df_restaurants.record_ID.isin([73, 763])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_score = nltk.edit_distance(df_restaurants.iloc[32,1], df_restaurants.iloc[759,1])\n",
    "#print(name_score)\n",
    "tokens1 = nltk.word_tokenize(df_restaurants.iloc[73,1]) \n",
    "tokens2 = nltk.word_tokenize( df_restaurants.iloc[763,1]) \n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "ng1_tokens = set(nltk.ngrams(tokens1, n=1))\n",
    "ng2_tokens = set(nltk.ngrams(tokens2, n=1))\n",
    "print(ng1_tokens)\n",
    "print(ng2_tokens)\n",
    "\n",
    "jd_sent_1_2 = nltk.jaccard_distance(ng1_tokens, ng2_tokens)\n",
    "print(jd_sent_1_2)\n",
    "# jd_ng1_ng2_adr <= 0.6,ce seuil de 0.6 suffira dire que les lignes 32 et 759 sont le même restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adresse\n",
    "#print(name_score)\n",
    "tokens1 = nltk.word_tokenize(df_restaurants.iloc[73,2]) \n",
    "tokens2 = nltk.word_tokenize( df_restaurants.iloc[763,2]) \n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "ng1_tokens = set(nltk.ngrams(tokens1, n=1))\n",
    "ng2_tokens = set(nltk.ngrams(tokens2, n=1))\n",
    "print(ng1_tokens)\n",
    "print(ng2_tokens)\n",
    "\n",
    "jd_sent_1_2 = nltk.jaccard_distance(ng1_tokens, ng2_tokens)\n",
    "print(jd_sent_1_2)\n",
    "# ça ne passe pas , mais c pas grave car mettre le seuil à 0.67 va nous rajouter beaucoup de faux positifs \n",
    "# on a testé ce seuil plus élevé de 0.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_score = nltk.edit_distance(df_restaurants.iloc[6,1], df_restaurants.iloc[754,1])\n",
    "name_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_score = nltk.edit_distance(df_restaurants.iloc[32,1], df_restaurants.iloc[759,1])\n",
    "#print(name_score)\n",
    "tokens1 = nltk.word_tokenize(df_restaurants.iloc[6,1]) \n",
    "tokens2 = nltk.word_tokenize( df_restaurants.iloc[754,1]) \n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "ng1_tokens = set(nltk.ngrams(tokens1, n=1))\n",
    "ng2_tokens = set(nltk.ngrams(tokens2, n=1))\n",
    "print(ng1_tokens)\n",
    "print(ng2_tokens)\n",
    "\n",
    "jd_sent_1_2 = nltk.jaccard_distance(ng1_tokens, ng2_tokens)\n",
    "print(jd_sent_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_score = nltk.edit_distance(df_restaurants.iloc[6,1], df_restaurants.iloc[754,1])\n",
    "name_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_score = nltk.edit_distance(df_restaurants.iloc[32,1], df_restaurants.iloc[759,1])\n",
    "#print(name_score)\n",
    "tokens1 = nltk.word_tokenize(df_restaurants.iloc[32,1]) \n",
    "tokens2 = nltk.word_tokenize( df_restaurants.iloc[759,1]) \n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "ng1_tokens = set(nltk.ngrams(tokens1, n=1))\n",
    "ng2_tokens = set(nltk.ngrams(tokens2, n=1))\n",
    "print(ng1_tokens)\n",
    "print(ng2_tokens)\n",
    "\n",
    "jd_sent_1_2 = nltk.jaccard_distance(ng1_tokens, ng2_tokens)\n",
    "print(jd_sent_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "for match in matches:\n",
    "    print(\"The following records {} and {} match\".format(match[0],match[1]))\n",
    "    print(\"The restaurants with the following names {} and {} match.\".format(df_restaurants.iloc[match[0],1],df_restaurants.iloc[match[1],1]))\n",
    "    print(\"The restaurants with the following addresses {} and {} match.\".format(df_restaurants.iloc[match[0],2],df_restaurants.iloc[match[1],2]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the rule applied in the above code is not great. You may want to try other kind of distances, other thresholds, and other rules to identify matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing the quality of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we first need to compute the ground truth (that is the list of correct matches) considering the attribute unique_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches = pd.read_csv(\"./restaurants.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches.insert(0, 'record_ID', range(0, len(ground_truth_matches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches = pd.merge(ground_truth_matches,\n",
    "                                ground_truth_matches,\n",
    "                                on = 'unique_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ground_truth_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches = ground_truth_matches.query('record_ID_x < record_ID_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches = ground_truth_matches[['record_ID_x','record_ID_y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ground_truth_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ground_truth_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(matches))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df.columns= ['record_ID_x','record_ID_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on s'assure que les couples record_ID_x et record_ID_y sont dans le bons sens (record_ID_x < record_ID_y)\n",
    "# comme dans ground_truth\n",
    "matches_df[matches_df['record_ID_x'] >= matches_df['record_ID_y'] ]\n",
    "# 0 lignes trouvées , donc c OK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df = pd.merge(ground_truth_matches, matches_df, how='outer', indicator='Exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives = diff_df[diff_df.Exist=='both']\n",
    "false_positives = diff_df[diff_df.Exist=='right_only']\n",
    "false_negatives = diff_df[diff_df.Exist=='left_only']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idCompany1</th>\n",
       "      <th>idCompany2</th>\n",
       "      <th>Exist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>b0002ibev4</td>\n",
       "      <td>1887899244694755891</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>b0007lw22g</td>\n",
       "      <td>18398718226932431716</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>b0001wn16m</td>\n",
       "      <td>10092468528845066077</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>b000ozhfsq</td>\n",
       "      <td>18438075297130458214</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>b00006sijr</td>\n",
       "      <td>6247936198343071793</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idCompany1            idCompany2 Exist\n",
       "0  b0002ibev4   1887899244694755891  both\n",
       "1  b0007lw22g  18398718226932431716  both\n",
       "5  b0001wn16m  10092468528845066077  both\n",
       "7  b000ozhfsq  18438075297130458214  both\n",
       "9  b00006sijr   6247936198343071793  both"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# les vrais duplicats que notre algo a pu détecter\n",
    "true_positives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of a true positive\n",
    "df_restaurants[df_restaurants.record_ID.isin(['6','754'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idCompany1</th>\n",
       "      <th>idCompany2</th>\n",
       "      <th>Exist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>b0000c6fjm</td>\n",
       "      <td>18394964067436310447</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>b0000c6fjm</td>\n",
       "      <td>10092468528845066077</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>b000ndibq4</td>\n",
       "      <td>9761533219806554318</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>b000o27t8a</td>\n",
       "      <td>9761533219806554318</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>b000o27t8a</td>\n",
       "      <td>14054232840925252286</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     idCompany1            idCompany2       Exist\n",
       "103  b0000c6fjm  18394964067436310447  right_only\n",
       "104  b0000c6fjm  10092468528845066077  right_only\n",
       "105  b000ndibq4   9761533219806554318  right_only\n",
       "106  b000o27t8a   9761533219806554318  right_only\n",
       "107  b000o27t8a  14054232840925252286  right_only"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notre algo les a sortis comme restos en double mais c pas vrai\n",
    "false_positives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notre critère de duplicate :\n",
    "# (jd_ng1_ng2_adr <= 0.6 and jd_ng1_ng2_name <= 0.6) or (name_score<=1 and jd_ng1_ng2_adr <= 0.6) \n",
    "# eliminer grace jd_ng1_ng2_adr = 0.6666\n",
    "df_restaurants[df_restaurants.record_ID.isin(['55','56'])]\n",
    "# le name est le même donc l'algo dit que ce le même restaurant alors que ce n'est pas vrai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pareil c pas le même resto alors que l'algo les a retenu comme duplicate\n",
    "# car les names diffèrent d'un seul caractère.\n",
    "df_restaurants[df_restaurants.record_ID.isin(['87','88'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_score<=1\n",
    "name_score = nltk.edit_distance(df_restaurants.iloc[87,1], df_restaurants.iloc[88,1])\n",
    "name_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idCompany1</th>\n",
       "      <th>idCompany2</th>\n",
       "      <th>Exist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>b000ofnri8</td>\n",
       "      <td>12244614697089679523</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>b0007d8r5k</td>\n",
       "      <td>13775362651326388438</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>b00099qrok</td>\n",
       "      <td>9755705822363275907</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>b0002e3g6o</td>\n",
       "      <td>18384557845547191313</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>b0009yx9be</td>\n",
       "      <td>1021042895134770712</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idCompany1            idCompany2      Exist\n",
       "2  b000ofnri8  12244614697089679523  left_only\n",
       "3  b0007d8r5k  13775362651326388438  left_only\n",
       "4  b00099qrok   9755705822363275907  left_only\n",
       "6  b0002e3g6o  18384557845547191313  left_only\n",
       "8  b0009yx9be   1021042895134770712  left_only"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# les vrais duplicates que l'algo n'a pas détecté\n",
    "false_negatives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants[df_restaurants.record_ID.isin(['32','759'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faux négatif\n",
    "# pour l'algo le 32 et le 759 c'est pas le même restaurant, pourtant c le même\n",
    "# en effet les names diffèrents en lettres et en mots : \n",
    "# name_score > 1 et jd_ng1_ng2_name > 0.6 (ça suffit pour l'algo pour l'éliminer ) et en plus jd_ng1_ng2_adr > 0.6\n",
    "name_score = nltk.edit_distance(df_restaurants.iloc[32,1], df_restaurants.iloc[759,1])\n",
    "name_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # (jd_ng1_ng2_adr <= 0.6) and jd_ng1_ng2_name <= 0.6) or (name_score<=1)\n",
    "    \n",
    "# name_score = nltk.edit_distance(df_restaurants.iloc[32,1], df_restaurants.iloc[759,1])\n",
    "\n",
    "tokens1 = nltk.word_tokenize(df_restaurants.iloc[32,1])   # name\n",
    "tokens2 = nltk.word_tokenize( df_restaurants.iloc[759,1]) \n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "ng1_tokens = set(nltk.ngrams(tokens1, n=1))\n",
    "ng2_tokens = set(nltk.ngrams(tokens2, n=1))\n",
    "print(ng1_tokens)\n",
    "print(ng2_tokens)\n",
    "\n",
    "jd_sent_1_2 = nltk.jaccard_distance(ng1_tokens, ng2_tokens)\n",
    "print(jd_sent_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # (jd_ng1_ng2_adr <= 0.6) and (name_score<=2 or jd_ng1_ng2_name <= 0.67)\n",
    "    \n",
    "# name_score = nltk.edit_distance(df_restaurants.iloc[32,1], df_restaurants.iloc[759,1])\n",
    "#print(name_score)\n",
    "tokens1 = nltk.word_tokenize(df_restaurants.iloc[73,2])   # adresse \n",
    "tokens2 = nltk.word_tokenize( df_restaurants.iloc[763,2]) \n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "ng1_tokens = set(nltk.ngrams(tokens1, n=1))\n",
    "ng2_tokens = set(nltk.ngrams(tokens2, n=1))\n",
    "print(ng1_tokens)\n",
    "print(ng2_tokens)\n",
    "\n",
    "jd_sent_1_2 = nltk.jaccard_distance(ng1_tokens, ng2_tokens)\n",
    "print(jd_sent_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ground_truth_matches))\n",
    "print(len(matches_df))\n",
    "print(len(true_positives) , 'true_positives')\n",
    "print(len(false_positives) ,'false_positives')\n",
    "print(len(false_negatives)  , 'false_negatives')\n",
    "\n",
    "# len(true_positives)  +  len(false_negatives) = len(ground_truth_matches)\n",
    "\n",
    "# len(matches_df)) - len(false_positif) + len(false_negatives)     = ground_truth_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = len(true_positives)/(len(true_positives)+ len(false_positives))\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you are using pyton 2.7 (instead of Python 3), you would need to convert integers to float prior to performing the division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = len(true_positives)/(len(true_positives)+ len(false_negatives))\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_measure = 2*(precision*recall)/(precision+recall)\n",
    "print(f_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windowing (SNM) method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 841 842\n",
    "# qq tests pour choisir sur quel champ on va faire le sort \n",
    "# le sorted name parait intéressant\n",
    "df_restaurants.sort_values(by=['name']).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le tri est fait dans ce qui suit selon le champ \"name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 50   # \n",
    "\n",
    "# tri par name car c ce qui permet d'avoir des resto en double les plus proches possibles \n",
    "\n",
    "df_restaurants= df_restaurants.sort_values(by=['name'])  \n",
    "\n",
    "number_of_matchesw = 0\n",
    "num_records = len(df_restaurants)\n",
    "matchesw = []\n",
    "matchescompletw = []\n",
    "\n",
    "start = time.process_time()\n",
    "for i in range(0,min(window,len(df_restaurants))):\n",
    "    \n",
    "    tokens1name = nltk.word_tokenize(df_restaurants.iloc[i,1]) \n",
    "    ng1_tokensname = set(nltk.ngrams(tokens1name, n=1))\n",
    "    \n",
    "    tokens1adr = nltk.word_tokenize(df_restaurants.iloc[i,2]) \n",
    "    ng1_tokensadr = set(nltk.ngrams(tokens1adr, n=1))\n",
    "    \n",
    "    \n",
    "    for j in range(i+1,min(window,len(df_restaurants))):\n",
    "        tokens2name = nltk.word_tokenize( df_restaurants.iloc[j,1]) \n",
    "        ng2_tokensname = set(nltk.ngrams(tokens2name, n=1))\n",
    "        \n",
    "        \n",
    "        tokens2adr = nltk.word_tokenize( df_restaurants.iloc[j,2]) \n",
    "        ng2_tokensadr = set(nltk.ngrams(tokens2adr, n=1))\n",
    "#         print(tokens1)\n",
    "#         print(tokens2)       \n",
    "        jd_ng1_ng2_name = nltk.jaccard_distance(ng1_tokensname, ng2_tokensname)  # jaccard distance entre les ngram=1 des names\n",
    "        jd_ng1_ng2_adr = nltk.jaccard_distance(ng1_tokensadr, ng2_tokensadr)  # jaccard distance entre les ngram=1 des adresses\n",
    "    \n",
    "        name_score = nltk.edit_distance(df_restaurants.iloc[i,1], df_restaurants.iloc[j,1])\n",
    "        \n",
    "        # Rule for matching: Distance between names is smaller or equal to 3 and the cuisine is the same \n",
    "        if (jd_ng1_ng2_adr <= 0.6 and jd_ng1_ng2_name <= 0.6) or name_score<=1 :\n",
    "            number_of_matchesw = number_of_matchesw +1 \n",
    "            # matchescomplet.append((df_restaurants.iloc[i,0],df_restaurants.iloc[i,1], df_restaurants.iloc[i,2],df_restaurants.iloc[i,5], df_restaurants.iloc[j,0],df_restaurants.iloc[j,1], df_restaurants.iloc[j,2],df_restaurants.iloc[j,5]))\n",
    "            matchesw.append((df_restaurants.iloc[i,0],df_restaurants.iloc[j,0]))\n",
    "            matchescompletw.append((df_restaurants.iloc[i,0],df_restaurants.iloc[i,1], df_restaurants.iloc[i,2],df_restaurants.iloc[i,5], df_restaurants.iloc[j,0],df_restaurants.iloc[j,1], df_restaurants.iloc[j,2],df_restaurants.iloc[j,5]))\n",
    "                     \n",
    "            \n",
    "            \n",
    "for i in range(window,len(df_restaurants)):\n",
    "    \n",
    "    tokens1name = nltk.word_tokenize(df_restaurants.iloc[i,1]) \n",
    "    ng1_tokensname = set(nltk.ngrams(tokens1name, n=1))\n",
    "    \n",
    "    tokens1adr = nltk.word_tokenize(df_restaurants.iloc[i,2]) \n",
    "    ng1_tokensadr = set(nltk.ngrams(tokens1adr, n=1))\n",
    "    \n",
    "    \n",
    "    for j in range(i-window+1,i):\n",
    "        tokens2name = nltk.word_tokenize( df_restaurants.iloc[j,1]) \n",
    "        ng2_tokensname = set(nltk.ngrams(tokens2name, n=1))\n",
    "        \n",
    "        \n",
    "        tokens2adr = nltk.word_tokenize( df_restaurants.iloc[j,2]) \n",
    "        ng2_tokensadr = set(nltk.ngrams(tokens2adr, n=1))\n",
    "     \n",
    "        jd_ng1_ng2_name = nltk.jaccard_distance(ng1_tokensname, ng2_tokensname)  # jaccard distance entre les ngram=1 des names\n",
    "        jd_ng1_ng2_adr = nltk.jaccard_distance(ng1_tokensadr, ng2_tokensadr)  # jaccard distance entre les ngram=1 des adresses\n",
    "    \n",
    "        name_score = nltk.edit_distance(df_restaurants.iloc[i,1], df_restaurants.iloc[j,1])\n",
    "        \n",
    "        # Rule for matching: Distance between names is smaller or equal to 3 and the cuisine is the same \n",
    "        if (jd_ng1_ng2_adr <= 0.6 and jd_ng1_ng2_name <= 0.6) or name_score<=1 :\n",
    "            number_of_matchesw = number_of_matchesw +1 \n",
    "            # matchescomplet.append((df_restaurants.iloc[i,0],df_restaurants.iloc[i,1], df_restaurants.iloc[i,2],df_restaurants.iloc[i,5], df_restaurants.iloc[j,0],df_restaurants.iloc[j,1], df_restaurants.iloc[j,2],df_restaurants.iloc[j,5]))\n",
    "            matchesw.append((df_restaurants.iloc[i,0],df_restaurants.iloc[j,0]))\n",
    "            matchescompletw.append((df_restaurants.iloc[i,0],df_restaurants.iloc[i,1], df_restaurants.iloc[i,2],df_restaurants.iloc[i,5], df_restaurants.iloc[j,0],df_restaurants.iloc[j,1], df_restaurants.iloc[j,2],df_restaurants.iloc[j,5]))\n",
    "            \n",
    "end = time.process_time()\n",
    "\n",
    "print(\"Number of matches: {}\".format(number_of_matchesw))\n",
    "print(\"Processing time: {}\".format(end - start))            \n",
    "for _ in matchescompletw:\n",
    "     print(_)  \n",
    "# for _ in matches:\n",
    "#      print(_)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "for match in matchesw:\n",
    "    print(\"The following records {} and {} match\".format(match[0],match[1]))\n",
    "    print(\"The restaurants with the following names {} and {} match.\".format(df_restaurants.iloc[match[0],1],df_restaurants.iloc[match[1],1]))\n",
    "    print(\"The restaurants with the following addresses {} and {} match.\".format(df_restaurants.iloc[match[0],2],df_restaurants.iloc[match[1],2]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matchesw_df = pd.DataFrame(matchesw)\n",
    "matchesw_df.columns= ['record_ID_x','record_ID_y']\n",
    "\n",
    "matchesw_df['MIN'] = matchesw_df[['record_ID_x','record_ID_y']].min(axis=1)\n",
    "matchesw_df['MAX'] = matchesw_df[['record_ID_x','record_ID_y']].max(axis=1)\n",
    "matchesw_df=matchesw_df[['MIN','MAX']]\n",
    "matchesw_df.columns=['record_ID_x','record_ID_y']\n",
    "matchesw_df\n",
    "\n",
    "\n",
    "diffw_df = pd.merge(ground_truth_matches, matchesw_df, how='outer', indicator='Exist')\n",
    "true_positivesw = diffw_df[diffw_df.Exist=='both']\n",
    "false_positivesw = diffw_df[diffw_df.Exist=='right_only']\n",
    "false_negativesw = diffw_df[diffw_df.Exist=='left_only']\n",
    "precisionw = len(true_positivesw)/(len(true_positivesw)+ len(false_positivesw))\n",
    "print(precisionw)\n",
    "recallw = len(true_positivesw)/(len(true_positivesw)+ len(false_negativesw))\n",
    "print(recallw)\n",
    "f_measurew = 2*(precisionw*recallw)/(precisionw+recallw)\n",
    "print(f_measurew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ground_truth_matches))\n",
    "print(len(matchesw_df))\n",
    "print(len(true_positivesw))\n",
    "print(len(false_positivesw))\n",
    "print(len(false_negativesw))  \n",
    "# len(true_positives)  +  len(false_negatives) = len(ground_truth_matches)\n",
    "# len(matches_df)) - len(false_positif) + len(false_negatives)     = ground_truth_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that in the above code, we do not implement the SNM algorithm in its entirety. In particular, we do not implement the last phase of inferring matches using transitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blocking method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants = pd.read_csv(\"./restaurants.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_restaurants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by adding a new column to identify the records (lines) in our dataframe\n",
    "df_restaurants.insert(0,'record_ID', range(0, len(df_restaurants)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The blocks correspond to resturants that are located in the same citydf_restaurants.loc[df_restaurants['city']==' atlanta']\n",
    "df_restaurants.loc[df_restaurants['city']==' atlanta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants.loc[df_restaurants['city'].str.strip()=='atlanta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on va créer un dict \"df_restov\" des restaurants de chaque ville\n",
    "# pour une clé= ville, la valeur du dict serait égale à un dataframe représentant les restos de cette ville\n",
    "df_restov= {}\n",
    "for ville in df_restaurants['city'].unique():\n",
    "    \n",
    "    df_restov[ville]   = df_restaurants.loc[df_restaurants['city']==ville]\n",
    "    num_records = len(df_restov[ville])\n",
    "    print(ville)   # on affiche la ville\n",
    "    print(num_records) # on affiche le nombre de restos par ville\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on vérifie  pour atlanta que ça marche bien, on a bien le dataframe qu'on voudrait.\n",
    "print(type(df_restov[\" atlanta\"]))\n",
    "print(df_restov[\" atlanta\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restov[\" atlanta\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on testel'algo précédent sur juste un dataframe celui des restos de \" atlanta\"  (avec un espace devant)\n",
    "num_records = len(df_restov[\" atlanta\"])\n",
    "amatches = []\n",
    "amatchescomplet = []\n",
    "\n",
    "anumber_of_matches = 0\n",
    "tokens1=[]\n",
    "tokens2=[]\n",
    "start = time.process_time()\n",
    "for i in range(0,num_records):\n",
    "    \n",
    "    # Après tokenization , calcul du ngrams (n=1) pour le name qui servira pour la Jaccard distance, pour la ligne i\n",
    "    tokens1name = nltk.word_tokenize(df_restov[\" atlanta\"].iloc[i,1]) \n",
    "    ng1_tokensname = set(nltk.ngrams(tokens1name, n=1))\n",
    "    \n",
    "    # Après tokenization , calcul du ngrams (n=1) pour l'adresse qui servira pour la Jaccard distance,, pour la ligne i\n",
    "    tokens1adr = nltk.word_tokenize(df_restov[\" atlanta\"].iloc[i,2]) \n",
    "    ng1_tokensadr = set(nltk.ngrams(tokens1adr, n=1))\n",
    "    \n",
    "    \n",
    "    for j in range(i+1,num_records):\n",
    "        \n",
    "        # Après tokenization , calcul du ngrams (n=1) pour le name qui servira pour la Jaccard distance, , pour la ligne j\n",
    "        tokens2name = nltk.word_tokenize( df_restov[\" atlanta\"].iloc[j,1]) \n",
    "        ng2_tokensname = set(nltk.ngrams(tokens2name, n=1))\n",
    "        \n",
    "        # Après tokenization , calcul du ngrams (n=1) pour le name qui servira pour la Jaccard distance, , pour la ligne j\n",
    "        tokens2adr = nltk.word_tokenize( df_restov[\" atlanta\"].iloc[j,2]) \n",
    "        ng2_tokensadr = set(nltk.ngrams(tokens2adr, n=1))\n",
    "     \n",
    "        # calcul de la Jaccard distance pour le name entre la ligne i et la ligne j (\"item based\" avec ngrams (n=1)) \n",
    "        jd_ng1_ng2_name = nltk.jaccard_distance(ng1_tokensname, ng2_tokensname)\n",
    "        \n",
    "        # calcul de la Jaccard distance pour l'adresse entre la ligne i et la ligne j (\"item based\" avec ngrams (n=1)) \n",
    "        jd_ng1_ng2_adr = nltk.jaccard_distance(ng1_tokensadr, ng2_tokensadr)  \n",
    "    \n",
    "        name_score = nltk.edit_distance(df_restov[\" atlanta\"].iloc[i,1], df_restov[\" atlanta\"].iloc[j,1])\n",
    "        \n",
    "        # Rule for matching: \n",
    "        # disjonction entre une similarité entre les names (name_score<=1) \n",
    "        # et une similarité conjugée entre les adresses et les noms (jd_ng1_ng2_adr <= 0.6 and jd_ng1_ng2_name <= 0.6)\n",
    "        if (jd_ng1_ng2_adr <= 0.6 and jd_ng1_ng2_name <= 0.6) or name_score<=1 :\n",
    "            anumber_of_matches = anumber_of_matches +1 \n",
    "            matchescomplet.append((df_restov[\" atlanta\"].iloc[i,0],df_restov[\" atlanta\"].iloc[i,1], \\\n",
    "            df_restov[\" atlanta\"].iloc[i,2],df_restov[\" atlanta\"].iloc[i,3], df_restov[\" atlanta\"].iloc[i,5], \\\n",
    "            df_restov[\" atlanta\"].iloc[j,0],df_restov[\" atlanta\"].iloc[j,1], df_restov[\" atlanta\"].iloc[j,2], \\\n",
    "            df_restov[\" atlanta\"].iloc[j,3],df_restov[\" atlanta\"].iloc[j,5]))\n",
    "            amatches.append((df_restov[\" atlanta\"].iloc[i,0],df_restov[\" atlanta\"].iloc[j,0]))\n",
    "\n",
    "end = time.process_time()\n",
    "\n",
    "print(\"Number of matches: {}\".format(anumber_of_matches))\n",
    "print(\"Processing time: {}\".format(end - start))\n",
    "for _ in amatchescomplet:\n",
    "     print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nous allons refaire le dict mais en éliminant les espaces saisis avant et après chaque ville\n",
    "# par précaution pour éviter des villes en double\n",
    "# et nous allons imprimer le nombre de restos par ville.\n",
    "\n",
    "df_restov={}\n",
    "cumul= 0\n",
    "# il faut enlever les espaces au début et à la fin de chaque ville dans le dataframe, \n",
    "# sinon on va rater des restos en double car ils ne seront pas dans le même block.\n",
    "\n",
    "for ville in df_restaurants['city'].str.strip().unique():   \n",
    "     print(ville)\n",
    "     df_restov[ville]   = df_restaurants.loc[df_restaurants['city'].str.strip()==ville]\n",
    "     print(len(df_restov[ville]))\n",
    "     cumul += len(df_restov[ville])\n",
    "\n",
    "print(cumul)\n",
    "# on vérifie qu'on retrouve bien un total de 865 restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Généralisation de la BLOCKING METHOD à toutes les villes \n",
    "bmatches = []\n",
    "bmatchescomplet = []\n",
    "bnumber_of_matches = 0\n",
    "start = time.process_time()\n",
    "    \n",
    "for ville in df_restaurants['city'].str.strip().unique():\n",
    "        # affichage de la ville et du nombre de restos par ville\n",
    "        # pour les matcher entre eux\n",
    "        print(ville)  \n",
    "        num_records = len(df_restov[ville])\n",
    "        print(num_records)\n",
    "        \n",
    "        tokens1=[]\n",
    "        tokens2=[]\n",
    "       \n",
    "        for i in range(0,num_records):\n",
    "\n",
    "            tokens1name = nltk.word_tokenize(df_restov[ville].iloc[i,1]) \n",
    "            ng1_tokensname = set(nltk.ngrams(tokens1name, n=1))\n",
    "\n",
    "            tokens1adr = nltk.word_tokenize(df_restov[ville].iloc[i,2]) \n",
    "            ng1_tokensadr = set(nltk.ngrams(tokens1adr, n=1))\n",
    "\n",
    "\n",
    "            for j in range(i+1,num_records):\n",
    "\n",
    "                tokens2name = nltk.word_tokenize( df_restov[ville].iloc[j,1]) \n",
    "                ng2_tokensname = set(nltk.ngrams(tokens2name, n=1))\n",
    "\n",
    "\n",
    "                tokens2adr = nltk.word_tokenize( df_restov[ville].iloc[j,2]) \n",
    "                ng2_tokensadr = set(nltk.ngrams(tokens2adr, n=1))\n",
    "\n",
    "                jd_ng1_ng2_name = nltk.jaccard_distance(ng1_tokensname, ng2_tokensname)  # jaccard distance entre les ngram=1 des names\n",
    "                jd_ng1_ng2_adr = nltk.jaccard_distance(ng1_tokensadr, ng2_tokensadr)  # jaccard distance entre les ngram=1 des adresses\n",
    "\n",
    "                name_score = nltk.edit_distance(df_restov[ville].iloc[i,1], df_restov[ville].iloc[j,1])\n",
    "\n",
    "                # Rule for matching: Item based Jaccard Distance with ngram=1 between adresses and between names or edit distance between names \n",
    "                if (jd_ng1_ng2_adr <= 0.6 and jd_ng1_ng2_name <= 0.6) or name_score<=1 :\n",
    "                    bnumber_of_matches = bnumber_of_matches +1 \n",
    "                    bmatchescomplet.append((df_restov[ville].iloc[i,0],df_restov[ville].iloc[i,1], \\\n",
    "                    df_restov[ville].iloc[i,2],df_restov[ville].iloc[i,3], df_restov[ville].iloc[i,5], \\\n",
    "                    df_restov[ville].iloc[j,0],df_restov[ville].iloc[j,1], df_restov[ville].iloc[j,2], \\\n",
    "                    df_restov[ville].iloc[j,3],df_restov[ville].iloc[j,5]))\n",
    "                    bmatches.append((df_restov[ville].iloc[i,0],df_restov[ville].iloc[j,0]))\n",
    "\n",
    "end = time.process_time()\n",
    "\n",
    "print(\"Number of matches: {}\".format(bnumber_of_matches))\n",
    "print(\"Processing time: {}\".format(end - start))\n",
    "# for _ in matchescomplet:\n",
    "#        print(_)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rappel des résultats de l'algo original sans blocking:\n",
    "####  Number of matches: 127\n",
    "#### Processing time: 167.984375\n",
    "\n",
    "#### les infos de l'algo avec  blocking ci-dessus\n",
    "#### Number of matches: 67\n",
    "#### Processing time: 25.6875\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ground_truth_matches = pd.read_csv(\"./restaurants.csv\")\n",
    "len(ground_truth_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches.insert(0, 'record_ID', range(0, len(ground_truth_matches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches = pd.merge(ground_truth_matches,\n",
    "                                ground_truth_matches,\n",
    "                                on = 'unique_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches = ground_truth_matches.query('record_ID_x < record_ID_y')\n",
    "len(ground_truth_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches = ground_truth_matches[['record_ID_x','record_ID_y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmatches_df = pd.DataFrame(bmatches)\n",
    "bmatches_df.columns= ['record_ID_x','record_ID_y']\n",
    "bmatches_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on s'assure que les couples record_ID_x et record_ID_y sont dans le bons sens (record_ID_x < record_ID_y)\n",
    "bmatches_df[bmatches_df['record_ID_x'] >= bmatches_df['record_ID_y'] ]\n",
    "# 0 lignes trouvées , donc c OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df = pd.merge(ground_truth_matches, bmatches_df, how='outer', indicator='Exist')\n",
    "diff_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btrue_positives = diff_df[diff_df.Exist=='both']\n",
    "bfalse_positives = diff_df[diff_df.Exist=='right_only']\n",
    "bfalse_negatives = diff_df[diff_df.Exist=='left_only']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# un vrai positif: c un vrai couple de restos en double qui a été détecté par notre algo sous forme de blocking method.\n",
    "# en effet il vérifie le critère de name (edit_distance=0) et en plus les 2 restos se trouve dans la même ville d'atlanta.\n",
    "df_restaurants[df_restaurants.record_ID.isin(['6','754'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# les couples détectés par notre algo comme des doubles mais à tort, ce ne sont pas des doubles.\n",
    "false_positives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants[df_restaurants.record_ID.isin(['96','196'])]\n",
    "# ce couple n'est pas dans le ground_truth car unique_id différent\n",
    "# mais il est dans le bmatches_df , (jd_ng1_ng2_adr <= 0.6 and jd_ng1_ng2_name <= 0.6) \n",
    "# cad les names sont proches pour la jaccard distance item based\n",
    "# et les adresses sont proches pour la jaccard distance item based.\n",
    "# et en plus ils se trouvent dans la même ville atlanta (blocking method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# les couples de restos en double mais qui ne sont pas détectés par notre algo comme des doubles.\n",
    "false_negatives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants[df_restaurants.record_ID.isin(['2','753'])]\n",
    "# ce couple est dans le ground_truth car même unique_id \n",
    "# mais il n'est pas dans le matches_df, malgré qu' ils ont le même name et la  même adresse (dans l'algo les détecte bien)\n",
    "# mais le Blocking method ne permet pas à l'algo de les matcher car ils sont considérés ayant des villes différentes :\n",
    "# 'new york' et 'new york city'  , à cause d'une mauvaise saisie de la ville."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bfalse_negatives) # y a beaucoup de false_ngatives par rapport à l'algo dans Blocking method (on avait 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false negative\n",
    "df_restaurants[df_restaurants.record_ID.isin(['26','756'])]\n",
    "# du au blocking method : new yor et new york city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false negative\n",
    "df_restaurants[df_restaurants.record_ID.isin(['32','759'])]\n",
    "# dû aux matching imprécis de l'algo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false negative\n",
    "df_restaurants[df_restaurants.record_ID.isin(['36','760'])]\n",
    "# du au blocking method : new yor et new york city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ground_truth_matches))\n",
    "print(len(bmatches_df))\n",
    "print(len(btrue_positives) , 'true_positives')\n",
    "print(len(bfalse_positives) ,'false_positives')\n",
    "print(len(bfalse_negatives)  , 'false_negatives')\n",
    "\n",
    "# len(true_positives)  +  len(false_negatives) = len(ground_truth_matches)\n",
    "\n",
    "# len(matches_df)) - len(false_positif) + len(false_negatives)     = ground_truth_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bprecision = len(btrue_positives)/(len(btrue_positives)+ len(bfalse_positives))\n",
    "print(bprecision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brecall = len(btrue_positives)/(len(btrue_positives)+ len(bfalse_negatives))\n",
    "print(brecall)\n",
    "# recall faible car y a beaucoup de false negatives\n",
    "# y a des duplicates que l'algo avec Blocking method n'a pas détecté car saisie à tort dans des villes différentes\n",
    "# surtout new york et new york city "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_measure = 2*(bprecision*brecall)/(bprecision+brecall)\n",
    "print(bf_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chiffres de l'algo original sans blocking method\n",
    "### precision: 0.7795\n",
    "\n",
    "### recall : 0.8839\n",
    "\n",
    "### f_measure :0.82845"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
