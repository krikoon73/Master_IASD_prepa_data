{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini projet Qualité de Données : Détections des doublons\n",
    "## ***Christophe COMPAIN / Sander COHEN***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectif et Données Disponibles\n",
    "L'objectif du projet est d'identifier les logiciels vendus sur les deux plateformes.\n",
    "\n",
    "Pour ce faire, nous disposons des données pour chacune des plateformes isolément, respectivement dans les fichiers ***Company1.csv*** et ***Company2.csv***. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages, Variables Globales et import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:\\\\OneDrive - Université Paris-Dauphine\\\\Bureau\\\\Cours Master\\\\12-Qualité de Données\\\\\\Projet\\\\mini-projet\\\\\"\n",
    "file1= \"SampleData\\\\Sample_Company1.csv\"\n",
    "file2= \"SampleData\\\\Sample_Company2.csv\"\n",
    "real= \"SampleData\\\\Sample_Groud_truth_mappings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "company1 = pd.read_csv(path+file1)\n",
    "company2 = pd.read_csv(path+file2)\n",
    "ground_truth_matches = pd.read_csv(path+real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>b0000aka82</td>\n",
       "      <td>studyworks! teaching pro: middle school math &amp;...</td>\n",
       "      <td>studyworks teaching pro middle school math &amp; s...</td>\n",
       "      <td>global software publishing</td>\n",
       "      <td>29.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>b000bnb72g</td>\n",
       "      <td>movies on psp</td>\n",
       "      <td>x-zoom movies on psp is the first commercial v...</td>\n",
       "      <td>x-oom</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>b0002e3g6o</td>\n",
       "      <td>eastwest bsendorfer 290 grand piano virtual in...</td>\n",
       "      <td>the 5-star awarded pmi bdorfer 290 is simply t...</td>\n",
       "      <td>eastwest</td>\n",
       "      <td>199.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>b00008ajjc</td>\n",
       "      <td>passwords plus 1.0</td>\n",
       "      <td>passwords plus stores all your personal inform...</td>\n",
       "      <td>dataviz</td>\n",
       "      <td>29.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>b0006g2wke</td>\n",
       "      <td>microsoft licenses project svr sa govt (h2200301)</td>\n",
       "      <td>microsoft project server win32 english softwar...</td>\n",
       "      <td>microsoft licenses</td>\n",
       "      <td>3601.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              title  \\\n",
       "0  b0000aka82  studyworks! teaching pro: middle school math &...   \n",
       "1  b000bnb72g                                      movies on psp   \n",
       "2  b0002e3g6o  eastwest bsendorfer 290 grand piano virtual in...   \n",
       "3  b00008ajjc                                 passwords plus 1.0   \n",
       "4  b0006g2wke  microsoft licenses project svr sa govt (h2200301)   \n",
       "\n",
       "                                         description  \\\n",
       "0  studyworks teaching pro middle school math & s...   \n",
       "1  x-zoom movies on psp is the first commercial v...   \n",
       "2  the 5-star awarded pmi bdorfer 290 is simply t...   \n",
       "3  passwords plus stores all your personal inform...   \n",
       "4  microsoft project server win32 english softwar...   \n",
       "\n",
       "                 manufacturer    price  \n",
       "0  global software publishing    29.99  \n",
       "1                       x-oom     0.00  \n",
       "2                    eastwest   199.95  \n",
       "3                     dataviz    29.99  \n",
       "4          microsoft licenses  3601.40  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>12244614697089679523</td>\n",
       "      <td>production prem cs3 mac upgrad</td>\n",
       "      <td>adobe cs3 production premium mac upgrade from ...</td>\n",
       "      <td>adobe software</td>\n",
       "      <td>805.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5781318040297863663</td>\n",
       "      <td>ipswitch wd-1000-0900 ws_ftp pro 9 sdk w/ ws_f...</td>\n",
       "      <td>no description available</td>\n",
       "      <td>ipswitch</td>\n",
       "      <td>325.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>9949258255997064102</td>\n",
       "      <td>money prem 2007 cd minibox</td>\n",
       "      <td>money premium 2007 win32 english north america...</td>\n",
       "      <td>microsoft</td>\n",
       "      <td>63.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9761533219806554318</td>\n",
       "      <td>adobe indesign cs3 for mac upgrade</td>\n",
       "      <td>system requirements powerpc g4 or g5 or intel ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>205.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>18226456193742595288</td>\n",
       "      <td>bamboo technology llc vault360 combo</td>\n",
       "      <td>capitalizing on the popularity of the ipod psp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               name  \\\n",
       "0  12244614697089679523                     production prem cs3 mac upgrad   \n",
       "1   5781318040297863663  ipswitch wd-1000-0900 ws_ftp pro 9 sdk w/ ws_f...   \n",
       "2   9949258255997064102                         money prem 2007 cd minibox   \n",
       "3   9761533219806554318                 adobe indesign cs3 for mac upgrade   \n",
       "4  18226456193742595288               bamboo technology llc vault360 combo   \n",
       "\n",
       "                                         description    manufacturer   price  \n",
       "0  adobe cs3 production premium mac upgrade from ...  adobe software  805.99  \n",
       "1                           no description available        ipswitch  325.51  \n",
       "2  money premium 2007 win32 english north america...       microsoft   63.99  \n",
       "3  system requirements powerpc g4 or g5 or intel ...             NaN  205.99  \n",
       "4  capitalizing on the popularity of the ipod psp...             NaN   24.58  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idCompany1</th>\n",
       "      <th>idCompany2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>b0002ibev4</td>\n",
       "      <td>1887899244694755891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>b0007lw22g</td>\n",
       "      <td>18398718226932431716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>b000ofnri8</td>\n",
       "      <td>12244614697089679523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>b0007d8r5k</td>\n",
       "      <td>13775362651326388438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>b00099qrok</td>\n",
       "      <td>9755705822363275907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idCompany1            idCompany2\n",
       "0  b0002ibev4   1887899244694755891\n",
       "1  b0007lw22g  18398718226932431716\n",
       "2  b000ofnri8  12244614697089679523\n",
       "3  b0007d8r5k  13775362651326388438\n",
       "4  b00099qrok   9755705822363275907"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_matches.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation d'un premier duplicat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>b0007lw22g</td>\n",
       "      <td>apple ilife '06 (mac dvd) [older version]</td>\n",
       "      <td>ilife '06 is the easiest way to make the most ...</td>\n",
       "      <td>apple computer</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                      title  \\\n",
       "32  b0007lw22g  apple ilife '06 (mac dvd) [older version]   \n",
       "\n",
       "                                          description    manufacturer  price  \n",
       "32  ilife '06 is the easiest way to make the most ...  apple computer   79.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company1[company1.id == ground_truth_matches.idCompany1[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>18398718226932431716</td>\n",
       "      <td>ilife '06 mac - apple - ma166z/a</td>\n",
       "      <td>ilife '06 the must-have update to apple's awar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.8 gbp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                              name  \\\n",
       "46  18398718226932431716  ilife '06 mac - apple - ma166z/a   \n",
       "\n",
       "                                          description manufacturer     price  \n",
       "46  ilife '06 the must-have update to apple's awar...          NaN  47.8 gbp  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company2[company2.id == ground_truth_matches.idCompany2[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 85\n",
      "Number of true positives: 70\n",
      "Number of false positives: 15\n",
      "Number of false negatives: 33\n",
      "Precision: 0.8235294117647058\n",
      "Recall: 0.6796116504854369\n",
      "F measure: 0.7446808510638298\n"
     ]
    }
   ],
   "source": [
    "##Methode Christophe\n",
    "number_of_matches = 0\n",
    "matches=[]\n",
    "for i in range(len(company1)):\n",
    "    tokens1name = nltk.word_tokenize(company1.iloc[i,1])\n",
    "    ng1_tokensname = set(nltk.ngrams(tokens1name, n=1))\n",
    "    #print(ng1_tokensname)\n",
    "    for j in range(len(company2)):\n",
    "        tokens2name = nltk.word_tokenize(company2.iloc[j,1])\n",
    "        ng2_tokensname = set(nltk.ngrams(tokens2name, n=1))\n",
    "        jd_ng1_ng2_name = nltk.jaccard_distance(ng1_tokensname, ng2_tokensname)\n",
    "        name_score = nltk.edit_distance(company1.iloc[i,1], company2.iloc[j,1])\n",
    "        if ((jd_ng1_ng2_name <= 0.75) or name_score<=1) :\n",
    "            number_of_matches = number_of_matches +1\n",
    "            matches.append((company1.iloc[i,0],company2.iloc[j,0]))\n",
    "print(\"Number of matches: {}\".format(number_of_matches))\n",
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df.columns= ['idCompany1','idCompany2']\n",
    "diff_df = pd.merge(ground_truth_matches, matches_df, how='outer', indicator='Exist')\n",
    "true_positives = diff_df[diff_df.Exist=='both']\n",
    "false_positives = diff_df[diff_df.Exist=='right_only']\n",
    "false_negatives = diff_df[diff_df.Exist=='left_only']\n",
    "print(\"Number of true positives: {}\".format(len(true_positives)))\n",
    "print(\"Number of false positives: {}\".format(len(false_positives)))\n",
    "print(\"Number of false negatives: {}\".format(len(false_negatives)))\n",
    "precision = len(true_positives)/(len(true_positives)+ len(false_positives))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "recall = len(true_positives)/(len(true_positives)+ len(false_negatives))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "f_measure = 2*(precision*recall)/(precision+recall)\n",
    "print(\"F measure: {}\".format(f_measure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 114\n",
      "Number of true positives: 87\n",
      "Number of false positives: 27\n",
      "Number of false negatives: 18\n",
      "Precision: 0.7631578947368421\n",
      "Recall: 0.8285714285714286\n",
      "F measure: 0.7945205479452055\n"
     ]
    }
   ],
   "source": [
    "##Methode 2 : jaccard distance en mixant nom et manufacturer\n",
    "number_of_matches = 0\n",
    "matches=[]\n",
    "for i in range(len(company1)):\n",
    "    tokens1name = nltk.word_tokenize(company1.iloc[i,1] + ' ' + str(company1.iloc[i,3]))\n",
    "    ng1_tokensname = set(nltk.ngrams(tokens1name, n=1))\n",
    "    #print(ng1_tokensname)\n",
    "    for j in range(len(company2)):\n",
    "        tokens2name = nltk.word_tokenize(company2.iloc[j,1] + ' ' + str(company2.iloc[j,3]))\n",
    "        ng2_tokensname = set(nltk.ngrams(tokens2name, n=1))\n",
    "        jd_ng1_ng2_name = nltk.jaccard_distance(ng1_tokensname, ng2_tokensname)\n",
    "        name_score = nltk.edit_distance(company1.iloc[i,1] , company2.iloc[j,1] )\n",
    "        if (jd_ng1_ng2_name <= 0.75) : #or name_score<=1 :\n",
    "            number_of_matches = number_of_matches +1\n",
    "            matches.append((company1.iloc[i,0],company2.iloc[j,0]))\n",
    "print(\"Number of matches: {}\".format(number_of_matches))\n",
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df.columns= ['idCompany1','idCompany2']\n",
    "diff_df = pd.merge(ground_truth_matches, matches_df, how='outer', indicator='Exist')\n",
    "true_positives = diff_df[diff_df.Exist=='both']\n",
    "false_positives = diff_df[diff_df.Exist=='right_only']\n",
    "false_negatives = diff_df[diff_df.Exist=='left_only']\n",
    "print(\"Number of true positives: {}\".format(len(true_positives)))\n",
    "print(\"Number of false positives: {}\".format(len(false_positives)))\n",
    "print(\"Number of false negatives: {}\".format(len(false_negatives)))\n",
    "precision = len(true_positives)/(len(true_positives)+ len(false_positives))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "recall = len(true_positives)/(len(true_positives)+ len(false_negatives))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "f_measure = 2*(precision*recall)/(precision+recall)\n",
    "print(\"F measure: {}\".format(f_measure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 38\n",
      "Number of true positives: 38\n",
      "Number of false positives: 0\n",
      "Number of false negatives: 64\n",
      "Precision: 1.0\n",
      "Recall: 0.37254901960784315\n",
      "F measure: 0.5428571428571428\n"
     ]
    }
   ],
   "source": [
    "##Methode 3 : idem avec des n-grams de 2\n",
    "number_of_matches = 0\n",
    "matches=[]\n",
    "for i in range(len(company1)):\n",
    "    tokens1name = nltk.word_tokenize(company1.iloc[i,1] + ' ' + str(company1.iloc[i,3]))\n",
    "    ng1_tokensname = set(nltk.ngrams(tokens1name, n=2))\n",
    "    #print(ng1_tokensname)\n",
    "    for j in range(len(company2)):\n",
    "        tokens2name = nltk.word_tokenize(company2.iloc[j,1] + ' ' + str(company2.iloc[j,3]))\n",
    "        ng2_tokensname = set(nltk.ngrams(tokens2name, n=2))\n",
    "        jd_ng1_ng2_name = nltk.jaccard_distance(ng1_tokensname, ng2_tokensname)\n",
    "        name_score = nltk.edit_distance(company1.iloc[i,1] , company2.iloc[j,1] )\n",
    "        if (jd_ng1_ng2_name <= 0.7) : #or name_score<=1 :\n",
    "            number_of_matches = number_of_matches +1\n",
    "            matches.append((company1.iloc[i,0],company2.iloc[j,0]))\n",
    "print(\"Number of matches: {}\".format(number_of_matches))\n",
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df.columns= ['idCompany1','idCompany2']\n",
    "diff_df = pd.merge(ground_truth_matches, matches_df, how='outer', indicator='Exist')\n",
    "true_positives = diff_df[diff_df.Exist=='both']\n",
    "false_positives = diff_df[diff_df.Exist=='right_only']\n",
    "false_negatives = diff_df[diff_df.Exist=='left_only']\n",
    "print(\"Number of true positives: {}\".format(len(true_positives)))\n",
    "print(\"Number of false positives: {}\".format(len(false_positives)))\n",
    "print(\"Number of false negatives: {}\".format(len(false_negatives)))\n",
    "precision = len(true_positives)/(len(true_positives)+ len(false_positives))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "recall = len(true_positives)/(len(true_positives)+ len(false_negatives))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "f_measure = 2*(precision*recall)/(precision+recall)\n",
    "print(\"F measure: {}\".format(f_measure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 25\n",
      "Number of true positives: 25\n",
      "Number of false positives: 0\n",
      "Number of false negatives: 75\n",
      "Precision: 1.0\n",
      "Recall: 0.25\n",
      "F measure: 0.4\n"
     ]
    }
   ],
   "source": [
    "##Methode 4 : idem avec des n-grams de 3\n",
    "number_of_matches = 0\n",
    "matches=[]\n",
    "for i in range(len(company1)):\n",
    "    tokens1name = nltk.word_tokenize(company1.iloc[i,1] + ' ' + str(company1.iloc[i,3]))\n",
    "    ng1_tokensname = set(nltk.ngrams(tokens1name, n=3))\n",
    "    #print(ng1_tokensname)\n",
    "    for j in range(len(company2)):\n",
    "        tokens2name = nltk.word_tokenize(company2.iloc[j,1] + ' ' + str(company2.iloc[j,3]))\n",
    "        ng2_tokensname = set(nltk.ngrams(tokens2name, n=3))\n",
    "        jd_ng1_ng2_name = nltk.jaccard_distance(ng1_tokensname, ng2_tokensname)\n",
    "        name_score = nltk.edit_distance(company1.iloc[i,1] , company2.iloc[j,1] )\n",
    "        if (jd_ng1_ng2_name <= 0.75) : #or name_score<=1 :\n",
    "            number_of_matches = number_of_matches +1\n",
    "            matches.append((company1.iloc[i,0],company2.iloc[j,0]))\n",
    "print(\"Number of matches: {}\".format(number_of_matches))\n",
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df.columns= ['idCompany1','idCompany2']\n",
    "diff_df = pd.merge(ground_truth_matches, matches_df, how='outer', indicator='Exist')\n",
    "true_positives = diff_df[diff_df.Exist=='both']\n",
    "false_positives = diff_df[diff_df.Exist=='right_only']\n",
    "false_negatives = diff_df[diff_df.Exist=='left_only']\n",
    "print(\"Number of true positives: {}\".format(len(true_positives)))\n",
    "print(\"Number of false positives: {}\".format(len(false_positives)))\n",
    "print(\"Number of false negatives: {}\".format(len(false_negatives)))\n",
    "precision = len(true_positives)/(len(true_positives)+ len(false_positives))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "recall = len(true_positives)/(len(true_positives)+ len(false_negatives))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "f_measure = 2*(precision*recall)/(precision+recall)\n",
    "print(\"F measure: {}\".format(f_measure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 38\n",
      "Number of true positives: 32\n",
      "Number of false positives: 6\n",
      "Number of false negatives: 70\n",
      "Precision: 0.8421052631578947\n",
      "Recall: 0.3137254901960784\n",
      "F measure: 0.45714285714285713\n"
     ]
    }
   ],
   "source": [
    "##Methode 4 : jaccard distance en mixant nom, description et manufacturer\n",
    "number_of_matches = 0\n",
    "matches=[]\n",
    "for i in range(len(company1)):\n",
    "    tokens1name = nltk.word_tokenize(company1.iloc[i,1] + ' ' + str(company1.iloc[i,2]) + ' ' + str(company1.iloc[i,3]))\n",
    "    ng1_tokensname = set(nltk.ngrams(tokens1name, n=1))\n",
    "    #print(ng1_tokensname)\n",
    "    for j in range(len(company2)):\n",
    "        tokens2name = nltk.word_tokenize(company2.iloc[j,1] + ' ' + str(company2.iloc[j,2]) + ' ' + str(company2.iloc[j,3]))\n",
    "        ng2_tokensname = set(nltk.ngrams(tokens2name, n=1))\n",
    "        jd_ng1_ng2_name = nltk.jaccard_distance(ng1_tokensname, ng2_tokensname)\n",
    "        name_score = nltk.edit_distance(company1.iloc[i,1] , company2.iloc[j,1] )\n",
    "        if (jd_ng1_ng2_name <= 0.8) : #or name_score<=1 :\n",
    "            number_of_matches = number_of_matches +1\n",
    "            matches.append((company1.iloc[i,0],company2.iloc[j,0]))\n",
    "print(\"Number of matches: {}\".format(number_of_matches))\n",
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df.columns= ['idCompany1','idCompany2']\n",
    "diff_df = pd.merge(ground_truth_matches, matches_df, how='outer', indicator='Exist')\n",
    "true_positives = diff_df[diff_df.Exist=='both']\n",
    "false_positives = diff_df[diff_df.Exist=='right_only']\n",
    "false_negatives = diff_df[diff_df.Exist=='left_only']\n",
    "print(\"Number of true positives: {}\".format(len(true_positives)))\n",
    "print(\"Number of false positives: {}\".format(len(false_positives)))\n",
    "print(\"Number of false negatives: {}\".format(len(false_negatives)))\n",
    "precision = len(true_positives)/(len(true_positives)+ len(false_positives))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "recall = len(true_positives)/(len(true_positives)+ len(false_negatives))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "f_measure = 2*(precision*recall)/(precision+recall)\n",
    "print(\"F measure: {}\".format(f_measure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 91\n",
      "Number of true positives: 81\n",
      "Number of false positives: 10\n",
      "Number of false negatives: 24\n",
      "Precision: 0.8901098901098901\n",
      "Recall: 0.7714285714285715\n",
      "F measure: 0.8265306122448981\n"
     ]
    }
   ],
   "source": [
    "##Methode 5 : methode 2 + price ratio<3\n",
    "number_of_matches = 0\n",
    "matches=[]\n",
    "for i in range(len(company1)):\n",
    "    try :  \n",
    "        price1 = float(company1.iloc[i,4]) \n",
    "    except : \n",
    "        price1 = 0\n",
    "    tokens1name = nltk.word_tokenize(str(company1.iloc[i,3]) + ' ' + company1.iloc[i,1])\n",
    "    ng1_tokensname = set(nltk.ngrams(tokens1name, n=1))\n",
    "    #print(ng1_tokensname)\n",
    "    for j in range(len(company2)):\n",
    "        try :  \n",
    "            price2 = float(company2.iloc[j,4]) \n",
    "        except : \n",
    "            price2 = 0\n",
    "        if price1* price2 == 0:\n",
    "            price_ratio=1\n",
    "        else:\n",
    "            price_ratio =max(price1, price2)/min(price1, price2)\n",
    "        tokens2name = nltk.word_tokenize(str(company2.iloc[j,3]) + ' ' + company2.iloc[j,1]  )\n",
    "        ng2_tokensname = set(nltk.ngrams(tokens2name, n=1))\n",
    "        jd_ng1_ng2_name = nltk.jaccard_distance(ng1_tokensname, ng2_tokensname)\n",
    "        name_score = nltk.edit_distance(company1.iloc[i,1] , company2.iloc[j,1] )\n",
    "        if ((jd_ng1_ng2_name <= 0.65) and price_ratio<3) or ((jd_ng1_ng2_name <= 0.75) and price_ratio<1.25): #or name_score<=1 :\n",
    "            number_of_matches = number_of_matches +1\n",
    "            matches.append((company1.iloc[i,0],company2.iloc[j,0]))\n",
    "print(\"Number of matches: {}\".format(number_of_matches))\n",
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df.columns= ['idCompany1','idCompany2']\n",
    "diff_df = pd.merge(ground_truth_matches, matches_df, how='outer', indicator='Exist')\n",
    "true_positives = diff_df[diff_df.Exist=='both']\n",
    "false_positives = diff_df[diff_df.Exist=='right_only']\n",
    "false_negatives = diff_df[diff_df.Exist=='left_only']\n",
    "print(\"Number of true positives: {}\".format(len(true_positives)))\n",
    "print(\"Number of false positives: {}\".format(len(false_positives)))\n",
    "print(\"Number of false negatives: {}\".format(len(false_negatives)))\n",
    "precision = len(true_positives)/(len(true_positives)+ len(false_positives))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "recall = len(true_positives)/(len(true_positives)+ len(false_negatives))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "f_measure = 2*(precision*recall)/(precision+recall)\n",
    "print(\"F measure: {}\".format(f_measure))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, the two records share the same value for attributes 'name' and 'address'. However, they have slightly different values for the columns 'city' and 'cuisine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>b000bnkbdm</td>\n",
       "      <td>sound studio 3: record edit add effects to aud...</td>\n",
       "      <td>sound studio 3 is an easy-to-use mac os x appl...</td>\n",
       "      <td>freeverse software</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title  \\\n",
       "27  b000bnkbdm  sound studio 3: record edit add effects to aud...   \n",
       "\n",
       "                                          description        manufacturer  \\\n",
       "27  sound studio 3 is an easy-to-use mac os x appl...  freeverse software   \n",
       "\n",
       "    price  \n",
       "27  79.99  "
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=8\n",
    "company1[company1['id']==false_negatives.iloc[f,0]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>18374143831267894200</td>\n",
       "      <td>sound studio 3 for mac</td>\n",
       "      <td>easytouse mac os x application for recording a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                    name  \\\n",
       "78  18374143831267894200  sound studio 3 for mac   \n",
       "\n",
       "                                          description manufacturer  price  \n",
       "78  easytouse mac os x application for recording a...          NaN  79.99  "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company2[company2['id']==false_negatives.iloc[f,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nan',\n",
       " 'weekly',\n",
       " 'reader',\n",
       " 'preparing',\n",
       " 'for',\n",
       " 'kindergarten',\n",
       " '2008',\n",
       " '(',\n",
       " 'pc/mac',\n",
       " ')',\n",
       " 'fogware']"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens2name "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, on the other hand, the two records are associated with different names, cities and cuisiones.\n",
    "\n",
    "This file represents a simple example of datasets, on which we can experiment with th etechniques presented in the course to try identify duplicates, without using (that is relying on the values of) the column \"unique_id\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by adding a new column to identify the records (lines) in our dataframe\n",
    "df_restaurants.insert(0,'record_ID', range(0, len(df_restaurants)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exhaustive comparisons: every record is compared with every other record\n",
    "\n",
    "We start by applying an exhaustive strategy whereby every record in the CSV file, is compared with every other record. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below does this for us. In doing so, it uses the following rule:\n",
    "\n",
    "For two records to match, i.e. refer to the same restaurant in the real world:\n",
    "* The edit distance between the attribute name values of the two records needs to be smaller or equal to 3, and \n",
    "* they need to have the same value for the cuisine attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants[df_restaurants.record_ID.isin([43, 622])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = len(df_restaurants)\n",
    "matches = []\n",
    "matchescomplet = []\n",
    "\n",
    "number_of_matches = 0\n",
    "tokens1=[]\n",
    "tokens2=[]\n",
    "start = time.process_time()\n",
    "for i in range(0,num_records):\n",
    "    \n",
    "    # Après tokenization , calcul du ngrams (n=1) pour le name qui servira pour la Jaccard distance, pour la ligne i\n",
    "    tokens1name = nltk.word_tokenize(df_restaurants.iloc[i,1]) \n",
    "    ng1_tokensname = set(nltk.ngrams(tokens1name, n=1))\n",
    "    \n",
    "    # Après tokenization , calcul du ngrams (n=1) pour l'adresse qui servira pour la Jaccard distance,, pour la ligne i\n",
    "    tokens1adr = nltk.word_tokenize(df_restaurants.iloc[i,2]) \n",
    "    ng1_tokensadr = set(nltk.ngrams(tokens1adr, n=1))\n",
    "    \n",
    "    \n",
    "    for j in range(i+1,num_records):\n",
    "        \n",
    "        # Après tokenization , calcul du ngrams (n=1) pour le name qui servira pour la Jaccard distance, , pour la ligne j\n",
    "        tokens2name = nltk.word_tokenize( df_restaurants.iloc[j,1]) \n",
    "        ng2_tokensname = set(nltk.ngrams(tokens2name, n=1))\n",
    "        \n",
    "        # Après tokenization , calcul du ngrams (n=1) pour le name qui servira pour la Jaccard distance, , pour la ligne j\n",
    "        tokens2adr = nltk.word_tokenize( df_restaurants.iloc[j,2]) \n",
    "        ng2_tokensadr = set(nltk.ngrams(tokens2adr, n=1))\n",
    "       \n",
    "        # calcul de la Jaccard distance pour le name entre la ligne i et la ligne j (\"item based\" avec ngrams (n=1)) \n",
    "        jd_ng1_ng2_name = nltk.jaccard_distance(ng1_tokensname, ng2_tokensname)  # jaccard distance entre les ngram=1 des names\n",
    "        \n",
    "        # calcul de la Jaccard distance pour l'adresse entre la ligne i et la ligne j (\"item based\" avec ngrams (n=1)) \n",
    "        jd_ng1_ng2_adr = nltk.jaccard_distance(ng1_tokensadr, ng2_tokensadr)  # jaccard distance entre les ngram=1 des adresses\n",
    "    \n",
    "        # Rule for matching: \n",
    "        # disjonction entre une similarité entre les names (name_score<=1) \n",
    "        # et une similarité conjugée entre les adresses et les noms (jd_ng1_ng2_adr <= 0.6 and jd_ng1_ng2_name <= 0.6)\n",
    "        name_score = nltk.edit_distance(df_restaurants.iloc[i,1], df_restaurants.iloc[j,1])\n",
    "        \n",
    "        # Rule for matching: Distance between names is smaller or equal to 3 and the cuisine is the same \n",
    "        if (jd_ng1_ng2_adr <= 0.6 and jd_ng1_ng2_name <= 0.6) or name_score<=1 :\n",
    "            number_of_matches = number_of_matches +1 \n",
    "            # matchescomplet.append((df_restaurants.iloc[i,0],df_restaurants.iloc[i,1], df_restaurants.iloc[i,2],df_restaurants.iloc[i,5], df_restaurants.iloc[j,0],df_restaurants.iloc[j,1], df_restaurants.iloc[j,2],df_restaurants.iloc[j,5]))\n",
    "            matches.append((df_restaurants.iloc[i,0],df_restaurants.iloc[j,0]))\n",
    "\n",
    "end = time.process_time()\n",
    "\n",
    "print(\"Number of matches: {}\".format(number_of_matches))\n",
    "print(\"Processing time: {}\".format(end - start))\n",
    "for _ in matchescomplet:\n",
    "     print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quelques tests pour ajuster les critères de notre algorithme\n",
    "name_score = nltk.edit_distance(df_restaurants.iloc[73,1], df_restaurants.iloc[763,1])\n",
    "name_score   # 11\n",
    "# comme on le voit ci-dessous, la différence est le mot \"restaurant\", l'edit distance est très importante (11), \n",
    "# on ne peut pas se baser dessus pour dire que c le même resto, il faut qu'on ajoute un critère \"item based\" \n",
    "# en plus du critère edit_distance name_score<=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qq tests pour ajuster les critères de notre alogorithme\n",
    "df_restaurants[df_restaurants.record_ID.isin([73, 763])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_score = nltk.edit_distance(df_restaurants.iloc[32,1], df_restaurants.iloc[759,1])\n",
    "#print(name_score)\n",
    "tokens1 = nltk.word_tokenize(df_restaurants.iloc[73,1]) \n",
    "tokens2 = nltk.word_tokenize( df_restaurants.iloc[763,1]) \n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "ng1_tokens = set(nltk.ngrams(tokens1, n=1))\n",
    "ng2_tokens = set(nltk.ngrams(tokens2, n=1))\n",
    "print(ng1_tokens)\n",
    "print(ng2_tokens)\n",
    "\n",
    "jd_sent_1_2 = nltk.jaccard_distance(ng1_tokens, ng2_tokens)\n",
    "print(jd_sent_1_2)\n",
    "# jd_ng1_ng2_adr <= 0.6,ce seuil de 0.6 suffira dire que les lignes 32 et 759 sont le même restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adresse\n",
    "#print(name_score)\n",
    "tokens1 = nltk.word_tokenize(df_restaurants.iloc[73,2]) \n",
    "tokens2 = nltk.word_tokenize( df_restaurants.iloc[763,2]) \n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "ng1_tokens = set(nltk.ngrams(tokens1, n=1))\n",
    "ng2_tokens = set(nltk.ngrams(tokens2, n=1))\n",
    "print(ng1_tokens)\n",
    "print(ng2_tokens)\n",
    "\n",
    "jd_sent_1_2 = nltk.jaccard_distance(ng1_tokens, ng2_tokens)\n",
    "print(jd_sent_1_2)\n",
    "# ça ne passe pas , mais c pas grave car mettre le seuil à 0.67 va nous rajouter beaucoup de faux positifs \n",
    "# on a testé ce seuil plus élevé de 0.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_score = nltk.edit_distance(df_restaurants.iloc[6,1], df_restaurants.iloc[754,1])\n",
    "name_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_score = nltk.edit_distance(df_restaurants.iloc[32,1], df_restaurants.iloc[759,1])\n",
    "#print(name_score)\n",
    "tokens1 = nltk.word_tokenize(df_restaurants.iloc[6,1]) \n",
    "tokens2 = nltk.word_tokenize( df_restaurants.iloc[754,1]) \n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "ng1_tokens = set(nltk.ngrams(tokens1, n=1))\n",
    "ng2_tokens = set(nltk.ngrams(tokens2, n=1))\n",
    "print(ng1_tokens)\n",
    "print(ng2_tokens)\n",
    "\n",
    "jd_sent_1_2 = nltk.jaccard_distance(ng1_tokens, ng2_tokens)\n",
    "print(jd_sent_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_score = nltk.edit_distance(df_restaurants.iloc[6,1], df_restaurants.iloc[754,1])\n",
    "name_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_score = nltk.edit_distance(df_restaurants.iloc[32,1], df_restaurants.iloc[759,1])\n",
    "#print(name_score)\n",
    "tokens1 = nltk.word_tokenize(df_restaurants.iloc[32,1]) \n",
    "tokens2 = nltk.word_tokenize( df_restaurants.iloc[759,1]) \n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "ng1_tokens = set(nltk.ngrams(tokens1, n=1))\n",
    "ng2_tokens = set(nltk.ngrams(tokens2, n=1))\n",
    "print(ng1_tokens)\n",
    "print(ng2_tokens)\n",
    "\n",
    "jd_sent_1_2 = nltk.jaccard_distance(ng1_tokens, ng2_tokens)\n",
    "print(jd_sent_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "for match in matches:\n",
    "    print(\"The following records {} and {} match\".format(match[0],match[1]))\n",
    "    print(\"The restaurants with the following names {} and {} match.\".format(df_restaurants.iloc[match[0],1],df_restaurants.iloc[match[1],1]))\n",
    "    print(\"The restaurants with the following addresses {} and {} match.\".format(df_restaurants.iloc[match[0],2],df_restaurants.iloc[match[1],2]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the rule applied in the above code is not great. You may want to try other kind of distances, other thresholds, and other rules to identify matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing the quality of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we first need to compute the ground truth (that is the list of correct matches) considering the attribute unique_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches = pd.read_csv(\"./restaurants.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches.insert(0, 'record_ID', range(0, len(ground_truth_matches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches = pd.merge(ground_truth_matches,\n",
    "                                ground_truth_matches,\n",
    "                                on = 'unique_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ground_truth_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches = ground_truth_matches.query('record_ID_x < record_ID_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches = ground_truth_matches[['record_ID_x','record_ID_y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ground_truth_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ground_truth_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(matches))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df.columns= ['record_ID_x','record_ID_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on s'assure que les couples record_ID_x et record_ID_y sont dans le bons sens (record_ID_x < record_ID_y)\n",
    "# comme dans ground_truth\n",
    "matches_df[matches_df['record_ID_x'] >= matches_df['record_ID_y'] ]\n",
    "# 0 lignes trouvées , donc c OK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df = pd.merge(ground_truth_matches, matches_df, how='outer', indicator='Exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives = diff_df[diff_df.Exist=='both']\n",
    "false_positives = diff_df[diff_df.Exist=='right_only']\n",
    "false_negatives = diff_df[diff_df.Exist=='left_only']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idCompany1</th>\n",
       "      <th>idCompany2</th>\n",
       "      <th>Exist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>b0002ibev4</td>\n",
       "      <td>1887899244694755891</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>b0007lw22g</td>\n",
       "      <td>18398718226932431716</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>b0001wn16m</td>\n",
       "      <td>10092468528845066077</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>b000ozhfsq</td>\n",
       "      <td>18438075297130458214</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>b00006sijr</td>\n",
       "      <td>6247936198343071793</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idCompany1            idCompany2 Exist\n",
       "0  b0002ibev4   1887899244694755891  both\n",
       "1  b0007lw22g  18398718226932431716  both\n",
       "5  b0001wn16m  10092468528845066077  both\n",
       "7  b000ozhfsq  18438075297130458214  both\n",
       "9  b00006sijr   6247936198343071793  both"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# les vrais duplicats que notre algo a pu détecter\n",
    "true_positives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of a true positive\n",
    "df_restaurants[df_restaurants.record_ID.isin(['6','754'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idCompany1</th>\n",
       "      <th>idCompany2</th>\n",
       "      <th>Exist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>b0000c6fjm</td>\n",
       "      <td>18394964067436310447</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>b0000c6fjm</td>\n",
       "      <td>10092468528845066077</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>b000ndibq4</td>\n",
       "      <td>9761533219806554318</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>b000o27t8a</td>\n",
       "      <td>9761533219806554318</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>b000o27t8a</td>\n",
       "      <td>14054232840925252286</td>\n",
       "      <td>right_only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     idCompany1            idCompany2       Exist\n",
       "103  b0000c6fjm  18394964067436310447  right_only\n",
       "104  b0000c6fjm  10092468528845066077  right_only\n",
       "105  b000ndibq4   9761533219806554318  right_only\n",
       "106  b000o27t8a   9761533219806554318  right_only\n",
       "107  b000o27t8a  14054232840925252286  right_only"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notre algo les a sortis comme restos en double mais c pas vrai\n",
    "false_positives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notre critère de duplicate :\n",
    "# (jd_ng1_ng2_adr <= 0.6 and jd_ng1_ng2_name <= 0.6) or (name_score<=1 and jd_ng1_ng2_adr <= 0.6) \n",
    "# eliminer grace jd_ng1_ng2_adr = 0.6666\n",
    "df_restaurants[df_restaurants.record_ID.isin(['55','56'])]\n",
    "# le name est le même donc l'algo dit que ce le même restaurant alors que ce n'est pas vrai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pareil c pas le même resto alors que l'algo les a retenu comme duplicate\n",
    "# car les names diffèrent d'un seul caractère.\n",
    "df_restaurants[df_restaurants.record_ID.isin(['87','88'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_score<=1\n",
    "name_score = nltk.edit_distance(df_restaurants.iloc[87,1], df_restaurants.iloc[88,1])\n",
    "name_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idCompany1</th>\n",
       "      <th>idCompany2</th>\n",
       "      <th>Exist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>b000ofnri8</td>\n",
       "      <td>12244614697089679523</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>b0007d8r5k</td>\n",
       "      <td>13775362651326388438</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>b00099qrok</td>\n",
       "      <td>9755705822363275907</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>b0002e3g6o</td>\n",
       "      <td>18384557845547191313</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>b0009yx9be</td>\n",
       "      <td>1021042895134770712</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idCompany1            idCompany2      Exist\n",
       "2  b000ofnri8  12244614697089679523  left_only\n",
       "3  b0007d8r5k  13775362651326388438  left_only\n",
       "4  b00099qrok   9755705822363275907  left_only\n",
       "6  b0002e3g6o  18384557845547191313  left_only\n",
       "8  b0009yx9be   1021042895134770712  left_only"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# les vrais duplicates que l'algo n'a pas détecté\n",
    "false_negatives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants[df_restaurants.record_ID.isin(['32','759'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faux négatif\n",
    "# pour l'algo le 32 et le 759 c'est pas le même restaurant, pourtant c le même\n",
    "# en effet les names diffèrents en lettres et en mots : \n",
    "# name_score > 1 et jd_ng1_ng2_name > 0.6 (ça suffit pour l'algo pour l'éliminer ) et en plus jd_ng1_ng2_adr > 0.6\n",
    "name_score = nltk.edit_distance(df_restaurants.iloc[32,1], df_restaurants.iloc[759,1])\n",
    "name_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # (jd_ng1_ng2_adr <= 0.6) and jd_ng1_ng2_name <= 0.6) or (name_score<=1)\n",
    "    \n",
    "# name_score = nltk.edit_distance(df_restaurants.iloc[32,1], df_restaurants.iloc[759,1])\n",
    "\n",
    "tokens1 = nltk.word_tokenize(df_restaurants.iloc[32,1])   # name\n",
    "tokens2 = nltk.word_tokenize( df_restaurants.iloc[759,1]) \n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "ng1_tokens = set(nltk.ngrams(tokens1, n=1))\n",
    "ng2_tokens = set(nltk.ngrams(tokens2, n=1))\n",
    "print(ng1_tokens)\n",
    "print(ng2_tokens)\n",
    "\n",
    "jd_sent_1_2 = nltk.jaccard_distance(ng1_tokens, ng2_tokens)\n",
    "print(jd_sent_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # (jd_ng1_ng2_adr <= 0.6) and (name_score<=2 or jd_ng1_ng2_name <= 0.67)\n",
    "    \n",
    "# name_score = nltk.edit_distance(df_restaurants.iloc[32,1], df_restaurants.iloc[759,1])\n",
    "#print(name_score)\n",
    "tokens1 = nltk.word_tokenize(df_restaurants.iloc[73,2])   # adresse \n",
    "tokens2 = nltk.word_tokenize( df_restaurants.iloc[763,2]) \n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "ng1_tokens = set(nltk.ngrams(tokens1, n=1))\n",
    "ng2_tokens = set(nltk.ngrams(tokens2, n=1))\n",
    "print(ng1_tokens)\n",
    "print(ng2_tokens)\n",
    "\n",
    "jd_sent_1_2 = nltk.jaccard_distance(ng1_tokens, ng2_tokens)\n",
    "print(jd_sent_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ground_truth_matches))\n",
    "print(len(matches_df))\n",
    "print(len(true_positives) , 'true_positives')\n",
    "print(len(false_positives) ,'false_positives')\n",
    "print(len(false_negatives)  , 'false_negatives')\n",
    "\n",
    "# len(true_positives)  +  len(false_negatives) = len(ground_truth_matches)\n",
    "\n",
    "# len(matches_df)) - len(false_positif) + len(false_negatives)     = ground_truth_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = len(true_positives)/(len(true_positives)+ len(false_positives))\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you are using pyton 2.7 (instead of Python 3), you would need to convert integers to float prior to performing the division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = len(true_positives)/(len(true_positives)+ len(false_negatives))\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_measure = 2*(precision*recall)/(precision+recall)\n",
    "print(f_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windowing (SNM) method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 841 842\n",
    "# qq tests pour choisir sur quel champ on va faire le sort \n",
    "# le sorted name parait intéressant\n",
    "df_restaurants.sort_values(by=['name']).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le tri est fait dans ce qui suit selon le champ \"name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 50   # \n",
    "\n",
    "# tri par name car c ce qui permet d'avoir des resto en double les plus proches possibles \n",
    "\n",
    "df_restaurants= df_restaurants.sort_values(by=['name'])  \n",
    "\n",
    "number_of_matchesw = 0\n",
    "num_records = len(df_restaurants)\n",
    "matchesw = []\n",
    "matchescompletw = []\n",
    "\n",
    "start = time.process_time()\n",
    "for i in range(0,min(window,len(df_restaurants))):\n",
    "    \n",
    "    tokens1name = nltk.word_tokenize(df_restaurants.iloc[i,1]) \n",
    "    ng1_tokensname = set(nltk.ngrams(tokens1name, n=1))\n",
    "    \n",
    "    tokens1adr = nltk.word_tokenize(df_restaurants.iloc[i,2]) \n",
    "    ng1_tokensadr = set(nltk.ngrams(tokens1adr, n=1))\n",
    "    \n",
    "    \n",
    "    for j in range(i+1,min(window,len(df_restaurants))):\n",
    "        tokens2name = nltk.word_tokenize( df_restaurants.iloc[j,1]) \n",
    "        ng2_tokensname = set(nltk.ngrams(tokens2name, n=1))\n",
    "        \n",
    "        \n",
    "        tokens2adr = nltk.word_tokenize( df_restaurants.iloc[j,2]) \n",
    "        ng2_tokensadr = set(nltk.ngrams(tokens2adr, n=1))\n",
    "#         print(tokens1)\n",
    "#         print(tokens2)       \n",
    "        jd_ng1_ng2_name = nltk.jaccard_distance(ng1_tokensname, ng2_tokensname)  # jaccard distance entre les ngram=1 des names\n",
    "        jd_ng1_ng2_adr = nltk.jaccard_distance(ng1_tokensadr, ng2_tokensadr)  # jaccard distance entre les ngram=1 des adresses\n",
    "    \n",
    "        name_score = nltk.edit_distance(df_restaurants.iloc[i,1], df_restaurants.iloc[j,1])\n",
    "        \n",
    "        # Rule for matching: Distance between names is smaller or equal to 3 and the cuisine is the same \n",
    "        if (jd_ng1_ng2_adr <= 0.6 and jd_ng1_ng2_name <= 0.6) or name_score<=1 :\n",
    "            number_of_matchesw = number_of_matchesw +1 \n",
    "            # matchescomplet.append((df_restaurants.iloc[i,0],df_restaurants.iloc[i,1], df_restaurants.iloc[i,2],df_restaurants.iloc[i,5], df_restaurants.iloc[j,0],df_restaurants.iloc[j,1], df_restaurants.iloc[j,2],df_restaurants.iloc[j,5]))\n",
    "            matchesw.append((df_restaurants.iloc[i,0],df_restaurants.iloc[j,0]))\n",
    "            matchescompletw.append((df_restaurants.iloc[i,0],df_restaurants.iloc[i,1], df_restaurants.iloc[i,2],df_restaurants.iloc[i,5], df_restaurants.iloc[j,0],df_restaurants.iloc[j,1], df_restaurants.iloc[j,2],df_restaurants.iloc[j,5]))\n",
    "                     \n",
    "            \n",
    "            \n",
    "for i in range(window,len(df_restaurants)):\n",
    "    \n",
    "    tokens1name = nltk.word_tokenize(df_restaurants.iloc[i,1]) \n",
    "    ng1_tokensname = set(nltk.ngrams(tokens1name, n=1))\n",
    "    \n",
    "    tokens1adr = nltk.word_tokenize(df_restaurants.iloc[i,2]) \n",
    "    ng1_tokensadr = set(nltk.ngrams(tokens1adr, n=1))\n",
    "    \n",
    "    \n",
    "    for j in range(i-window+1,i):\n",
    "        tokens2name = nltk.word_tokenize( df_restaurants.iloc[j,1]) \n",
    "        ng2_tokensname = set(nltk.ngrams(tokens2name, n=1))\n",
    "        \n",
    "        \n",
    "        tokens2adr = nltk.word_tokenize( df_restaurants.iloc[j,2]) \n",
    "        ng2_tokensadr = set(nltk.ngrams(tokens2adr, n=1))\n",
    "     \n",
    "        jd_ng1_ng2_name = nltk.jaccard_distance(ng1_tokensname, ng2_tokensname)  # jaccard distance entre les ngram=1 des names\n",
    "        jd_ng1_ng2_adr = nltk.jaccard_distance(ng1_tokensadr, ng2_tokensadr)  # jaccard distance entre les ngram=1 des adresses\n",
    "    \n",
    "        name_score = nltk.edit_distance(df_restaurants.iloc[i,1], df_restaurants.iloc[j,1])\n",
    "        \n",
    "        # Rule for matching: Distance between names is smaller or equal to 3 and the cuisine is the same \n",
    "        if (jd_ng1_ng2_adr <= 0.6 and jd_ng1_ng2_name <= 0.6) or name_score<=1 :\n",
    "            number_of_matchesw = number_of_matchesw +1 \n",
    "            # matchescomplet.append((df_restaurants.iloc[i,0],df_restaurants.iloc[i,1], df_restaurants.iloc[i,2],df_restaurants.iloc[i,5], df_restaurants.iloc[j,0],df_restaurants.iloc[j,1], df_restaurants.iloc[j,2],df_restaurants.iloc[j,5]))\n",
    "            matchesw.append((df_restaurants.iloc[i,0],df_restaurants.iloc[j,0]))\n",
    "            matchescompletw.append((df_restaurants.iloc[i,0],df_restaurants.iloc[i,1], df_restaurants.iloc[i,2],df_restaurants.iloc[i,5], df_restaurants.iloc[j,0],df_restaurants.iloc[j,1], df_restaurants.iloc[j,2],df_restaurants.iloc[j,5]))\n",
    "            \n",
    "end = time.process_time()\n",
    "\n",
    "print(\"Number of matches: {}\".format(number_of_matchesw))\n",
    "print(\"Processing time: {}\".format(end - start))            \n",
    "for _ in matchescompletw:\n",
    "     print(_)  \n",
    "# for _ in matches:\n",
    "#      print(_)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "for match in matchesw:\n",
    "    print(\"The following records {} and {} match\".format(match[0],match[1]))\n",
    "    print(\"The restaurants with the following names {} and {} match.\".format(df_restaurants.iloc[match[0],1],df_restaurants.iloc[match[1],1]))\n",
    "    print(\"The restaurants with the following addresses {} and {} match.\".format(df_restaurants.iloc[match[0],2],df_restaurants.iloc[match[1],2]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matchesw_df = pd.DataFrame(matchesw)\n",
    "matchesw_df.columns= ['record_ID_x','record_ID_y']\n",
    "\n",
    "matchesw_df['MIN'] = matchesw_df[['record_ID_x','record_ID_y']].min(axis=1)\n",
    "matchesw_df['MAX'] = matchesw_df[['record_ID_x','record_ID_y']].max(axis=1)\n",
    "matchesw_df=matchesw_df[['MIN','MAX']]\n",
    "matchesw_df.columns=['record_ID_x','record_ID_y']\n",
    "matchesw_df\n",
    "\n",
    "\n",
    "diffw_df = pd.merge(ground_truth_matches, matchesw_df, how='outer', indicator='Exist')\n",
    "true_positivesw = diffw_df[diffw_df.Exist=='both']\n",
    "false_positivesw = diffw_df[diffw_df.Exist=='right_only']\n",
    "false_negativesw = diffw_df[diffw_df.Exist=='left_only']\n",
    "precisionw = len(true_positivesw)/(len(true_positivesw)+ len(false_positivesw))\n",
    "print(precisionw)\n",
    "recallw = len(true_positivesw)/(len(true_positivesw)+ len(false_negativesw))\n",
    "print(recallw)\n",
    "f_measurew = 2*(precisionw*recallw)/(precisionw+recallw)\n",
    "print(f_measurew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ground_truth_matches))\n",
    "print(len(matchesw_df))\n",
    "print(len(true_positivesw))\n",
    "print(len(false_positivesw))\n",
    "print(len(false_negativesw))  \n",
    "# len(true_positives)  +  len(false_negatives) = len(ground_truth_matches)\n",
    "# len(matches_df)) - len(false_positif) + len(false_negatives)     = ground_truth_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that in the above code, we do not implement the SNM algorithm in its entirety. In particular, we do not implement the last phase of inferring matches using transitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blocking method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants = pd.read_csv(\"./restaurants.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_restaurants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by adding a new column to identify the records (lines) in our dataframe\n",
    "df_restaurants.insert(0,'record_ID', range(0, len(df_restaurants)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The blocks correspond to resturants that are located in the same citydf_restaurants.loc[df_restaurants['city']==' atlanta']\n",
    "df_restaurants.loc[df_restaurants['city']==' atlanta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants.loc[df_restaurants['city'].str.strip()=='atlanta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on va créer un dict \"df_restov\" des restaurants de chaque ville\n",
    "# pour une clé= ville, la valeur du dict serait égale à un dataframe représentant les restos de cette ville\n",
    "df_restov= {}\n",
    "for ville in df_restaurants['city'].unique():\n",
    "    \n",
    "    df_restov[ville]   = df_restaurants.loc[df_restaurants['city']==ville]\n",
    "    num_records = len(df_restov[ville])\n",
    "    print(ville)   # on affiche la ville\n",
    "    print(num_records) # on affiche le nombre de restos par ville\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on vérifie  pour atlanta que ça marche bien, on a bien le dataframe qu'on voudrait.\n",
    "print(type(df_restov[\" atlanta\"]))\n",
    "print(df_restov[\" atlanta\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restov[\" atlanta\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on testel'algo précédent sur juste un dataframe celui des restos de \" atlanta\"  (avec un espace devant)\n",
    "num_records = len(df_restov[\" atlanta\"])\n",
    "amatches = []\n",
    "amatchescomplet = []\n",
    "\n",
    "anumber_of_matches = 0\n",
    "tokens1=[]\n",
    "tokens2=[]\n",
    "start = time.process_time()\n",
    "for i in range(0,num_records):\n",
    "    \n",
    "    # Après tokenization , calcul du ngrams (n=1) pour le name qui servira pour la Jaccard distance, pour la ligne i\n",
    "    tokens1name = nltk.word_tokenize(df_restov[\" atlanta\"].iloc[i,1]) \n",
    "    ng1_tokensname = set(nltk.ngrams(tokens1name, n=1))\n",
    "    \n",
    "    # Après tokenization , calcul du ngrams (n=1) pour l'adresse qui servira pour la Jaccard distance,, pour la ligne i\n",
    "    tokens1adr = nltk.word_tokenize(df_restov[\" atlanta\"].iloc[i,2]) \n",
    "    ng1_tokensadr = set(nltk.ngrams(tokens1adr, n=1))\n",
    "    \n",
    "    \n",
    "    for j in range(i+1,num_records):\n",
    "        \n",
    "        # Après tokenization , calcul du ngrams (n=1) pour le name qui servira pour la Jaccard distance, , pour la ligne j\n",
    "        tokens2name = nltk.word_tokenize( df_restov[\" atlanta\"].iloc[j,1]) \n",
    "        ng2_tokensname = set(nltk.ngrams(tokens2name, n=1))\n",
    "        \n",
    "        # Après tokenization , calcul du ngrams (n=1) pour le name qui servira pour la Jaccard distance, , pour la ligne j\n",
    "        tokens2adr = nltk.word_tokenize( df_restov[\" atlanta\"].iloc[j,2]) \n",
    "        ng2_tokensadr = set(nltk.ngrams(tokens2adr, n=1))\n",
    "     \n",
    "        # calcul de la Jaccard distance pour le name entre la ligne i et la ligne j (\"item based\" avec ngrams (n=1)) \n",
    "        jd_ng1_ng2_name = nltk.jaccard_distance(ng1_tokensname, ng2_tokensname)\n",
    "        \n",
    "        # calcul de la Jaccard distance pour l'adresse entre la ligne i et la ligne j (\"item based\" avec ngrams (n=1)) \n",
    "        jd_ng1_ng2_adr = nltk.jaccard_distance(ng1_tokensadr, ng2_tokensadr)  \n",
    "    \n",
    "        name_score = nltk.edit_distance(df_restov[\" atlanta\"].iloc[i,1], df_restov[\" atlanta\"].iloc[j,1])\n",
    "        \n",
    "        # Rule for matching: \n",
    "        # disjonction entre une similarité entre les names (name_score<=1) \n",
    "        # et une similarité conjugée entre les adresses et les noms (jd_ng1_ng2_adr <= 0.6 and jd_ng1_ng2_name <= 0.6)\n",
    "        if (jd_ng1_ng2_adr <= 0.6 and jd_ng1_ng2_name <= 0.6) or name_score<=1 :\n",
    "            anumber_of_matches = anumber_of_matches +1 \n",
    "            matchescomplet.append((df_restov[\" atlanta\"].iloc[i,0],df_restov[\" atlanta\"].iloc[i,1], \\\n",
    "            df_restov[\" atlanta\"].iloc[i,2],df_restov[\" atlanta\"].iloc[i,3], df_restov[\" atlanta\"].iloc[i,5], \\\n",
    "            df_restov[\" atlanta\"].iloc[j,0],df_restov[\" atlanta\"].iloc[j,1], df_restov[\" atlanta\"].iloc[j,2], \\\n",
    "            df_restov[\" atlanta\"].iloc[j,3],df_restov[\" atlanta\"].iloc[j,5]))\n",
    "            amatches.append((df_restov[\" atlanta\"].iloc[i,0],df_restov[\" atlanta\"].iloc[j,0]))\n",
    "\n",
    "end = time.process_time()\n",
    "\n",
    "print(\"Number of matches: {}\".format(anumber_of_matches))\n",
    "print(\"Processing time: {}\".format(end - start))\n",
    "for _ in amatchescomplet:\n",
    "     print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nous allons refaire le dict mais en éliminant les espaces saisis avant et après chaque ville\n",
    "# par précaution pour éviter des villes en double\n",
    "# et nous allons imprimer le nombre de restos par ville.\n",
    "\n",
    "df_restov={}\n",
    "cumul= 0\n",
    "# il faut enlever les espaces au début et à la fin de chaque ville dans le dataframe, \n",
    "# sinon on va rater des restos en double car ils ne seront pas dans le même block.\n",
    "\n",
    "for ville in df_restaurants['city'].str.strip().unique():   \n",
    "     print(ville)\n",
    "     df_restov[ville]   = df_restaurants.loc[df_restaurants['city'].str.strip()==ville]\n",
    "     print(len(df_restov[ville]))\n",
    "     cumul += len(df_restov[ville])\n",
    "\n",
    "print(cumul)\n",
    "# on vérifie qu'on retrouve bien un total de 865 restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Généralisation de la BLOCKING METHOD à toutes les villes \n",
    "bmatches = []\n",
    "bmatchescomplet = []\n",
    "bnumber_of_matches = 0\n",
    "start = time.process_time()\n",
    "    \n",
    "for ville in df_restaurants['city'].str.strip().unique():\n",
    "        # affichage de la ville et du nombre de restos par ville\n",
    "        # pour les matcher entre eux\n",
    "        print(ville)  \n",
    "        num_records = len(df_restov[ville])\n",
    "        print(num_records)\n",
    "        \n",
    "        tokens1=[]\n",
    "        tokens2=[]\n",
    "       \n",
    "        for i in range(0,num_records):\n",
    "\n",
    "            tokens1name = nltk.word_tokenize(df_restov[ville].iloc[i,1]) \n",
    "            ng1_tokensname = set(nltk.ngrams(tokens1name, n=1))\n",
    "\n",
    "            tokens1adr = nltk.word_tokenize(df_restov[ville].iloc[i,2]) \n",
    "            ng1_tokensadr = set(nltk.ngrams(tokens1adr, n=1))\n",
    "\n",
    "\n",
    "            for j in range(i+1,num_records):\n",
    "\n",
    "                tokens2name = nltk.word_tokenize( df_restov[ville].iloc[j,1]) \n",
    "                ng2_tokensname = set(nltk.ngrams(tokens2name, n=1))\n",
    "\n",
    "\n",
    "                tokens2adr = nltk.word_tokenize( df_restov[ville].iloc[j,2]) \n",
    "                ng2_tokensadr = set(nltk.ngrams(tokens2adr, n=1))\n",
    "\n",
    "                jd_ng1_ng2_name = nltk.jaccard_distance(ng1_tokensname, ng2_tokensname)  # jaccard distance entre les ngram=1 des names\n",
    "                jd_ng1_ng2_adr = nltk.jaccard_distance(ng1_tokensadr, ng2_tokensadr)  # jaccard distance entre les ngram=1 des adresses\n",
    "\n",
    "                name_score = nltk.edit_distance(df_restov[ville].iloc[i,1], df_restov[ville].iloc[j,1])\n",
    "\n",
    "                # Rule for matching: Item based Jaccard Distance with ngram=1 between adresses and between names or edit distance between names \n",
    "                if (jd_ng1_ng2_adr <= 0.6 and jd_ng1_ng2_name <= 0.6) or name_score<=1 :\n",
    "                    bnumber_of_matches = bnumber_of_matches +1 \n",
    "                    bmatchescomplet.append((df_restov[ville].iloc[i,0],df_restov[ville].iloc[i,1], \\\n",
    "                    df_restov[ville].iloc[i,2],df_restov[ville].iloc[i,3], df_restov[ville].iloc[i,5], \\\n",
    "                    df_restov[ville].iloc[j,0],df_restov[ville].iloc[j,1], df_restov[ville].iloc[j,2], \\\n",
    "                    df_restov[ville].iloc[j,3],df_restov[ville].iloc[j,5]))\n",
    "                    bmatches.append((df_restov[ville].iloc[i,0],df_restov[ville].iloc[j,0]))\n",
    "\n",
    "end = time.process_time()\n",
    "\n",
    "print(\"Number of matches: {}\".format(bnumber_of_matches))\n",
    "print(\"Processing time: {}\".format(end - start))\n",
    "# for _ in matchescomplet:\n",
    "#        print(_)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rappel des résultats de l'algo original sans blocking:\n",
    "####  Number of matches: 127\n",
    "#### Processing time: 167.984375\n",
    "\n",
    "#### les infos de l'algo avec  blocking ci-dessus\n",
    "#### Number of matches: 67\n",
    "#### Processing time: 25.6875\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ground_truth_matches = pd.read_csv(\"./restaurants.csv\")\n",
    "len(ground_truth_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches.insert(0, 'record_ID', range(0, len(ground_truth_matches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches = pd.merge(ground_truth_matches,\n",
    "                                ground_truth_matches,\n",
    "                                on = 'unique_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches = ground_truth_matches.query('record_ID_x < record_ID_y')\n",
    "len(ground_truth_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_matches = ground_truth_matches[['record_ID_x','record_ID_y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmatches_df = pd.DataFrame(bmatches)\n",
    "bmatches_df.columns= ['record_ID_x','record_ID_y']\n",
    "bmatches_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on s'assure que les couples record_ID_x et record_ID_y sont dans le bons sens (record_ID_x < record_ID_y)\n",
    "bmatches_df[bmatches_df['record_ID_x'] >= bmatches_df['record_ID_y'] ]\n",
    "# 0 lignes trouvées , donc c OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df = pd.merge(ground_truth_matches, bmatches_df, how='outer', indicator='Exist')\n",
    "diff_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btrue_positives = diff_df[diff_df.Exist=='both']\n",
    "bfalse_positives = diff_df[diff_df.Exist=='right_only']\n",
    "bfalse_negatives = diff_df[diff_df.Exist=='left_only']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# un vrai positif: c un vrai couple de restos en double qui a été détecté par notre algo sous forme de blocking method.\n",
    "# en effet il vérifie le critère de name (edit_distance=0) et en plus les 2 restos se trouve dans la même ville d'atlanta.\n",
    "df_restaurants[df_restaurants.record_ID.isin(['6','754'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# les couples détectés par notre algo comme des doubles mais à tort, ce ne sont pas des doubles.\n",
    "false_positives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants[df_restaurants.record_ID.isin(['96','196'])]\n",
    "# ce couple n'est pas dans le ground_truth car unique_id différent\n",
    "# mais il est dans le bmatches_df , (jd_ng1_ng2_adr <= 0.6 and jd_ng1_ng2_name <= 0.6) \n",
    "# cad les names sont proches pour la jaccard distance item based\n",
    "# et les adresses sont proches pour la jaccard distance item based.\n",
    "# et en plus ils se trouvent dans la même ville atlanta (blocking method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# les couples de restos en double mais qui ne sont pas détectés par notre algo comme des doubles.\n",
    "false_negatives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants[df_restaurants.record_ID.isin(['2','753'])]\n",
    "# ce couple est dans le ground_truth car même unique_id \n",
    "# mais il n'est pas dans le matches_df, malgré qu' ils ont le même name et la  même adresse (dans l'algo les détecte bien)\n",
    "# mais le Blocking method ne permet pas à l'algo de les matcher car ils sont considérés ayant des villes différentes :\n",
    "# 'new york' et 'new york city'  , à cause d'une mauvaise saisie de la ville."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bfalse_negatives) # y a beaucoup de false_ngatives par rapport à l'algo dans Blocking method (on avait 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false negative\n",
    "df_restaurants[df_restaurants.record_ID.isin(['26','756'])]\n",
    "# du au blocking method : new yor et new york city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false negative\n",
    "df_restaurants[df_restaurants.record_ID.isin(['32','759'])]\n",
    "# dû aux matching imprécis de l'algo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false negative\n",
    "df_restaurants[df_restaurants.record_ID.isin(['36','760'])]\n",
    "# du au blocking method : new yor et new york city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ground_truth_matches))\n",
    "print(len(bmatches_df))\n",
    "print(len(btrue_positives) , 'true_positives')\n",
    "print(len(bfalse_positives) ,'false_positives')\n",
    "print(len(bfalse_negatives)  , 'false_negatives')\n",
    "\n",
    "# len(true_positives)  +  len(false_negatives) = len(ground_truth_matches)\n",
    "\n",
    "# len(matches_df)) - len(false_positif) + len(false_negatives)     = ground_truth_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bprecision = len(btrue_positives)/(len(btrue_positives)+ len(bfalse_positives))\n",
    "print(bprecision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brecall = len(btrue_positives)/(len(btrue_positives)+ len(bfalse_negatives))\n",
    "print(brecall)\n",
    "# recall faible car y a beaucoup de false negatives\n",
    "# y a des duplicates que l'algo avec Blocking method n'a pas détecté car saisie à tort dans des villes différentes\n",
    "# surtout new york et new york city "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_measure = 2*(bprecision*brecall)/(bprecision+brecall)\n",
    "print(bf_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chiffres de l'algo original sans blocking method\n",
    "### precision: 0.7795\n",
    "\n",
    "### recall : 0.8839\n",
    "\n",
    "### f_measure :0.82845"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
